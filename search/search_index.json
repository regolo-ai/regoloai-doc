{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with Regolo.ai","text":"<p>Tip</p> <p>For API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"#create-an-account","title":"Create an Account","text":"<p>To get started with Regolo.ai, sign up for an account at dashboard.regolo.ai.</p>"},{"location":"#generate-an-api-key","title":"Generate an API Key","text":"<p>Once logged in, navigate to the Virtual Keys section and create a new key. You can choose a specific model or select \"All models\" to use the key across all available models.</p>"},{"location":"#choose-your-client","title":"Choose your client","text":"<p>Regolo.ai is fully compatible with the OpenAI API, so you can use either:</p> <p>Regolo Python Library or OpenAI Python Library</p>"},{"location":"#chat-example","title":"Chat Example","text":"<p>Below is an example of how to create a simple chat application in python using regolo client.</p> Using Regolo Client <pre><code>    import streamlit as st\n    import regolo\n\n    regolo.default_key = \"YOUR-API-KEY-HERE\"\n    regolo.default_model = \"Llama-3.3-70B-Instruct\"\n\n    client = regolo.RegoloClient()\n\n    st.title(\"Regolo.ai Chat\")\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    for msg in st.session_state.messages:\n        with st.chat_message(msg[\"role\"]):\n            st.markdown(msg[\"content\"])\n\n    user_input = st.chat_input(\"Write a message...\")\n    if user_input:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        client.add_prompt_to_chat(role=\"user\", prompt=user_input)\n        for msg in st.session_state.messages:\n            client.add_prompt_to_chat(role=msg[\"role\"], prompt=msg[\"content\"])\n\n        response = client.run_chat()\n\n        role, content = response\n\n        st.session_state.messages.append({\"role\": role, \"content\": content})\n\n        with st.chat_message(role):\n            st.markdown(content)\n</code></pre>"},{"location":"completions/","title":"Completions and Chat","text":""},{"location":"completions/#static-completions","title":"Static Completions","text":"<p>Static completions allow you to generate text responses based on a given prompt using the Regolo API.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_completions(prompt=\"Tell me something about Rome.\"))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}'\n</code></pre>"},{"location":"completions/#static-chat-completions","title":"Static Chat Completions","text":"<p>Static chat completions enable a more interactive session by providing conversation-like exchanges, you can send a series of messages. Each message has a role, such as <code>user</code>, <code>assistant</code> or <code>system</code>. The model processes these to continue the conversation naturally. This is useful for applications requiring a back-and-forth dialogue.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_chat_completions(messages=[{\"role\": \"user\", \"content\": \"Tell me something about rome\"}]))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ]\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about Rome.\"\n        }\n    ]\n}'\n</code></pre>"},{"location":"completions/#stream-chat-completions","title":"Stream Chat Completions","text":"<p>Stream chat completions provide real-time, incremental responses from the model, enabling dynamic interactions and reducing latency. This feature is beneficial for applications that require immediate feedback and continuous conversation flow.</p> <p>The streaming response is structured as JSON objects sent line by line. Each line typically contains metadata, including fields like <code>id</code>, <code>created</code>, <code>model</code>, and <code>object</code>, along with the <code>choices</code> array. Within <code>choices</code>, there is a <code>delta</code> object, which holds the <code>content</code> field representing the actual text response from the model. This structure allows applications to parse and process the conversational content as it arrives, ensuring efficient and timely updates to the user interface.</p> Using Regolo ClientPython <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(user_prompt=\"Tell me something about Rome.\", full_output=True, stream=True)\n\n\nwhile True:\n    try:\n        print(next(response))\n    except StopIteration:\n        break\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\nfor line in response.iter_lines():\n    if line:\n        print(line.decode('utf-8'))\n</code></pre> <p>Tip</p> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"}]}