{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with Regolo.ai","text":"<p>Tip</p> <p>For API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"#create-an-account","title":"Create an Account","text":"<p>To get started with Regolo.ai, sign up for an account at dashboard.regolo.ai.</p>"},{"location":"#generate-an-api-key","title":"Generate an API Key","text":"<p>Once logged in, navigate to the Virtual Keys section and create a new key. You can choose a specific model or select \"All models\" to use the key across all available models.</p>"},{"location":"#choose-your-client","title":"Choose your client","text":"<p>Regolo.ai is fully compatible with the OpenAI API, so you can use either:</p> <p>Regolo Python Library or OpenAI Python Library</p>"},{"location":"#chat-example","title":"Chat Example","text":"<p>Below is an example of how to create a simple chat application in python using regolo client.</p> Using Regolo Client <pre><code>    import streamlit as st\n    import regolo\n\n    regolo.default_key = \"YOUR-API-KEY-HERE\"\n    regolo.default_model = \"Llama-3.3-70B-Instruct\"\n\n    client = regolo.RegoloClient()\n\n    st.title(\"Regolo.ai Chat\")\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    for msg in st.session_state.messages:\n        with st.chat_message(msg[\"role\"]):\n            st.markdown(msg[\"content\"])\n\n    user_input = st.chat_input(\"Write a message...\")\n    if user_input:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        client.add_prompt_to_chat(role=\"user\", prompt=user_input)\n        for msg in st.session_state.messages:\n            client.add_prompt_to_chat(role=msg[\"role\"], prompt=msg[\"content\"])\n\n        response = client.run_chat()\n\n        role, content = response\n\n        st.session_state.messages.append({\"role\": role, \"content\": content})\n\n        with st.chat_message(role):\n            st.markdown(content)\n</code></pre>"},{"location":"completions/","title":"Completions and Chat","text":""},{"location":"completions/#static-completions","title":"Static Completions","text":"<p>Static completions allow you to generate text responses based on a given prompt using the Regolo API.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_completions(prompt=\"Tell me something about Rome.\"))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}'\n</code></pre>"},{"location":"completions/#static-chat-completions","title":"Static Chat Completions","text":"<p>Static chat completions enable a more interactive session by providing conversation-like exchanges, you can send a series of messages. Each message has a role, such as <code>user</code>, <code>assistant</code> or <code>system</code>. The model processes these to continue the conversation naturally. This is useful for applications requiring a back-and-forth dialogue.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_chat_completions(messages=[{\"role\": \"user\", \"content\": \"Tell me something about rome\"}]))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ]\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"gpt-4o\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about Rome.\"\n        }\n    ]\n}'\n</code></pre>"},{"location":"completions/#stream-chat-completions","title":"Stream Chat Completions","text":"<p>Stream chat completions provide real-time, incremental responses from the model, enabling dynamic interactions and reducing latency. This feature is beneficial for applications that require immediate feedback and continuous conversation flow.</p> <p>The streaming response is structured as JSON objects sent line by line. Each line typically contains metadata, including fields like <code>id</code>, <code>created</code>, <code>model</code>, and <code>object</code>, along with the <code>choices</code> array. Within <code>choices</code>, there is a <code>delta</code> object, which holds the <code>content</code> field representing the actual text response from the model. This structure allows applications to parse and process the conversational content as it arrives, ensuring efficient and timely updates to the user interface.</p> Using Regolo ClientPython <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(user_prompt=\"Tell me something about Rome.\", full_output=True, stream=True)\n\n\nwhile True:\n    try:\n        print(next(response))\n    except StopIteration:\n        break\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\nfor line in response.iter_lines():\n    if line:\n        print(line.decode('utf-8'))\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"embedding/","title":"Embedding","text":"<p>The embedding API allows you to get a vector representation of the input to be used from machine learning models or algorithms, leveraging models like <code>gte-Qwen2</code>.</p>"},{"location":"embedding/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>input</code>: A string describing the sentece, such as \"A white cat resting in Rome.\"</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"gte-Qwen2.\"</li> </ul> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_embedder_model = \"gte-Qwen2\"\n\n\nembeddings = regolo.static_embeddings(input_text=[\"A white cat resting in Rome\", \"A white cat resting in Paris\"])\n\nprint(embeddings)\n</code></pre> <pre><code>import requests\nimport json\n\nurl = 'https://api.regolo.ai/v1/embeddings'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"prompt\": \"A white cat resting in Rome\",\n    \"model\": \"gte-Qwen2\",\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    with open(\"./embedding.json\", 'w') as _file:\n        json.dump(response.json(), _file)\nelse:\n    print(\"Failed embedding request:\", response.status_code, response.text)\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/embeddings\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\"\n-d '{\n    \"model\": \"gte-Qwen2\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"images/","title":"Images Generation","text":"<p>The image generation API allows you to create images based on textual descriptions, leveraging models like <code>FLUX.1-dev</code>.</p>"},{"location":"images/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>prompt</code>: A string describing the desired image, such as \"A white cat resting in Rome.\"</li> <li><code>n</code>: An integer specifying the number of images to generate. Generating more images increases response time, so it's best to keep this number small for faster performance.</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"FLUX.1-dev.\"</li> <li><code>size</code>: A string defining the dimensions of the images. Supported sizes are \"256x256,\" \"512x512,\" and \"1024x1024.\"</li> </ul> <p>Larger images take longer to generate, so consider using smaller sizes for quicker results.</p> <p>Tip</p> <p>If you require larger images, consider using an image upscaler after generation. This can help achieve the desired resolution without increasing the generation time</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nfrom io import BytesIO\nfrom PIL import Image\n\nregolo.default_image_model = \"FLUX.1-dev\"\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\n\nimg_bytes = regolo.static_image_create(prompt=\"A white cat resting in Rome\")[0]\n\nimage = Image.open(BytesIO(img_bytes))\n\nimage.show()\n</code></pre> <pre><code>import requests\nimport json\nfrom PIL import Image\nimport io\nimport base64\n\nurl = 'https://api.regolo.ai/v1/images/generations'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"prompt\": \"A white cat resting in Rome\",\n    \"n\": 2,\n    \"model\": \"FLUX.1-dev\",\n    \"size\": \"1024x1024\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    response_data = response.json()\n\n    for index, item in enumerate(response_data['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n\n        image_stream = io.BytesIO(image_data)\n        image = Image.open(image_stream)\n\n        image.show(title=f'Generated Image {index + 1}')\nelse:\n    print(\"Failed to generate images:\", response.status_code, response.text)\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/images/generations?=' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"prompt\": \"A white cat resting in Rome\",\n    \"n\": 2,\n    \"model\": \"FLUX.1-dev\",\n    \"size\": \"1024x1024\"\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"}]}