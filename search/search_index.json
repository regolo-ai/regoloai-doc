{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Getting Started with Regolo.ai","text":"<p>To get started with Regolo.ai, sign up for an account at dashboard.regolo.ai.</p>"},{"location":"#generate-an-api-key","title":"Generate an API Key","text":"<p>Once logged in, navigate to the Virtual Keys section and create a new key. You can choose a specific model or select \"All models\" to use the key across all available models.</p>"},{"location":"#choose-your-client","title":"Choose your client","text":"<p>Regolo.ai is fully compatible with the OpenAI API, so you can use either:</p> <p>Regolo Python Library or OpenAI Python Library</p>"},{"location":"#chat-example","title":"Chat Example","text":"<p>Below is an example of how to create a simple chat application in python using regolo client.</p> Using Regolo Client <pre><code>    import streamlit as st\n    import regolo\n\n    regolo.default_key = \"YOUR-API-KEY-HERE\"\n    regolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\n    client = regolo.RegoloClient()\n\n    st.title(\"Regolo.ai Chat\")\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    for msg in st.session_state.messages:\n        with st.chat_message(msg[\"role\"]):\n            st.markdown(msg[\"content\"])\n\n    user_input = st.chat_input(\"Write a message...\")\n    if user_input:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        client.add_prompt_to_chat(role=\"user\", prompt=user_input)\n        for msg in st.session_state.messages:\n            client.add_prompt_to_chat(role=msg[\"role\"], prompt=msg[\"content\"])\n\n        response = client.run_chat()\n\n        role, content = response\n\n        st.session_state.messages.append({\"role\": role, \"content\": content})\n\n        with st.chat_message(role):\n            st.markdown(content)\n</code></pre>"},{"location":"catalog/","title":"Regolo Catalog","text":"<p>The Regolo Catalog is the official catalog of available models on the  Regolo AI platform. It provides an up-to-date list of all models that can be  used with the Regolo API.</p>"},{"location":"catalog/#overview","title":"Overview","text":"<p>The catalog is automatically updated every 24 hours by fetching the latest list  of available models from the Regolo API. This ensures that all applications  using Regolo models always have access to the most up-to-date model list.</p>"},{"location":"catalog/#accessing-the-catalog","title":"Accessing the Catalog","text":""},{"location":"catalog/#repository","title":"Repository","text":"<p>Visit the official Regolo Model Catalog repository on GitHub:</p> <p>\ud83d\udd17 regolo-ai/regolo-model-catalog</p>"},{"location":"catalog/#list-available-models","title":"List Available Models","text":"<p>You can retrieve the list of available models directly from the Regolo API  using the <code>/models</code> endpoint:</p> PythonCURL <pre><code>import requests\n\nurl = \"https://api.regolo.ai/models\"\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    models = response.json()\n    print(models)\nelse:\n    print(\"Failed to fetch models:\", response.status_code, \n          response.text)\n</code></pre> <pre><code>curl -X GET https://api.regolo.ai/models\n</code></pre>"},{"location":"catalog/#request-a-model","title":"Request a Model","text":"<p>Want to see a specific model on Regolo? Let us know! You can:</p> <ul> <li>Open an issue on the catalog repository</li> <li>Start a discussion</li> <li>Contact us at help@regolo.ai</li> </ul>"},{"location":"models/families/completions/","title":"Completions and Chat","text":""},{"location":"models/families/completions/#static-chat-completions","title":"Static Chat Completions","text":"<p>Static chat completions enable a more interactive session by providing conversation-like exchanges, you can send a series of messages. Each message has a role, such as <code>user</code>, <code>assistant</code> or <code>system</code>. The model processes these to continue the conversation naturally. This is useful for applications requiring a back-and-forth dialogue.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_chat_completions(messages=[{\"role\": \"user\", \"content\": \"Tell me something about rome\"}]))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ]\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_REGOLO_KEY\" \\\n    -d '{\n     \"model\": \"Llama-3.3-70B-Instruct\",\n     \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about Rome.\"\n        }\n    ]\n}'\n</code></pre>"},{"location":"models/families/completions/#stream-chat-completions","title":"Stream Chat Completions","text":"<p>Stream chat completions provide real-time, incremental responses from the model, enabling dynamic interactions and reducing latency. This feature is beneficial for applications that require immediate feedback and continuous conversation flow.</p> <p>The streaming response is structured as JSON objects sent line by line. Each line typically contains metadata, including fields like <code>id</code>, <code>created</code>, <code>model</code>, and <code>object</code>, along with the <code>choices</code> array. Within <code>choices</code>, there is a <code>delta</code> object, which holds the <code>content</code> field representing the actual text response from the model. This structure allows applications to parse and process the conversational content as it arrives, ensuring efficient and timely updates to the user interface.</p> Regolo Client - CleanRegolo Client - FullPython - CleanPython - Full <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(\n    user_prompt=\"Tell me something about Rome.\",\n    full_output=True,\n    stream=True\n)\n\n# Process streamed responses - extract only content (words)\nwhile True:\n    try:\n        chunk = next(response)\n        # Extract content from delta if it's a dict\n        if isinstance(chunk, dict):\n            content = chunk.get('choices', [{}])[0].get('delta', {}).get('content', '')\n            if content:\n                print(content, end=\"\", flush=True)\n        else:\n            # If it's already a string, print it directly\n            print(chunk, end=\"\", flush=True)\n    except StopIteration:\n        break\n</code></pre> <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(\n    user_prompt=\"Tell me something about Rome.\",\n    full_output=True,\n    stream=True\n)\n\n# Process streamed responses - show full JSON structure\nwhile True:\n    try:\n        chunk = next(response)\n        print(chunk)\n    except StopIteration:\n        break\n</code></pre> <pre><code>import requests\nimport json\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\n# Extract only content from streamed JSON\nfor line in response.iter_lines():\n    if line:\n        decoded_line = line.decode('utf-8')\n        if decoded_line.startswith('data: '):\n            json_str = decoded_line[6:]\n            if json_str.strip() == '[DONE]':\n                break\n            try:\n                json_data = json.loads(json_str)\n                if 'choices' in json_data:\n                    delta = json_data['choices'][0].get('delta', {})\n                    content = delta.get('content', '')\n                    if content:\n                        print(content, end='', flush=True)\n            except json.JSONDecodeError:\n                pass\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\n# Show full JSON response\nfor line in response.iter_lines():\n    if line:\n        print(line.decode('utf-8'))\n</code></pre>"},{"location":"models/families/completions/#text-static-completions-deprecated","title":"Text Static Completions (Deprecated)","text":"<p>Warning</p> <p>The static text completions are currently deprecated, and Regolo no longer provides any model that supports them natively. They are still listed among the available endpoints only for backward compatibility and internal use, but no public model currently supports them. Use the chat completions instead.</p> <p>Static completions allow you to generate text responses based on a given prompt using the Regolo API.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_completions(prompt=\"Tell me something about Rome.\"))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/embedding/","title":"Embedding","text":"<p>The embedding API allows you to get a vector representation of the input to be used from machine learning models or algorithms, leveraging models like <code>gte-Qwen2</code>.</p>"},{"location":"models/families/embedding/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>input</code>: A string describing the sentence, such as \"A white cat resting in Rome.\"</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"gte-Qwen2.\"</li> </ul> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_embedder_model = \"gte-Qwen2\"\n\n\nembeddings = regolo.static_embeddings(input_text=[\"A white cat resting in Rome\", \"A white cat resting in Paris\"])\n\nprint(embeddings)\n</code></pre> <pre><code>import requests\nimport json\n\nurl = 'https://api.regolo.ai/v1/embeddings'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"prompt\": \"A white cat resting in Rome\",\n    \"model\": \"gte-Qwen2\",\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    with open(\"./embedding.json\", 'w') as _file:\n        json.dump(response.json(), _file)\nelse:\n    print(\"Failed embedding request:\", response.status_code, response.text)\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/embeddings\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\"\n-d '{\n    \"model\": \"gte-Qwen2\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/images/","title":"Images Generation","text":"<p>The image generation API allows you to create images based on textual descriptions, leveraging models like <code>Qwen-Image</code>.</p>"},{"location":"models/families/images/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>prompt</code>: A string describing the desired image, such as \"A white cat resting in Rome.\"</li> <li><code>n</code>: An integer specifying the number of images to generate. Generating more images increases response time, so it's best to keep this number small for faster performance.</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"Qwen-Image.\"</li> <li><code>size</code>: A string defining the dimensions of the images. Supported sizes are \"256x256,\" \"512x512,\" and \"1024x1024.\"</li> </ul> <p>Larger images take longer to generate, so consider using smaller sizes for quicker results.</p> <p>Tip</p> <p>If you require larger images, consider using an image upscaler after generation. This can help achieve the desired resolution without increasing the generation time</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nfrom io import BytesIO\nfrom PIL import Image\n\n# pip install regolo Pillow\n\nregolo.default_image_generation_model = \"Qwen-Image\"\nregolo.default_key = \"YOUR_REGOLO_KEY\"\n\nimg_bytes = regolo.static_image_create(prompt=\"A Boat in the sea\")[0]\n\nimage = Image.open(BytesIO(img_bytes))\n\n# Save the Image\noutput_path = \"generated_image.png\"\nimage.save(output_path)\nprint(f\"Image saved to: {output_path}\")\n</code></pre> <pre><code>import requests\nimport json\nfrom PIL import Image\nimport io\nimport base64\n\nurl = 'https://api.regolo.ai/v1/images/generations'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"prompt\": \"A white cat resting in Rome\",\n    \"n\": 2,\n    \"model\": \"Qwen-Image\",\n    \"size\": \"1024x1024\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    response_data = response.json()\n\n    for index, item in enumerate(response_data['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n\n        image_stream = io.BytesIO(image_data)\n        image = Image.open(image_stream)\n\n        # Save the Image\n        output_path = f\"generated_image_{index + 1}.png\"\n        image.save(output_path)\n        print(f\"Image saved to: {output_path}\")\nelse:\n    print(\"Failed to generate images:\", response.status_code, response.text)\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/images/generations' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"prompt\": \"A Boat in the sea\",\n    \"n\": 2,\n    \"model\": \"Qwen-Image\",\n    \"size\": \"1024x1024\"\n}' | python3 -c \"\nimport sys\nimport json\nimport base64\n\nresponse = json.load(sys.stdin)\nif 'data' in response:\n    for index, item in enumerate(response['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n        output_path = f'generated_image_{index + 1}.png'\n        with open(output_path, 'wb') as f:\n            f.write(image_data)\n        print(f'Image saved to: {output_path}')\nelse:\n    print('Failed to generate images:', response)\n\"\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/rerank/","title":"Rerank","text":"<p>The Rerank API lets you re\u2011order a list of documents (or passages) according to their relevance to a given query. It is powered by models such as <code>Qwen3\u2011Reranker\u20114B</code> and returns the top\u2011N most relevant documents together with a relevance score.</p> <p>When to use it \u2013 After you have retrieved a large set of candidate documents (e.g., with BM25, vector search, or another LLM), feed them to the Rerank endpoint to obtain a concise, high\u2011quality ranking that can be directly presented to users or passed to a downstream LLM for answer generation.</p>"},{"location":"models/families/rerank/#api-call-parameters","title":"API Call Parameters","text":"<p>Tip</p> <p>Important note \u2013 The endpoint expects a JSON payload and the total request size (including all documents) must stay lighter as possible. If you have many candidates, consider chunking them and calling the API multiple times.</p> Parameter Type Description model <code>string</code> (required) Identifier of the reranker model, e.g., <code>Qwen3\u2011Reranker\u20114B</code>. query <code>string</code> (required) The user\u2019s question or search query. documents <code>array[string]</code> (required) List of candidate documents/passages to be reranked. top_n <code>integer</code> (optional) Number of highest\u2011scoring documents to return. CURLPython <pre><code>curl --request POST \\\n  --url https://api.regolo.ai/rerank \\\n  --header 'Authorization: Bearer REGOLO-API-KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"model\": \"Qwen3-Reranker-4B\",\n    \"query\": \"What is the capital of the United States?\",\n    \"documents\": [\n      \"Carson City is the capital city of the American state of Nevada.\",\n      \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.\",\n      \"Washington, D.C. is the capital of the United States.\",\n      \"Capital punishment has existed in the United States since before it was a country.\"\n    ],\n    \"top_n\": 3\n}'\n</code></pre> <pre><code>import json\nimport requests\n\nAPI_KEY = \"REGOLO-API-KEY\"\nENDPOINT = \"https://api.regolo.ai/rerank\"\n\npayload = {\n    \"model\": \"Qwen3-Reranker-4B\",\n    \"query\": \"What is the capital of the United States?\",\n    \"documents\": [\n        \"Carson City is the capital city of the American state of Nevada.\",\n        \"The Commonwealth of the Northern Mariana Islands is a group of islands in the Pacific Ocean. Its capital is Saipan.\",\n        \"Washington, D.C. is the capital of the United States.\",\n        \"Capital punishment has existed in the United States since before it was a country.\"\n    ],\n    \"top_n\": 3\n}\n\nheaders = {\n    \"Authorization\": f\"Bearer {API_KEY}\",\n    \"Content-Type\": \"application/json\"\n}\n\nresponse = requests.post(ENDPOINT, headers=headers, data=json.dumps(payload))\n\nif response.ok:\n    results = response.json()\n    print(\"Top documents:\")\n    for doc in results.get(\"results\", []):\n        print(f\"- score: {doc['score']:.4f}  \u2192  {doc['document']}\")\nelse:\n    print(\"Error:\", response.status_code, response.text)\n</code></pre>"},{"location":"models/families/rerank/#response","title":"Response","text":"<pre><code>{\n  \"id\": \"rerank-906c2c4ec189b5fe\",\n  \"results\": [\n    {\n      \"index\": 2,\n      \"relevance_score\": 0.9892732501029968,\n      \"document\": {\n        \"text\": \"Washington, D.C. is the capital of the United States.\"\n      }\n    },\n    {\n      \"index\": 3,\n      \"relevance_score\": 0.425626623916626,\n      \"document\": {\n        \"text\": \"Capital punishment has existed in the United States since before it was a country.\"\n      }\n    },\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.4123265105247498,\n      \"document\": {\n        \"text\": \"Carson City is the capital city of the American state of Nevada.\"\n      }\n    }\n  ],\n  \"meta\": null\n}\n</code></pre>"},{"location":"models/families/stt/","title":"Speech To Text","text":"<p>The Speech to Text (STT) API enables you to extract and transcribe text from audio files using models such as <code>faster-whisper-large-v3</code>. We recommend using audio chunks of less than 2 minutes to prevent hallucinations and duplicate transcriptions.</p>"},{"location":"models/families/stt/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>file</code>: A binary audio file in OGG format.</li> <li><code>model</code>: The identifier for the model used for transcription, e.g., <code>faster-whisper-large-v3</code>.</li> <li><code>language</code>: A two-letter ISO language code specifying the language of the audio, such as <code>en</code> (English), <code>it</code> (Italian), etc.</li> </ul>"},{"location":"models/families/stt/#important-note","title":"Important Note","text":"<p>The models have a timeout limit. It is recommended to split audio files into smaller segments, such as five-minute clips, to ensure optimal performance.</p>"},{"location":"models/families/stt/#example-requests","title":"Example Requests","text":"Using Regolo ClientPython ClientCURL <pre><code>import regolo\nfrom pathlib import Path\n\n# Regolo configuration\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_audio_transcription_model = \"faster-whisper-large-v3\"\n\n# Audio file to transcribe\nAUDIO_FILE = \"/path/to/your/audio\"\nOUTPUT_FILE = \"/path/to/output/transcription.txt\"\n\n# Transcribe the file\ntranscript = regolo.static_audio_transcription(file=AUDIO_FILE)\n\n# Save the transcription\noutput_path = Path(OUTPUT_FILE)\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(transcript)\n\nprint(f\"Transcription saved to: {OUTPUT_FILE}\")\n</code></pre> <pre><code>import openai\nfrom pathlib import Path\n\n# OpenAI client configuration\nopenai.api_key = \"YOUR_REGOLO_KEY\"\nopenai.base_url = \"https://api.regolo.ai/v1/\"\n\n# Audio file to transcribe\nAUDIO_FILE = \"/path/to/your/audio\"\nOUTPUT_FILE = \"/path/to/output/transcription.txt\"\n\n# Transcribe the file\nwith open(AUDIO_FILE, \"rb\") as audio_file:\n    transcript = openai.audio.transcriptions.create(\n        model=\"faster-whisper-large-v3\",\n        file=audio_file,\n        language=\"en\",\n        response_format=\"text\"\n    )\n\n# Save the transcription\noutput_path = Path(OUTPUT_FILE)\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(transcript)\n\nprint(f\"Transcription saved to: {OUTPUT_FILE}\")\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/audio/transcriptions' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  -F \"file=@/path/to/your/audio\" \\\n  -F \"model=faster-whisper-large-v3\"\n</code></pre>"},{"location":"models/families/stt/#example-implementation","title":"Example Implementation","text":"<p>For a practical example of how to use this API, you can refer to the Telegram Transcriber GitHub Repository. This repository provides a complete implementation for transcribing audio messages from Telegram using the Speech to Text API.</p> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/vision/","title":"Vision models","text":"<p>Vision completions enable the processing of images alongside text, allowing for a wide range of applications such as image description, object recognition, and data extraction from visual content. By sending a combination of text prompts and image URLs, the model can provide insightful responses based on the visual input.</p>"},{"location":"models/families/vision/#note","title":"Note","text":"<p>This API supports only images, PDF files are not supported.</p>"},{"location":"models/families/vision/#vision-completions","title":"Vision Completions","text":"<p>You can provide images in three ways:</p> <ul> <li>Remote URL: Supply a publicly accessible URL pointing to the image.</li> <li>Base64 (Image from PATH): Read a local image file, encode it as Base64, and send it to the model.</li> <li>Base64 Encoding (Remote URL): Download an image from a URL, encode it as Base64, and send it to the model.</li> </ul>"},{"location":"models/families/vision/#remote-image-url","title":"Remote Image URL","text":"<p>This section demonstrates how to use a remote image URL directly with the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\nprint(regolo.static_chat_completions(messages=[{\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Describe this image in detail.\"\n        },\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                \"format\": \"image/jpeg\"\n            }\n        }\n    ]\n}]))\n</code></pre> <pre><code>import requests\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": \"qwen3-vl-32b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ]\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \\\n-d '{\n    \"model\": \"qwen3-vl-32b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ]\n}'\n</code></pre>"},{"location":"models/families/vision/#base64-image-from-path","title":"Base64 (Image from PATH)","text":"<p>This section demonstrates how to read a local image file from your filesystem, encode it as Base64, and send it to the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nimport base64\nfrom pathlib import Path\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\nIMAGE_PATH = Path(\"beagle-hound-dog.jpg\")\n\nif not IMAGE_PATH.exists():\n    raise FileNotFoundError(f\"Image not found: {IMAGE_PATH.resolve()}\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\nresult = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in detail.\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                    \"format\": \"image/jpeg\"\n                }\n            }\n        ]\n    }],\n    max_tokens=4096,  # Important: maintain a wide output token window to avoid issues\n    full_output=True\n)\n\nprint(\"Model response:\")\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <pre><code>import base64\nimport json\nimport requests\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"qwen3-vl-32b\"\n\nIMAGE_PATH = Path(\"beagle-hound-dog.jpg\")\n\nif not IMAGE_PATH.exists():\n    raise FileNotFoundError(f\"Image not found: {IMAGE_PATH.resolve()}\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096  # Important: maintain a wide output token window to avoid issues\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\nprint(\"Sending request to Regolo AI API...\")\nresponse = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n\nif response.status_code != 200:\n    print(f\"Error {response.status_code}:\")\n    print(response.text)\nelse:\n    result = response.json()\n    try:\n        content = result[\"choices\"][0][\"message\"][\"content\"]\n        print(\"Model response:\")\n        print(content)\n    except Exception:\n        print(\"Unexpected response format:\")\n        print(response.text)\n</code></pre> <pre><code># Encode local image to base64\nIMAGE_B64=$(base64 -w 0 beagle-hound-dog.jpg)\n\ncurl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR-API-KEY\" \\\n-d \"{\n    \\\"model\\\": \\\"qwen3-vl-32b\\\",\n    \\\"messages\\\": [\n        {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": [\n                {\n                    \\\"type\\\": \\\"text\\\",\n                    \\\"text\\\": \\\"Describe this image in detail.\\\"\n                },\n                {\n                    \\\"type\\\": \\\"image_url\\\",\n                    \\\"image_url\\\": {\n                        \\\"url\\\": \\\"data:image/jpeg;base64,$IMAGE_B64\\\",\n                        \\\"format\\\": \\\"image/jpeg\\\"\n                    }\n                }\n            ]\n        }\n    ],\n    \\\"max_tokens\\\": 4096  # Important: maintain a wide output token window to avoid issues\n}\"\n</code></pre>"},{"location":"models/families/vision/#base64-encoding-remote-url","title":"Base64 Encoding (Remote URL)","text":"<p>This section demonstrates how to download an image from a remote URL, encode it as Base64, and send it to the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nimport base64\nimport requests\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\n# Download image and convert to base64\nimage_url = \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\nresult = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in detail.\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                    \"format\": \"image/jpeg\"\n                }\n            }\n        ]\n    }],\n    temperature=0.2,\n    max_tokens=4096,  # Important: maintain a wide output token window to avoid issues\n    full_output=True\n)\n\nprint(\"Model response:\")\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <pre><code>import base64\nimport requests\nimport json\n\n# Regolo API credentials configuration\napi_key = \"YOUR_API_KEY\"\nmodel = \"qwen3-vl-32b\"  # Vision-compatible model\n\n# Download image and convert to base64\nimage_url = \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\n# Direct API call for multimodal messages\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": model,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"temperature\": 0.2,\n    \"max_tokens\": 4096  # Important: maintain a wide output token window to avoid issues\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\nprint(\"Sending request to Regolo AI API...\")\nresponse = requests.post(url, json=payload, headers=headers)\n\nif response.status_code == 200:\n    result = response.json()\n    try:\n        content = result[\"choices\"][0][\"message\"][\"content\"]\n        print(\"Model response:\")\n        print(content)\n    except KeyError as e:\n        print(f\"KeyError: {e}\")\n        print(\"Response structure:\")\n        print(json.dumps(result, indent=2))\nelse:\n    print(f\"Error {response.status_code}:\")\n    print(response.text)\n</code></pre> <pre><code># First, download the image and encode it to base64\n# This example assumes you have the image URL\nIMAGE_URL=\"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nIMAGE_B64=$(curl -s \"$IMAGE_URL\" | base64 -w 0)\n\ncurl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_API_KEY\" \\\n-d \"{\n    \\\"model\\\": \\\"qwen3-vl-32b\\\",\n    \\\"messages\\\": [\n        {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": [\n                {\n                    \\\"type\\\": \\\"text\\\",\n                    \\\"text\\\": \\\"Describe this image in detail.\\\"\n                },\n                {\n                    \\\"type\\\": \\\"image_url\\\",\n                    \\\"image_url\\\": {\n                        \\\"url\\\": \\\"data:image/jpeg;base64,$IMAGE_B64\\\",\n                        \\\"format\\\": \\\"image/jpeg\\\"\n                    }\n                }\n            ]\n        }\n    ],\n    \\\"temperature\\\": 0.2,\n    \\\"max_tokens\\\": 4096  # Important: maintain a wide output token window to avoid issues\n}\"\n</code></pre>"},{"location":"models/features/response-parameters/","title":"Response Parameters","text":"<p>The Regolo API follows the OpenAI Chat Completions API specification and supports standard parameters that control the behavior and quality of model responses.</p>"},{"location":"models/features/response-parameters/#overview","title":"Overview","text":"<p>These parameters allow you to fine-tune how the model generates responses, controlling aspects like creativity, length, diversity, and repetition.</p>"},{"location":"models/features/response-parameters/#standard-parameters","title":"Standard Parameters","text":""},{"location":"models/features/response-parameters/#temperature","title":"<code>temperature</code>","text":"<p>Controls the randomness of the model's output. </p> <ul> <li>Type: <code>float</code></li> <li>Range: 0.0 to 2.0</li> <li>Default: 1.0</li> </ul> <p>Lower values (e.g., 0.2) make the output more deterministic and focused, while higher values (e.g., 0.8) increase creativity and variability.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me a story\"}\n    ],\n    \"temperature\": 0.7  # Balanced creativity\n}\n</code></pre>"},{"location":"models/features/response-parameters/#max_tokens","title":"<code>max_tokens</code>","text":"<p>Sets the maximum number of tokens that can be generated in the response.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: Varies by model</li> </ul> <p>Higher values allow for longer responses but may increase latency and costs. Lower values provide more concise outputs.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ],\n    \"max_tokens\": 1000  # Limit response length\n}\n</code></pre>"},{"location":"models/features/response-parameters/#top_p","title":"<code>top_p</code>","text":"<p>Nucleus sampling parameter that controls the diversity of tokens considered.</p> <ul> <li>Type: <code>float</code></li> <li>Range: 0.0 to 1.0</li> <li>Default: 1.0</li> </ul> <p>Lower values make the model more focused on likely tokens, while higher values allow more diverse choices.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a creative poem\"}\n    ],\n    \"top_p\": 0.9  # Allow more diverse word choices\n}\n</code></pre>"},{"location":"models/features/response-parameters/#frequency_penalty","title":"<code>frequency_penalty</code>","text":"<p>Reduces the likelihood of the model repeating frequent tokens.</p> <ul> <li>Type: <code>float</code></li> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> </ul> <p>Positive values reduce repetition, while negative values increase it. Useful for avoiding repetitive phrases.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"List 10 different ideas\"}\n    ],\n    \"frequency_penalty\": 0.5  # Reduce repetition\n}\n</code></pre>"},{"location":"models/features/response-parameters/#presence_penalty","title":"<code>presence_penalty</code>","text":"<p>Reduces the likelihood of the model reusing tokens that are already present in the text.</p> <ul> <li>Type: <code>float</code></li> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> </ul> <p>Positive values encourage the model to use new tokens and explore different approaches.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Generate diverse solutions\"}\n    ],\n    \"presence_penalty\": 0.6  # Encourage new approaches\n}\n</code></pre>"},{"location":"models/features/response-parameters/#stop","title":"<code>stop</code>","text":"<p>Array of strings that cause the model to stop generating when encountered.</p> <ul> <li>Type: <code>array[string]</code> or <code>string</code></li> <li>Default: <code>null</code></li> </ul> <p>Useful for controlling where the response ends or preventing certain phrases.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Count to 10\"}\n    ],\n    \"stop\": [\"11\", \"twelve\"]  # Stop at these sequences\n}\n</code></pre>"},{"location":"models/features/response-parameters/#n","title":"<code>n</code>","text":"<p>Number of completions to generate for each prompt.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: 1</li> </ul> <p>Generates multiple response variations.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Suggest a name\"}\n    ],\n    \"n\": 3  # Generate 3 different suggestions\n}\n</code></pre>"},{"location":"models/features/response-parameters/#seed","title":"<code>seed</code>","text":"<p>Seed for reproducible outputs.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: <code>null</code></li> </ul> <p>When set, the model will produce more deterministic outputs, useful for testing and reproducibility.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Generate a random number\"}\n    ],\n    \"seed\": 42  # Reproducible output\n}\n</code></pre>"},{"location":"models/features/response-parameters/#complete-example","title":"Complete Example","text":"<pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a creative story\"}\n    ],\n    \"temperature\": 0.8,\n    \"max_tokens\": 500,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.3,\n    \"presence_penalty\": 0.4\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nresult = response.json()\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"models/features/response-parameters/#best-practices","title":"Best Practices","text":"<ul> <li>For factual tasks: Use lower <code>temperature</code> (0.2-0.4) for more deterministic outputs</li> <li>For creative tasks: Use higher <code>temperature</code> (0.7-0.9) for more varied responses</li> <li>For long responses: Set appropriate <code>max_tokens</code> to avoid truncation</li> <li>To reduce repetition: Use <code>frequency_penalty</code> (0.3-0.7) and <code>presence_penalty</code> (0.4-0.6)</li> <li>For consistency: Use <code>seed</code> when you need reproducible outputs</li> </ul>"},{"location":"models/features/thinking/","title":"Thinking","text":"<p>Thinking is a feature that allows models to reason through problems step by step, showing their internal thought process before providing a final answer.</p>"},{"location":"models/features/thinking/#overview","title":"Overview","text":"<p>The thinking feature enables models to break down complex problems into smaller steps, making their reasoning process transparent and allowing for better understanding of how they arrive at their conclusions.</p>"},{"location":"models/features/thinking/#usage","title":"Usage","text":"<p>To enable thinking, you can use the <code>thinking</code> parameter in your API requests.</p> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What color was Napoleon's white horse?\"}\n    ],\n    \"thinking\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nresult = response.json()\n\n# Extract main content and reasoning\nmessage = result.get(\"choices\", [{}])[0].get(\"message\", {})\ncontent = message.get(\"content\", \"\")\nreasoning = message.get(\"reasoning_content\", \"\")\n\nif reasoning:\n    print(\"=== Reasoning ===\")\n    print(reasoning)\n    print(\"\\n=== Final Answer ===\")\n\nprint(content)\n</code></pre>"},{"location":"models/features/thinking/#parameters","title":"Parameters","text":""},{"location":"models/features/thinking/#reasoning_effort","title":"<code>reasoning_effort</code>","text":"<p>Controls the depth and detail of the reasoning process. Available values:</p> <ul> <li><code>low</code>: Minimal reasoning effort, faster responses with brief reasoning</li> <li><code>medium</code>: Balanced reasoning effort (default)</li> <li><code>high</code>: Maximum reasoning effort, more detailed and thorough reasoning</li> </ul> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Solve this step by step: What is 15% of 240?\"}\n    ],\n    \"thinking\": True,\n    \"reasoning_effort\": \"high\"  # low, medium, or high\n}\n</code></pre> <p>Standard Parameters</p> <p>Standard API parameters like <code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>, <code>frequency_penalty</code>, and <code>presence_penalty</code> can also influence the thinking process. See Response Parameters for detailed documentation.</p>"},{"location":"models/features/thinking/#benefits","title":"Benefits","text":"<ul> <li>Transparency: See how the model reasons through problems</li> <li>Debugging: Understand where the model might make mistakes</li> <li>Education: Learn problem-solving strategies from the model's reasoning</li> <li>Quality: Better results for complex, multi-step problems</li> </ul>"}]}