{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started with Regolo.ai","text":"<p>To get started with Regolo.ai, sign up for an account at dashboard.regolo.ai.</p>"},{"location":"#generate-an-api-key","title":"Generate an API Key","text":"<p>Once logged in, navigate to the Virtual Keys section and create a new key. You can choose a specific model or select \"All models\" to use the key across all available models.</p>"},{"location":"#choose-your-client","title":"Choose your client","text":"<p>Regolo.ai is fully compatible with the OpenAI API, so you can use either:</p> <p>Regolo Python Library or OpenAI Python Library</p>"},{"location":"#chat-example","title":"Chat Example","text":"<p>Below is an example of how to create a simple chat application in python using regolo client.</p> Using Regolo Client <pre><code>    import streamlit as st\n    import regolo\n\n    regolo.default_key = \"YOUR-API-KEY-HERE\"\n    regolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\n    client = regolo.RegoloClient()\n\n    st.title(\"Regolo.ai Chat\")\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    for msg in st.session_state.messages:\n        with st.chat_message(msg[\"role\"]):\n            st.markdown(msg[\"content\"])\n\n    user_input = st.chat_input(\"Write a message...\")\n    if user_input:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        client.add_prompt_to_chat(role=\"user\", prompt=user_input)\n        for msg in st.session_state.messages:\n            client.add_prompt_to_chat(role=msg[\"role\"], prompt=msg[\"content\"])\n\n        response = client.run_chat()\n\n        role, content = response\n\n        st.session_state.messages.append({\"role\": role, \"content\": content})\n\n        with st.chat_message(role):\n            st.markdown(content)\n</code></pre>"},{"location":"catalog/","title":"Regolo Catalog","text":"<p>The Regolo Catalog is the official catalog of available models on the  Regolo AI platform. It provides an up-to-date list of all models that can be  used with the Regolo API.</p>"},{"location":"catalog/#overview","title":"Overview","text":"<p>The catalog is automatically updated every 24 hours by fetching the latest list  of available models from the Regolo API. This ensures that all applications  using Regolo models always have access to the most up-to-date model list.</p>"},{"location":"catalog/#accessing-the-catalog","title":"Accessing the Catalog","text":""},{"location":"catalog/#repository","title":"Repository","text":"<p>Visit the official Regolo Model Catalog repository on GitHub:</p> <p>\ud83d\udd17 regolo-ai/regolo-model-catalog</p>"},{"location":"catalog/#list-available-models","title":"List Available Models","text":"<p>You can retrieve the list of available models directly from the Regolo API  using the <code>/models</code> endpoint:</p> PythonCURL <pre><code>import requests\n\nurl = \"https://api.regolo.ai/models\"\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    models = response.json()\n    print(models)\nelse:\n    print(\"Failed to fetch models:\", response.status_code, \n          response.text)\n</code></pre> <pre><code>curl -X GET https://api.regolo.ai/models\n</code></pre>"},{"location":"catalog/#request-a-model","title":"Request a Model","text":"<p>Want to see a specific model on Regolo? Let us know! You can:</p> <ul> <li>Open an issue on the catalog repository</li> <li>Start a discussion</li> <li>Contact us at help@regolo.ai</li> </ul>"},{"location":"models/custom-models/","title":"Custom Models","text":"<p>Regolo\u2019s Custom Models functionality allows you to upload and deploy your own AI models from Hugging Face onto dedicated infrastructure. You maintain full control over the GPU resources, ensuring consistent performance for your specific use cases.</p>"},{"location":"models/custom-models/#quick-start-guide","title":"Quick Start Guide","text":"<p>Follow these steps to bring your own model to Regolo:</p> <ol> <li>Add your model: Click the \"Add model\" button in the top-right corner of your dashboard.</li> <li>Import from Hugging Face: Enter the Hugging Face model URL.</li> <li>Save to Library: Rename the model if desired and save it to your personal library.</li> <li>Deploy: Click \"Deploy model\" from your library list.</li> <li>Select Instance: Choose a hardware instance with the appropriate GPU size and count for your model.</li> <li>Authenticate: Use an active Regolo API key with \"All models\" permissions to begin making calls.</li> </ol> <p>Warning</p> <p>A Regolo key with proper authorization for all models is required to ensure successful API calls.</p>"},{"location":"models/custom-models/#model-support-specific-endpoints","title":"Model Support &amp; Specific Endpoints","text":"<p>While you are free to upload any supported model, the API endpoint you use depends on the Model Family. You must follow the same URI and body structure used for our regular inference services.</p> Model Type Endpoint Path Text Generation (LLMs) <code>/custom-model/v1/chat/completions</code> Image Generation <code>/custom-model/v1/images/generations</code> Speech-To-Text <code>/custom-model/v1/audio/transcriptions</code> Embeddings <code>/custom-model/v1/embeddings</code> Rerankers <code>/custom-model/v1/rerank</code> <p>Info</p> <p>Ensure you use the specific endpoint corresponding to the model's functionality (e.g., Image, Audio, or Reranker) as detailed in our Swagger or on Model Families documentation.</p>"},{"location":"models/custom-models/#deployment-requirements-stability","title":"Deployment Requirements &amp; Stability","text":"<p>To ensure a successful deployment and avoid service interruptions, please verify the following before launching:</p> <ul> <li>vLLM Compatibility: For LLMs, ensure your model architecture is supported by vLLM. Check the vLLM supported models list.</li> <li>Hardware Matching: Verify the model's size and minimum VRAM requirements. The instance size must match or exceed these demands to prevent deployment failures.</li> <li>Startup Latency: Startup times typically range from a few minutes up to 15 minutes. During this window, the endpoint will remain unavailable while the weights are loaded.</li> </ul>"},{"location":"models/custom-models/#custom-inference-example","title":"Custom Inference Example","text":"<p>Base URL: <code>https://api.regolo.ai/custom-model/v1/</code></p> <pre><code>curl -X POST \\\nhttps://api.regolo.ai/custom-model/v1/chat/completions/ \\\n-H \"Authorization: Bearer YOUR-REGOLO-API-KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"YOUR_CUSTOM_MODEL_NAME\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n}'\n</code></pre>"},{"location":"models/custom-models/#pricing-billing","title":"Pricing &amp; Billing","text":"<ul> <li>Hourly Usage: Charges are based on the duration the instance is active.</li> <li>Initial Charge: You will be billed for the first hour immediately upon deployment.</li> <li>Invoicing: Monthly invoices detailing total hours and charges are available in your dashboard.</li> </ul>"},{"location":"models/families/completions/","title":"Completions and Chat","text":""},{"location":"models/families/completions/#static-chat-completions","title":"Static Chat Completions","text":"<p>Static chat completions enable a more interactive session by providing conversation-like exchanges, you can send a series of messages. Each message has a role, such as <code>user</code>, <code>assistant</code> or <code>system</code>. The model processes these to continue the conversation naturally. This is useful for applications requiring a back-and-forth dialogue.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_chat_completions(messages=[{\"role\": \"user\", \"content\": \"Tell me something about rome\"}]))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ]\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_REGOLO_KEY\" \\\n    -d '{\n     \"model\": \"Llama-3.3-70B-Instruct\",\n     \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about Rome.\"\n        }\n    ]\n}'\n</code></pre>"},{"location":"models/families/completions/#stream-chat-completions","title":"Stream Chat Completions","text":"<p>Stream chat completions provide real-time, incremental responses from the model, enabling dynamic interactions and reducing latency. This feature is beneficial for applications that require immediate feedback and continuous conversation flow.</p> <p>The streaming response is structured as JSON objects sent line by line. Each line typically contains metadata, including fields like <code>id</code>, <code>created</code>, <code>model</code>, and <code>object</code>, along with the <code>choices</code> array. Within <code>choices</code>, there is a <code>delta</code> object, which holds the <code>content</code> field representing the actual text response from the model. This structure allows applications to parse and process the conversational content as it arrives, ensuring efficient and timely updates to the user interface.</p> Regolo Client - CleanRegolo Client - FullPython - CleanPython - Full <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(\n    user_prompt=\"Tell me something about Rome.\",\n    full_output=True,\n    stream=True\n)\n\n# Process streamed responses - extract only content (words)\nwhile True:\n    try:\n        chunk = next(response)\n        # Extract content from delta if it's a dict\n        if isinstance(chunk, dict):\n            content = chunk.get('choices', [{}])[0].get('delta', {}).get('content', '')\n            if content:\n                print(content, end=\"\", flush=True)\n        else:\n            # If it's already a string, print it directly\n            print(chunk, end=\"\", flush=True)\n    except StopIteration:\n        break\n</code></pre> <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(\n    user_prompt=\"Tell me something about Rome.\",\n    full_output=True,\n    stream=True\n)\n\n# Process streamed responses - show full JSON structure\nwhile True:\n    try:\n        chunk = next(response)\n        print(chunk)\n    except StopIteration:\n        break\n</code></pre> <pre><code>import requests\nimport json\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\n# Extract only content from streamed JSON\nfor line in response.iter_lines():\n    if line:\n        decoded_line = line.decode('utf-8')\n        if decoded_line.startswith('data: '):\n            json_str = decoded_line[6:]\n            if json_str.strip() == '[DONE]':\n                break\n            try:\n                json_data = json.loads(json_str)\n                if 'choices' in json_data:\n                    delta = json_data['choices'][0].get('delta', {})\n                    content = delta.get('content', '')\n                    if content:\n                        print(content, end='', flush=True)\n            except json.JSONDecodeError:\n                pass\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\n# Show full JSON response\nfor line in response.iter_lines():\n    if line:\n        print(line.decode('utf-8'))\n</code></pre>"},{"location":"models/families/completions/#text-static-completions-deprecated","title":"Text Static Completions (Deprecated)","text":"<p>Warning</p> <p>The static text completions are currently deprecated, and Regolo no longer provides any model that supports them natively. They are still listed among the available endpoints only for backward compatibility and internal use, but no public model currently supports them. Use the chat completions instead.</p> <p>Static completions allow you to generate text responses based on a given prompt using the Regolo API.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_completions(prompt=\"Tell me something about Rome.\"))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/embedding/","title":"Embedding","text":"<p>The embedding API allows you to get a vector representation of the input to be used from machine learning models or algorithms, leveraging models like <code>gte-Qwen2</code>.</p>"},{"location":"models/families/embedding/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>input</code>: A string describing the sentence, such as \"A white cat resting in Rome.\"</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"gte-Qwen2.\"</li> </ul> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_embedder_model = \"gte-Qwen2\"\n\n\nembeddings = regolo.static_embeddings(input_text=[\"A white cat resting in Rome\", \"A white cat resting in Paris\"])\n\nprint(embeddings)\n</code></pre> <pre><code>import requests\nimport json\n\nurl = 'https://api.regolo.ai/v1/embeddings'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"input\": \"A white cat resting in Rome\",\n    \"model\": \"gte-Qwen2\",\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    with open(\"./embedding.json\", 'w') as _file:\n        json.dump(response.json(), _file)\nelse:\n    print(\"Failed embedding request:\", response.status_code, response.text)\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/embeddings\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\"\n-d '{\n    \"model\": \"gte-Qwen2\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/images/","title":"Images Generation","text":"<p>The image generation API allows you to create images based on textual descriptions, leveraging models like <code>Qwen-Image</code>.</p>"},{"location":"models/families/images/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>prompt</code>: A string describing the desired image, such as \"A white cat resting in Rome.\"</li> <li><code>n</code>: An integer specifying the number of images to generate. Generating more images increases response time, so it's best to keep this number small for faster performance.</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"Qwen-Image.\"</li> <li><code>size</code>: A string defining the dimensions of the images. Supported sizes are \"256x256,\" \"512x512,\" and \"1024x1024.\"</li> </ul> <p>Larger images take longer to generate, so consider using smaller sizes for quicker results.</p> <p>Tip</p> <p>If you require larger images, consider using an image upscaler after generation. This can help achieve the desired resolution without increasing the generation time</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nfrom io import BytesIO\nfrom PIL import Image\n\n# pip install regolo Pillow\n\nregolo.default_image_generation_model = \"Qwen-Image\"\nregolo.default_key = \"YOUR_REGOLO_KEY\"\n\nimg_bytes = regolo.static_image_create(prompt=\"A Boat in the sea\")[0]\n\nimage = Image.open(BytesIO(img_bytes))\n\n# Save the Image\noutput_path = \"generated_image.png\"\nimage.save(output_path)\nprint(f\"Image saved to: {output_path}\")\n</code></pre> <pre><code>import requests\nimport json\nfrom PIL import Image\nimport io\nimport base64\n\nurl = 'https://api.regolo.ai/v1/images/generations'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"prompt\": \"A white cat resting in Rome\",\n    \"n\": 2,\n    \"model\": \"Qwen-Image\",\n    \"size\": \"1024x1024\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    response_data = response.json()\n\n    for index, item in enumerate(response_data['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n\n        image_stream = io.BytesIO(image_data)\n        image = Image.open(image_stream)\n\n        # Save the Image\n        output_path = f\"generated_image_{index + 1}.png\"\n        image.save(output_path)\n        print(f\"Image saved to: {output_path}\")\nelse:\n    print(\"Failed to generate images:\", response.status_code, response.text)\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/images/generations' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"prompt\": \"A Boat in the sea\",\n    \"n\": 2,\n    \"model\": \"Qwen-Image\",\n    \"size\": \"1024x1024\"\n}' | python3 -c \"\nimport sys\nimport json\nimport base64\n\nresponse = json.load(sys.stdin)\nif 'data' in response:\n    for index, item in enumerate(response['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n        output_path = f'generated_image_{index + 1}.png'\n        with open(output_path, 'wb') as f:\n            f.write(image_data)\n        print(f'Image saved to: {output_path}')\nelse:\n    print('Failed to generate images:', response)\n\"\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/ocr/","title":"OCR","text":"<p>DeepSeek OCR is a powerful optical character recognition model that enables accurate text extraction from images and documents. It supports various modes including native and dynamic resolutions, making it suitable for different use cases from simple OCR to complex document parsing.</p>"},{"location":"models/families/ocr/#deepseek-ocr-usage-with-regolo-api","title":"Deepseek-OCR Usage with Regolo API","text":"<p>Use DeepSeek OCR on the Regolo platform with model name: <code>\"deepseek-ocr\"</code></p>"},{"location":"models/families/ocr/#python-examples","title":"Python Examples","text":"Remote Image URLBase64 (Image from PATH)Base64 Encoding (Remote URL) <pre><code>import requests\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": \"deepseek-ocr\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Convert the document to markdown.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/86/22386-050-51E63D13/Silicon-silicon-symbol-square-Si-properties-some.jpg\",\n                        \"format\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"skip_special_tokens\": False\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nprint(response.json())\n</code></pre> <pre><code>import base64\nimport requests\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"deepseek-ocr\"\n\nIMAGE_PATH = Path(\"document.png\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Convert the document to markdown.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{image_b64}\",\n                        \"format\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096,\n    \"skip_special_tokens\": False\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\nresponse = requests.post(API_URL, headers=headers, json=payload)\nresult = response.json()\ncontent = result[\"choices\"][0][\"message\"][\"content\"]\nprint(content)\n</code></pre> <pre><code>import base64\nimport requests\n\napi_key = \"YOUR_API_KEY\"\nmodel = \"deepseek-ocr\"\n\nimage_url = \"https://example.com/document.png\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": model,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Convert the document to markdown.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{image_b64}\",\n                        \"format\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096,\n    \"skip_special_tokens\": False\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nresult = response.json()\ncontent = result[\"choices\"][0][\"message\"][\"content\"]\nprint(content)\n</code></pre> <p>Note on skip_special_tokens</p> <p>The <code>skip_special_tokens</code> parameter controls whether special tokens (like <code>&lt;|grounding|&gt;</code>) are included in the response: - <code>skip_special_tokens=False</code>: Keeps special tokens (default). Use when you need document structure info. - <code>skip_special_tokens=True</code>: Removes special tokens. Use for clean text output only.</p>"},{"location":"models/families/ocr/#prompt-examples","title":"Prompt Examples","text":"<pre><code>DeepSeek OCR supports various prompt formats for different use cases:\n\n# Convert the document contents to markdown format\n&lt;|grounding|&gt;Convert the document to markdown.\n\n# Perform text recognition on this image\n&lt;|grounding|&gt;OCR this image.\n\n# Extract all text without layout consideration\nFree OCR.\n\n# Parse any figures or tables in the document\nParse the figure.\n\n# Provide a detailed description of the image content\nDescribe this image in detail.\n\n# Locate the position of &lt;|ref|&gt;xxxx&lt;|/ref|&gt; in the image\nLocate &lt;|ref|&gt;xxxx&lt;|/ref|&gt; in the image.\n</code></pre>"},{"location":"models/families/ocr/#pdf-ocr-reader","title":"PDF OCR Reader","text":"<p>This example demonstrates how to extract text from a PDF document by converting each page to an image and processing it through the OCR API. All extracted text is aggregated and saved to a markdown file.</p> <pre><code>import base64\nimport requests\nimport fitz\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"deepseek-ocr\"\n\nPDF_PATH = Path(\"document.pdf\")\nOUTPUT_PATH = PDF_PATH.with_suffix(\".md\")\n\ndoc = fitz.open(PDF_PATH)\nall_text = []\n\nfor page_num in range(len(doc)):\n    page = doc[page_num]\n    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n    img_bytes = pix.tobytes(\"png\")\n\n    image_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n\n    payload = {\n        \"model\": MODEL,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Convert the document to markdown.\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/png;base64,{image_b64}\",\n                            \"format\": \"image/png\"\n                        }\n                    }\n                ]\n            }\n        ],\n        \"max_tokens\": 4096,\n        \"skip_special_tokens\": False\n    }\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {API_KEY}\"\n    }\n\n    response = requests.post(API_URL, headers=headers, json=payload)\n    result = response.json()\n    content = result[\"choices\"][0][\"message\"][\"content\"]\n\n    all_text.append(f\"\\n\\n--- Page {page_num + 1} ---\\n\\n\")\n    all_text.append(content)\n\ndoc.close()\n\nwith open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\".join(all_text))\n\nprint(f\"Completed : file saved in \\\"{OUTPUT_PATH}\\\"\")\n</code></pre>"},{"location":"models/families/ocr/#resources","title":"Resources","text":"<p>Deepseek-OCR Links: - GitHub Repository - Hugging Face - Arxiv Paper</p> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/rerank/","title":"Rerank","text":"<p>The Rerank API lets you re\u2011order a list of documents (or passages) according to their relevance to a given query. It is powered by models such as <code>Qwen3\u2011Reranker\u20114B</code> and returns the top\u2011N most relevant documents together with a relevance score.</p> <p>When to use it \u2013 After you have retrieved a large set of candidate documents (e.g., with BM25, vector search, or another LLM), feed them to the Rerank endpoint to obtain a concise, high\u2011quality ranking that can be directly presented to users or passed to a downstream LLM for answer generation.</p>"},{"location":"models/families/rerank/#api-call-parameters","title":"API Call Parameters","text":"<p>Tip</p> <p>Important note \u2013 The endpoint expects a JSON payload and the total request size (including all documents) must stay lighter as possible. If you have many candidates, consider chunking them and calling the API multiple times.</p> Parameter Type Description model <code>string</code> (required) Identifier of the reranker model, e.g., <code>Qwen3\u2011Reranker\u20114B</code>. query <code>string</code> (required) The user\u2019s question or search query. documents <code>array[string]</code> (required) List of candidate documents/passages to be reranked. top_n <code>integer</code> (optional) Number of highest\u2011scoring documents to return."},{"location":"models/families/rerank/#technical-explanation-of-prompt-tags","title":"Technical Explanation of Prompt Tags","text":"<p>To achieve optimal performance with the Qwen3-Reranker model, the input strings must follow a specific prompt template. This structure allows the underlying Cross-Encoder to differentiate between instructions, user intent, and the content to be ranked.</p> Tag Purpose Description <code>&lt;Instruct&gt;</code> Task Specification Defines the context of the retrieval (e.g., \"retrieve relevant passages\"). This guides the model's focus based on the specific use case. <code>&lt;Query&gt;</code> User Intent Marks the beginning of the actual question or search term. This helps the model isolate the core information the user is looking for. <code>&lt;Document&gt;</code> Content Boundary Identifies each candidate passage. By explicitly tagging documents, the model better understands where one passage ends and another begins during the scoring process."},{"location":"models/families/rerank/#why-these-tags-are-necessary","title":"Why these tags are necessary","text":"<p>Modern LLM-based rerankers are trained using these semantic markers to improve zero-shot accuracy. Omitting these tags or using inconsistent formatting between the query and the documents can lead to sub-optimal relevance scores, as the model might fail to distinguish between the instruction and the data.</p> CURLPython <pre><code>  curl --request POST \\\n    --url https://api.regolo.ai/rerank \\\n    --header 'Authorization: Bearer REGOLO_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n      \"model\": \"Qwen3-Reranker-4B\",\n      \"query\": \"&lt;Instruct&gt;: Given a web search query, retrieve relevant passages that answer the query\\n&lt;Query&gt;: What is the capital of China?\",\n      \"documents\": [\n        \"&lt;Document&gt;: The capital of China is Beijing.\",\n        \"&lt;Document&gt;: Gravity is a force that attracts two bodies towards each other...\"\n      ],\n      \"top_n\": 5\n    }'\n</code></pre> <pre><code>import requests\n\napi_key = \"REGOLO_API_KEY\"\nurl = \"https://api.regolo.ai/rerank\"\n\ntask = \"Given a web search query, retrieve relevant passages that answer the query\"\nquery_text = \"What is the capital of China?\"\n\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other...\"\n]\n\npayload = {\n    \"model\": \"Qwen3-Reranker-4B\",\n    \"query\": f\"&lt;Instruct&gt;: {task}\\n&lt;Query&gt;: {query_text}\",\n    \"documents\": [f\"&lt;Document&gt;: {doc}\" for doc in documents],\n    \"top_n\": 5\n}\n\nresponse = requests.post(\n    url,\n    json=payload,\n    headers={\"Authorization\": f\"Bearer {api_key}\"}\n)\n\nresults = response.json().get('results', [])\nfor res in results:\n    score = res['relevance_score']\n    clean_text = res['document']['text'].replace(\"&lt;Document&gt;: \", \"\")\n    print(f\"Score: {score:.4f} | Text: {clean_text}\")\n</code></pre>"},{"location":"models/families/rerank/#response","title":"Response","text":"<pre><code>{\n  \"id\": \"rerank-8b37844a3beeecb7\",\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.8835278153419495,\n      \"document\": {\n        \"text\": \"&lt;Document&gt;: The capital of China is Beijing.\"\n      }\n    },\n    {\n      \"index\": 1,\n      \"relevance_score\": 0.08649543672800064,\n      \"document\": {\n        \"text\": \"&lt;Document&gt;: Gravity is a force that attracts two bodies towards each other...\"\n      }\n    }\n  ],\n  \"meta\": null\n}\n</code></pre>"},{"location":"models/families/stt/","title":"Speech To Text","text":"<p>The Speech to Text (STT) API enables you to extract and transcribe text from audio files using models such as <code>faster-whisper-large-v3</code>. We recommend using audio chunks of less than 2 minutes to prevent hallucinations and duplicate transcriptions.</p>"},{"location":"models/families/stt/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>file</code>: A binary audio file in OGG format.</li> <li><code>model</code>: The identifier for the model used for transcription, e.g., <code>faster-whisper-large-v3</code>.</li> <li><code>language</code>: A two-letter ISO language code specifying the language of the audio, such as <code>en</code> (English), <code>it</code> (Italian), etc.</li> </ul>"},{"location":"models/families/stt/#important-note","title":"Important Note","text":"<p>The models have a timeout limit. It is recommended to split audio files into smaller segments, such as five-minute clips, to ensure optimal performance.</p>"},{"location":"models/families/stt/#example-requests","title":"Example Requests","text":"Using Regolo ClientOpenAI ClientPythonCURL <pre><code>import regolo\nfrom pathlib import Path\n\n# Regolo configuration\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_audio_transcription_model = \"faster-whisper-large-v3\"\n\n# Audio file to transcribe\nAUDIO_FILE = \"/path/to/your/audio\"\nOUTPUT_FILE = \"/path/to/output/transcription.txt\"\n\n# Transcribe the file\ntranscript = regolo.static_audio_transcription(file=AUDIO_FILE)\n\n# Save the transcription\noutput_path = Path(OUTPUT_FILE)\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(transcript)\n\nprint(f\"Transcription saved to: {OUTPUT_FILE}\")\n</code></pre> <pre><code>import openai\nfrom pathlib import Path\n\n# OpenAI client configuration\nopenai.api_key = \"YOUR_REGOLO_KEY\"\nopenai.base_url = \"https://api.regolo.ai/v1/\"\n\n# Audio file to transcribe\nAUDIO_FILE = \"/path/to/your/audio\"\nOUTPUT_FILE = \"/path/to/output/transcription.txt\"\n\n# Transcribe the file\nwith open(AUDIO_FILE, \"rb\") as audio_file:\n    transcript = openai.audio.transcriptions.create(\n        model=\"faster-whisper-large-v3\",\n        file=audio_file,\n        language=\"en\",\n        response_format=\"text\"\n    )\n\n# Save the transcription\noutput_path = Path(OUTPUT_FILE)\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(transcript)\n\nprint(f\"Transcription saved to: {OUTPUT_FILE}\")\n</code></pre> <pre><code>import requests\nfrom pathlib import Path\n\ndef main():\n    api_url = \"https://api.regolo.ai/v1/audio/transcriptions\"\n    api_key = \"YOUR_REGOLO_KEY\"\n\n    AUDIO_FILE = \"/path/to/your/audio.ogg\"\n\n    audio_path = Path(AUDIO_FILE)\n    if not audio_path.is_file():\n        print(f\"Audio file does not exist: {audio_path}\")\n        return\n\n    headers = {\n        # Don't set Content-Type here; requests will set correct multipart boundary\n        \"Authorization\": f\"Bearer {api_key}\",\n    }\n\n    with audio_path.open(\"rb\") as audio_file:\n        files = {\n            \"file\": (audio_path.name, audio_file, \"application/octet-stream\")\n        }\n        data = {\n            \"model\": \"faster-whisper-large-v3\",\n            \"language\": \"en\",\n            \"response_format\": \"text\",\n        }\n\n        response = requests.post(api_url, headers=headers, data=data, files=files)\n\n    if response.status_code == 200:\n        transcript_text = response.text\n        print(\"=== Transcription ===\")\n        print(transcript_text)\n        print(\"=====================\")\n    else:\n        print(\"Failed transcription request:\")\n        print(\"Status code:\", response.status_code)\n        print(\"Response body:\", response.text)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/audio/transcriptions' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  -F \"file=@/path/to/your/audio\" \\\n  -F \"model=faster-whisper-large-v3\"\n</code></pre>"},{"location":"models/families/stt/#example-implementation","title":"Example Implementation","text":"<p>For a practical example of how to use this API, you can refer to the Telegram Transcriber GitHub Repository. This repository provides a complete implementation for transcribing audio messages from Telegram using the Speech to Text API.</p> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"models/families/vision/","title":"Vision models","text":"<p>Vision completions enable the processing of images alongside text, allowing for a wide range of applications such as image description, object recognition, and data extraction from visual content. By sending a combination of text prompts and image URLs, the model can provide insightful responses based on the visual input.</p>"},{"location":"models/families/vision/#note","title":"Note","text":"<p>This API supports only images, PDF files are not supported.</p>"},{"location":"models/families/vision/#vision-completions","title":"Vision Completions","text":"<p>You can provide images in three ways:</p> <ul> <li>Remote URL: Supply a publicly accessible URL pointing to the image.</li> <li>Base64 (Image from PATH): Read a local image file, encode it as Base64, and send it to the model.</li> <li>Base64 Encoding (Remote URL): Download an image from a URL, encode it as Base64, and send it to the model.</li> </ul>"},{"location":"models/families/vision/#remote-image-url","title":"Remote Image URL","text":"<p>This section demonstrates how to use a remote image URL directly with the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\nprint(regolo.static_chat_completions(messages=[{\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Describe this image in detail.\"\n        },\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                \"format\": \"image/jpeg\"\n            }\n        }\n    ]\n}]))\n</code></pre> <pre><code>import requests\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": \"qwen3-vl-32b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ]\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \\\n-d '{\n    \"model\": \"qwen3-vl-32b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ]\n}'\n</code></pre>"},{"location":"models/families/vision/#base64-image-from-path","title":"Base64 (Image from PATH)","text":"<p>This section demonstrates how to read a local image file from your filesystem, encode it as Base64, and send it to the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nimport base64\nfrom pathlib import Path\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\nIMAGE_PATH = Path(\"beagle-hound-dog.jpg\")\n\nif not IMAGE_PATH.exists():\n    raise FileNotFoundError(f\"Image not found: {IMAGE_PATH.resolve()}\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\nresult = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in detail.\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                    \"format\": \"image/jpeg\"\n                }\n            }\n        ]\n    }],\n    max_tokens=4096,  # Important: maintain a wide output token window to avoid issues\n    full_output=True\n)\n\nprint(\"Model response:\")\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <pre><code>import base64\nimport json\nimport requests\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"qwen3-vl-32b\"\n\nIMAGE_PATH = Path(\"beagle-hound-dog.jpg\")\n\nif not IMAGE_PATH.exists():\n    raise FileNotFoundError(f\"Image not found: {IMAGE_PATH.resolve()}\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096  # Important: maintain a wide output token window to avoid issues\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\nprint(\"Sending request to Regolo AI API...\")\nresponse = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n\nif response.status_code != 200:\n    print(f\"Error {response.status_code}:\")\n    print(response.text)\nelse:\n    result = response.json()\n    try:\n        content = result[\"choices\"][0][\"message\"][\"content\"]\n        print(\"Model response:\")\n        print(content)\n    except Exception:\n        print(\"Unexpected response format:\")\n        print(response.text)\n</code></pre> <pre><code># Encode local image to base64\nIMAGE_B64=$(base64 -w 0 beagle-hound-dog.jpg)\n\ncurl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR-API-KEY\" \\\n-d \"{\n    \\\"model\\\": \\\"qwen3-vl-32b\\\",\n    \\\"messages\\\": [\n        {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": [\n                {\n                    \\\"type\\\": \\\"text\\\",\n                    \\\"text\\\": \\\"Describe this image in detail.\\\"\n                },\n                {\n                    \\\"type\\\": \\\"image_url\\\",\n                    \\\"image_url\\\": {\n                        \\\"url\\\": \\\"data:image/jpeg;base64,$IMAGE_B64\\\",\n                        \\\"format\\\": \\\"image/jpeg\\\"\n                    }\n                }\n            ]\n        }\n    ],\n    \\\"max_tokens\\\": 4096  # Important: maintain a wide output token window to avoid issues\n}\"\n</code></pre>"},{"location":"models/families/vision/#base64-encoding-remote-url","title":"Base64 Encoding (Remote URL)","text":"<p>This section demonstrates how to download an image from a remote URL, encode it as Base64, and send it to the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nimport base64\nimport requests\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\n# Download image and convert to base64\nimage_url = \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\nresult = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in detail.\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                    \"format\": \"image/jpeg\"\n                }\n            }\n        ]\n    }],\n    temperature=0.2,\n    max_tokens=4096,  # Important: maintain a wide output token window to avoid issues\n    full_output=True\n)\n\nprint(\"Model response:\")\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <pre><code>import base64\nimport requests\nimport json\n\n# Regolo API credentials configuration\napi_key = \"YOUR_API_KEY\"\nmodel = \"qwen3-vl-32b\"  # Vision-compatible model\n\n# Download image and convert to base64\nimage_url = \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\n# Direct API call for multimodal messages\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": model,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"temperature\": 0.2,\n    \"max_tokens\": 4096  # Important: maintain a wide output token window to avoid issues\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\nprint(\"Sending request to Regolo AI API...\")\nresponse = requests.post(url, json=payload, headers=headers)\n\nif response.status_code == 200:\n    result = response.json()\n    try:\n        content = result[\"choices\"][0][\"message\"][\"content\"]\n        print(\"Model response:\")\n        print(content)\n    except KeyError as e:\n        print(f\"KeyError: {e}\")\n        print(\"Response structure:\")\n        print(json.dumps(result, indent=2))\nelse:\n    print(f\"Error {response.status_code}:\")\n    print(response.text)\n</code></pre> <pre><code># First, download the image and encode it to base64\n# This example assumes you have the image URL\nIMAGE_URL=\"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nIMAGE_B64=$(curl -s \"$IMAGE_URL\" | base64 -w 0)\n\ncurl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_API_KEY\" \\\n-d \"{\n    \\\"model\\\": \\\"qwen3-vl-32b\\\",\n    \\\"messages\\\": [\n        {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": [\n                {\n                    \\\"type\\\": \\\"text\\\",\n                    \\\"text\\\": \\\"Describe this image in detail.\\\"\n                },\n                {\n                    \\\"type\\\": \\\"image_url\\\",\n                    \\\"image_url\\\": {\n                        \\\"url\\\": \\\"data:image/jpeg;base64,$IMAGE_B64\\\",\n                        \\\"format\\\": \\\"image/jpeg\\\"\n                    }\n                }\n            ]\n        }\n    ],\n    \\\"temperature\\\": 0.2,\n    \\\"max_tokens\\\": 4096  # Important: maintain a wide output token window to avoid issues\n}\"\n</code></pre>"},{"location":"models/features/function-calling/","title":"Function Calling","text":"<p>Function calling enables the language model to invoke external functions during a conversation, allowing it to retrieve real-time information or perform operations.</p>"},{"location":"models/features/function-calling/#supported-models","title":"Supported Models","text":"<p>The following models support function calling:</p> <ul> <li><code>deepseek-r1-70b</code></li> <li><code>gpt-oss-120b</code></li> <li><code>Llama-3.3-70B-Instruct</code></li> <li><code>mistral-small3.2</code></li> <li><code>qwen3-30b</code></li> <li><code>qwen3-coder-30b</code></li> </ul>"},{"location":"models/features/function-calling/#web-search-example","title":"Web Search Example","text":"<p>Here's a complete example using DuckDuckGo web search with the Regolo API:</p> <pre><code>from ddgs import DDGS\nimport logging\nimport os\nimport requests\nimport json\n\nlogger = logging.getLogger(__name__)\n\nREGOLO_BASE_URL = \"https://api.regolo.ai/v1\"\nAPI_KEY = os.getenv(\"REGOLO_API_KEY\", \"YOUR_REGOLO_KEY\")\n\n\ndef duckduckgo_search(query: str, max_results: int = 10, region: str = \"wt-wt\") -&gt; str:\n    \"\"\"\n    Search the web using DuckDuckGo and return formatted results.\n\n    Args:\n        query: Search query string\n        max_results: Maximum number of results (default: 10)\n        region: Region code for localized results (default: \"wt-wt\" for worldwide)\n\n    Returns:\n        Formatted markdown string with search results\n    \"\"\"\n    try:\n        logger.info(f\"DUCKDUCKGO: Searching for '{query}' (max_results={max_results})\")\n\n        with DDGS() as ddgs:\n            # ddgs package uses 'query' parameter\n            results = list(ddgs.text(query=query, max_results=max_results, region=region))\n\n        if not results:\n            return \"No search results found.\"\n\n        # Format results as markdown\n        formatted = \"## Search Results\\n\\n\"\n        for result in results:\n            title = result.get(\"title\", \"No title\")\n            url = result.get(\"href\", \"\")\n            body = result.get(\"body\", \"\")\n\n            formatted += f\"[{title}]({url})\\n\"\n            formatted += f\"{body}\\n\\n\"\n\n        logger.info(f\"DUCKDUCKGO: Found {len(results)} results\")\n        return formatted.strip()\n\n    except Exception as e:\n        logger.error(f\"DUCKDUCKGO: Error: {e}\")\n        return f\"Error performing search: {str(e)}\"\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"duckduckgo_search\",\n            \"description\": (\n                \"Search the web for current information using DuckDuckGo. \"\n                \"Use this when you need up-to-date information, news, facts, \"\n                \"or any information that may change over time.\"\n            ),\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The search query string\"\n                    },\n                    \"max_results\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Maximum number of results to return (1-20)\",\n                        \"minimum\": 1,\n                        \"maximum\": 20,\n                        \"default\": 10\n                    },\n                    \"region\": {\n                        \"type\": \"string\",\n                        \"description\": \"Region code for localized results (e.g., 'wt-wt' for worldwide, 'us-en' for US English)\",\n                        \"default\": \"wt-wt\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }\n]\n</code></pre> Non-StreamingStreaming CleanFull <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with updated messages\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload\n            )\n\n            followup_response.raise_for_status()\n            final_answer = followup_response.json()[\"choices\"][0][\"message\"][\"content\"]\n            print(final_answer)\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre> <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\n# Show full JSON response\nprint(response.json())\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with updated messages\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload\n            )\n\n            followup_response.raise_for_status()\n            # Show full JSON response\n            print(followup_response.json())\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre> CleanFull <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with streaming enabled\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages,\n                \"stream\": True\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload,\n                stream=True\n            )\n\n            # Extract only content from streamed JSON\n            for line in followup_response.iter_lines():\n                if line:\n                    decoded_line = line.decode('utf-8')\n                    if decoded_line.startswith('data: '):\n                        json_str = decoded_line[6:]\n                        if json_str.strip() == '[DONE]':\n                            break\n                        try:\n                            json_data = json.loads(json_str)\n                            if 'choices' in json_data:\n                                delta = json_data['choices'][0].get('delta', {})\n                                content = delta.get('content', '')\n                                if content:\n                                    print(content, end='', flush=True)\n                        except json.JSONDecodeError:\n                            pass\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre> <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\n# Show full JSON response\nprint(response.json())\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with streaming enabled\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages,\n                \"stream\": True\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload,\n                stream=True\n            )\n\n            # Show full JSON response\n            for line in followup_response.iter_lines():\n                if line:\n                    print(line.decode('utf-8'))\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre>"},{"location":"models/features/function-calling/#tool-choice","title":"Tool Choice","text":"<p>By default, the model will determine when and how many tools to use. You can force specific behavior with the <code>tool_choice</code> parameter.</p>"},{"location":"models/features/function-calling/#auto-default","title":"Auto (Default)","text":"<p>The model decides whether to call zero, one, or multiple functions:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"  # Default behavior\n}\n</code></pre>"},{"location":"models/features/function-calling/#required","title":"Required","text":"<p>Force the model to call one or more functions:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"required\"\n}\n</code></pre>"},{"location":"models/features/function-calling/#forced-function","title":"Forced Function","text":"<p>Force the model to call exactly one specific function:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\"name\": \"duckduckgo_search\"}\n    }\n}\n</code></pre>"},{"location":"models/features/function-calling/#allowed-tools","title":"Allowed Tools","text":"<p>Restrict the tool calls the model can make to a subset of the tools available. This is useful when you want to make only a subset of tools available across model requests without modifying the list of tools you pass in, maximizing savings from prompt caching.</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,  # All available tools\n    \"tool_choice\": {\n        \"type\": \"allowed_tools\",\n        \"mode\": \"auto\",\n        \"tools\": [\n            {\"type\": \"function\", \"name\": \"duckduckgo_search\"}\n        ]\n    }\n}\n</code></pre>"},{"location":"models/features/function-calling/#none","title":"None","text":"<p>Disable function calling entirely, imitating the behavior of passing no functions:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"none\"\n}\n</code></pre>"},{"location":"models/features/function-calling/#further-reading","title":"Further Reading","text":"<ul> <li>OpenAI Function Calling docs</li> </ul>"},{"location":"models/features/response-parameters/","title":"Response Parameters","text":"<p>The Regolo API follows the OpenAI Chat Completions API specification and supports standard parameters that control the behavior and quality of model responses.</p>"},{"location":"models/features/response-parameters/#overview","title":"Overview","text":"<p>These parameters allow you to fine-tune how the model generates responses, controlling aspects like creativity, length, diversity, and repetition.</p>"},{"location":"models/features/response-parameters/#standard-parameters","title":"Standard Parameters","text":""},{"location":"models/features/response-parameters/#temperature","title":"<code>temperature</code>","text":"<p>Controls the randomness of the model's output. </p> <ul> <li>Type: <code>float</code></li> <li>Range: 0.0 to 2.0</li> <li>Default: 1.0</li> </ul> <p>Lower values (e.g., 0.2) make the output more deterministic and focused, while higher values (e.g., 0.8) increase creativity and variability.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me a story\"}\n    ],\n    \"temperature\": 0.7  # Balanced creativity\n}\n</code></pre>"},{"location":"models/features/response-parameters/#max_tokens","title":"<code>max_tokens</code>","text":"<p>Sets the maximum number of tokens that can be generated in the response.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: Varies by model</li> </ul> <p>Higher values allow for longer responses but may increase latency and costs. Lower values provide more concise outputs.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ],\n    \"max_tokens\": 1000  # Limit response length\n}\n</code></pre>"},{"location":"models/features/response-parameters/#top_p","title":"<code>top_p</code>","text":"<p>Nucleus sampling parameter that controls the diversity of tokens considered.</p> <ul> <li>Type: <code>float</code></li> <li>Range: 0.0 to 1.0</li> <li>Default: 1.0</li> </ul> <p>Lower values make the model more focused on likely tokens, while higher values allow more diverse choices.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a creative poem\"}\n    ],\n    \"top_p\": 0.9  # Allow more diverse word choices\n}\n</code></pre>"},{"location":"models/features/response-parameters/#frequency_penalty","title":"<code>frequency_penalty</code>","text":"<p>Reduces the likelihood of the model repeating frequent tokens.</p> <ul> <li>Type: <code>float</code></li> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> </ul> <p>Positive values reduce repetition, while negative values increase it. Useful for avoiding repetitive phrases.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"List 10 different ideas\"}\n    ],\n    \"frequency_penalty\": 0.5  # Reduce repetition\n}\n</code></pre>"},{"location":"models/features/response-parameters/#presence_penalty","title":"<code>presence_penalty</code>","text":"<p>Reduces the likelihood of the model reusing tokens that are already present in the text.</p> <ul> <li>Type: <code>float</code></li> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> </ul> <p>Positive values encourage the model to use new tokens and explore different approaches.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Generate diverse solutions\"}\n    ],\n    \"presence_penalty\": 0.6  # Encourage new approaches\n}\n</code></pre>"},{"location":"models/features/response-parameters/#stop","title":"<code>stop</code>","text":"<p>Array of strings that cause the model to stop generating when encountered.</p> <ul> <li>Type: <code>array[string]</code> or <code>string</code></li> <li>Default: <code>null</code></li> </ul> <p>Useful for controlling where the response ends or preventing certain phrases.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Count to 10\"}\n    ],\n    \"stop\": [\"11\", \"twelve\"]  # Stop at these sequences\n}\n</code></pre>"},{"location":"models/features/response-parameters/#n","title":"<code>n</code>","text":"<p>Number of completions to generate for each prompt.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: 1</li> </ul> <p>Generates multiple response variations.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Suggest a name\"}\n    ],\n    \"n\": 3  # Generate 3 different suggestions\n}\n</code></pre>"},{"location":"models/features/response-parameters/#seed","title":"<code>seed</code>","text":"<p>Seed for reproducible outputs.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: <code>null</code></li> </ul> <p>When set, the model will produce more deterministic outputs, useful for testing and reproducibility.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Generate a random number\"}\n    ],\n    \"seed\": 42  # Reproducible output\n}\n</code></pre>"},{"location":"models/features/response-parameters/#complete-example","title":"Complete Example","text":"<pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a creative story\"}\n    ],\n    \"temperature\": 0.8,\n    \"max_tokens\": 500,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.3,\n    \"presence_penalty\": 0.4\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nresult = response.json()\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"models/features/response-parameters/#best-practices","title":"Best Practices","text":"<ul> <li>For factual tasks: Use lower <code>temperature</code> (0.2-0.4) for more deterministic outputs</li> <li>For creative tasks: Use higher <code>temperature</code> (0.7-0.9) for more varied responses</li> <li>For long responses: Set appropriate <code>max_tokens</code> to avoid truncation</li> <li>To reduce repetition: Use <code>frequency_penalty</code> (0.3-0.7) and <code>presence_penalty</code> (0.4-0.6)</li> <li>For consistency: Use <code>seed</code> when you need reproducible outputs</li> </ul>"},{"location":"models/features/thinking/","title":"Thinking","text":"<p>Thinking is a feature that allows models to reason through problems step by step, showing their internal thought process before providing a final answer.</p>"},{"location":"models/features/thinking/#overview","title":"Overview","text":"<p>The thinking feature enables models to break down complex problems into smaller steps, making their reasoning process transparent and allowing for better understanding of how they arrive at their conclusions.</p>"},{"location":"models/features/thinking/#usage","title":"Usage","text":"<p>To enable thinking, you can use the <code>thinking</code> parameter in your API requests.</p> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What color was Napoleon's white horse?\"}\n    ],\n    \"thinking\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nresult = response.json()\n\n# Extract main content and reasoning\nmessage = result.get(\"choices\", [{}])[0].get(\"message\", {})\ncontent = message.get(\"content\", \"\")\nreasoning = message.get(\"reasoning_content\", \"\")\n\nif reasoning:\n    print(\"=== Reasoning ===\")\n    print(reasoning)\n    print(\"\\n=== Final Answer ===\")\n\nprint(content)\n</code></pre>"},{"location":"models/features/thinking/#parameters","title":"Parameters","text":""},{"location":"models/features/thinking/#reasoning_effort","title":"<code>reasoning_effort</code>","text":"<p>Controls the depth and detail of the reasoning process. Available values:</p> <ul> <li><code>low</code>: Minimal reasoning effort, faster responses with brief reasoning</li> <li><code>medium</code>: Balanced reasoning effort (default)</li> <li><code>high</code>: Maximum reasoning effort, more detailed and thorough reasoning</li> </ul> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Solve this step by step: What is 15% of 240?\"}\n    ],\n    \"thinking\": True,\n    \"reasoning_effort\": \"high\"  # low, medium, or high\n}\n</code></pre> <p>Standard Parameters</p> <p>Standard API parameters like <code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>, <code>frequency_penalty</code>, and <code>presence_penalty</code> can also influence the thinking process. See Response Parameters for detailed documentation.</p>"},{"location":"models/features/thinking/#benefits","title":"Benefits","text":"<ul> <li>Transparency: See how the model reasons through problems</li> <li>Debugging: Understand where the model might make mistakes</li> <li>Education: Learn problem-solving strategies from the model's reasoning</li> <li>Quality: Better results for complex, multi-step problems</li> </ul>"}]}