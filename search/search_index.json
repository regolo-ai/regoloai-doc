{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Documentation","text":"<p>Regolo lets you run AI models with a cloud API, without having to understand machine learning or manage your own infrastructure.</p> <p>You can use chat completions, vision models, image generation, and more.</p>"},{"location":"#get-started","title":"Get started","text":"-   :material-language-python:{ .lg .middle } **Run a model from Python**      ---      The language of the machine learning world.      [:octicons-arrow-right-24: Quick Start](getting-started/quick-start.md)  -   :material-language-javascript:{ .lg .middle } **Run a model from Node.js**      ---      Get started with a few lines of JavaScript.      [:octicons-arrow-right-24: Choose Language](getting-started/choose-language.md)  -   :material-console:{ .lg .middle } **Run a model from cURL**      ---      Test the API directly from your terminal.      [:octicons-arrow-right-24: First API Call](getting-started/first-api-call.md)  -   :material-key:{ .lg .middle } **Get your API Key**      ---      Sign up and create your first API key.      [:octicons-arrow-right-24: Sign Up](getting-started/sign-up.md)"},{"location":"#models","title":"Models","text":"-   :material-chat:{ .lg .middle } **Completions &amp; Chat**      ---      Text generation and conversational AI models.      [:octicons-arrow-right-24: Learn more](core-features/inference-api/completions-and-chat.md)  -   :material-eye:{ .lg .middle } **Vision**      ---      Image understanding and analysis.      [:octicons-arrow-right-24: Learn more](core-features/inference-api/vision-analysis.md)  -   :material-image:{ .lg .middle } **Image Generation**      ---      Create images from text prompts.      [:octicons-arrow-right-24: Learn more](core-features/inference-api/image-generation.md)  -   :material-microphone:{ .lg .middle } **Speech to Text**      ---      Transcribe audio to text.      [:octicons-arrow-right-24: Learn more](core-features/inference-api/speech-to-text.md)  -   :material-vector-combine:{ .lg .middle } **Embeddings**      ---      Convert text to vector representations.      [:octicons-arrow-right-24: Learn more](core-features/inference-api/embedding.md)  -   :material-sort-variant:{ .lg .middle } **Rerank**      ---      Reorder search results by relevance.      [:octicons-arrow-right-24: Learn more](core-features/inference-api/rerank.md)"},{"location":"#custom-models","title":"Custom Models","text":"-   :material-plus:{ .lg .middle } **Deploy Hugging Face Models**      ---      Add any model from Hugging Face and deploy it with your choice of GPU instance.      [:octicons-arrow-right-24: Get Started](core-features/model-management/custom-endpoints.md)  -   :material-folder-open:{ .lg .middle } **Model Management**      ---      Organize, version, and manage your models with fine-tuning and custom endpoints.      [:octicons-arrow-right-24: Learn more](core-features/model-management/overview.md)"},{"location":"#features","title":"Features","text":"-   :material-brain:{ .lg .middle } **Thinking**      ---      Enable reasoning capabilities in models.      [:octicons-arrow-right-24: Learn more](core-features/advanced/thinking.md)  -   :material-tune:{ .lg .middle } **Response Parameters**      ---      Control model output with parameters.      [:octicons-arrow-right-24: Learn more](core-features/advanced/response-parameters.md)"},{"location":"#resources","title":"Resources","text":"-   :material-api:{ .lg .middle } **API Reference**      ---      Complete API documentation with Swagger UI.      [:octicons-arrow-right-24: API Docs](https://docs.api.regolo.ai)  -   :material-view-list:{ .lg .middle } **Model Catalog**      ---      Browse all available models.      [:octicons-arrow-right-24: Catalog](catalog.md)  -   :material-chart-line:{ .lg .middle } **Observability**      ---      Monitor your API usage and performance.      [:octicons-arrow-right-24: Status](https://obs.regolo.ai/)  -   :material-flask:{ .lg .middle } **Playground**      ---      Test models interactively in the browser.      [:octicons-arrow-right-24: Try it](getting-started/sandbox.md)"},{"location":"catalog/","title":"Regolo Catalog","text":"<p>The Regolo Catalog is the official catalog of available models on the  Regolo AI platform. It provides an up-to-date list of all models that can be  used with the Regolo API.</p>"},{"location":"catalog/#overview","title":"Overview","text":"<p>The catalog is automatically updated every 24 hours by fetching the latest list  of available models from the Regolo API. This ensures that all applications  using Regolo models always have access to the most up-to-date model list.</p>"},{"location":"catalog/#accessing-the-catalog","title":"Accessing the Catalog","text":""},{"location":"catalog/#repository","title":"Repository","text":"<p>Visit the official Regolo Model Catalog repository on GitHub:</p> <p>\ud83d\udd17 regolo-ai/regolo-model-catalog</p>"},{"location":"catalog/#list-available-models","title":"List Available Models","text":"<p>You can retrieve the list of available models directly from the Regolo API  using the <code>/models</code> endpoint:</p> PythonCURL <pre><code>import requests\n\nurl = \"https://api.regolo.ai/models\"\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    models = response.json()\n    print(models)\nelse:\n    print(\"Failed to fetch models:\", response.status_code, \n          response.text)\n</code></pre> <pre><code>curl -X GET https://api.regolo.ai/models\n</code></pre>"},{"location":"catalog/#request-a-model","title":"Request a Model","text":"<p>Want to see a specific model on Regolo? Let us know! You can:</p> <ul> <li>Open an issue on the catalog repository</li> <li>Start a discussion</li> <li>Contact us at help@regolo.ai</li> </ul>"},{"location":"core-features/advanced/cache-optimization/","title":"Cache &amp; Optimization","text":"<p>Optimize performance and reduce costs with intelligent caching strategies.</p>"},{"location":"core-features/advanced/cache-optimization/#cache-types","title":"Cache Types","text":""},{"location":"core-features/advanced/cache-optimization/#request-caching","title":"Request Caching","text":"<p>Automatically cache identical requests and return cached results:</p> <pre><code>import regolo\n\nresponse = regolo.static_chat_completions(\n    messages=[{\"role\": \"user\", \"content\": \"What is 2+2?\"}],\n    cache_control=\"max-age=3600\"  # Cache for 1 hour\n)\n</code></pre> <p>Benefits: - Reduce API costs for repeated queries - Instant response times for cached results - Reduced server load</p>"},{"location":"core-features/advanced/cache-optimization/#prompt-caching","title":"Prompt Caching","text":"<p>Cache expensive prompt prefixes to optimize costs:</p> <pre><code>response = regolo.static_chat_completions(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant...\"},  # Cached\n        {\"role\": \"user\", \"content\": \"User question\"}\n    ],\n    cache_prompt=True\n)\n</code></pre> <p>Use cases: - Long system prompts (documentation, templates) - Multi-turn conversations - Batch processing with common prefixes</p>"},{"location":"core-features/advanced/cache-optimization/#cache-configuration","title":"Cache Configuration","text":""},{"location":"core-features/advanced/cache-optimization/#ttl-time-to-live","title":"TTL (Time to Live)","text":"<pre><code># Cache for 1 hour\nresponse = regolo.static_chat_completions(\n    messages=messages,\n    cache_ttl=3600\n)\n\n# Cache until manual expiration\nresponse = regolo.static_chat_completions(\n    messages=messages,\n    cache_ttl=\"forever\"\n)\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#cache-keys","title":"Cache Keys","text":"<p>Customize cache key strategy:</p> <pre><code>response = regolo.static_chat_completions(\n    messages=messages,\n    cache_key=\"user_123:conversation_456\"\n)\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#cache-statistics","title":"Cache Statistics","text":"<p>Monitor cache performance:</p> <pre><code>response = regolo.static_chat_completions(messages=messages)\n\nprint(f\"Cache hit: {response.cache_hit}\")\nprint(f\"Cache age: {response.cache_age}\")\nprint(f\"Tokens saved: {response.cache_tokens_saved}\")\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#cost-optimization","title":"Cost Optimization","text":""},{"location":"core-features/advanced/cache-optimization/#before-caching","title":"Before Caching","text":"<pre><code>1000 requests \u00d7 50 tokens = 50,000 tokens\nCost: $0.50\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#after-caching-90-hit-rate","title":"After Caching (90% hit rate)","text":"<pre><code>100 new requests \u00d7 50 tokens = 5,000 tokens\n900 cached requests = 0 tokens\nCost: $0.05 (90% reduction)\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#best-practices","title":"Best Practices","text":"<ol> <li>Cache high-frequency queries: Focus on queries executed many times</li> <li>Use appropriate TTL: Balance freshness vs. cost savings</li> <li>Monitor hit rates: Track cache effectiveness</li> <li>Segment by user: Separate caches per user when appropriate</li> <li>Test thoroughly: Ensure cached responses meet quality standards</li> </ol>"},{"location":"core-features/advanced/cache-optimization/#advanced-optimization","title":"Advanced Optimization","text":""},{"location":"core-features/advanced/cache-optimization/#batch-processing","title":"Batch Processing","text":"<pre><code>requests = [\n    {\"role\": \"user\", \"content\": \"Query 1\"},\n    {\"role\": \"user\", \"content\": \"Query 2\"},\n    # ...\n]\n\nresponses = regolo.batch_completions(\n    requests=requests,\n    enable_cache=True\n)\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#distributed-caching","title":"Distributed Caching","text":"<p>For multi-region deployments:</p> <pre><code>response = regolo.static_chat_completions(\n    messages=messages,\n    cache_strategy=\"distributed\",  # Shared across regions\n    cache_ttl=7200\n)\n</code></pre>"},{"location":"core-features/advanced/cache-optimization/#cache-invalidation","title":"Cache Invalidation","text":"<p>Manually clear cache when needed:</p> <pre><code># Clear all caches\nregolo.clear_cache()\n\n# Clear specific cache\nregolo.clear_cache(key=\"user_123:*\")\n\n# Clear by age\nregolo.clear_cache(older_than=3600)  # Older than 1 hour\n</code></pre>"},{"location":"core-features/advanced/function-calling/","title":"Function Calling","text":"<p>Function calling enables the language model to invoke external functions during a conversation, allowing it to retrieve real-time information or perform operations.</p>"},{"location":"core-features/advanced/function-calling/#supported-models","title":"Supported Models","text":"<p>The following models support function calling:</p> <ul> <li><code>deepseek-r1-70b</code></li> <li><code>gpt-oss-120b</code></li> <li><code>Llama-3.3-70B-Instruct</code></li> <li><code>mistral-small3.2</code></li> <li><code>qwen3-30b</code></li> <li><code>qwen3-coder-30b</code></li> </ul>"},{"location":"core-features/advanced/function-calling/#web-search-example","title":"Web Search Example","text":"<p>Here's a complete example using DuckDuckGo web search with the Regolo API:</p> <pre><code>from ddgs import DDGS\nimport logging\nimport os\nimport requests\nimport json\n\nlogger = logging.getLogger(__name__)\n\nREGOLO_BASE_URL = \"https://api.regolo.ai/v1\"\nAPI_KEY = os.getenv(\"REGOLO_API_KEY\", \"YOUR_REGOLO_KEY\")\n\n\ndef duckduckgo_search(query: str, max_results: int = 10, region: str = \"wt-wt\") -&gt; str:\n    \"\"\"\n    Search the web using DuckDuckGo and return formatted results.\n\n    Args:\n        query: Search query string\n        max_results: Maximum number of results (default: 10)\n        region: Region code for localized results (default: \"wt-wt\" for worldwide)\n\n    Returns:\n        Formatted markdown string with search results\n    \"\"\"\n    try:\n        logger.info(f\"DUCKDUCKGO: Searching for '{query}' (max_results={max_results})\")\n\n        with DDGS() as ddgs:\n            # ddgs package uses 'query' parameter\n            results = list(ddgs.text(query=query, max_results=max_results, region=region))\n\n        if not results:\n            return \"No search results found.\"\n\n        # Format results as markdown\n        formatted = \"## Search Results\\n\\n\"\n        for result in results:\n            title = result.get(\"title\", \"No title\")\n            url = result.get(\"href\", \"\")\n            body = result.get(\"body\", \"\")\n\n            formatted += f\"[{title}]({url})\\n\"\n            formatted += f\"{body}\\n\\n\"\n\n        logger.info(f\"DUCKDUCKGO: Found {len(results)} results\")\n        return formatted.strip()\n\n    except Exception as e:\n        logger.error(f\"DUCKDUCKGO: Error: {e}\")\n        return f\"Error performing search: {str(e)}\"\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"duckduckgo_search\",\n            \"description\": (\n                \"Search the web for current information using DuckDuckGo. \"\n                \"Use this when you need up-to-date information, news, facts, \"\n                \"or any information that may change over time.\"\n            ),\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The search query string\"\n                    },\n                    \"max_results\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Maximum number of results to return (1-20)\",\n                        \"minimum\": 1,\n                        \"maximum\": 20,\n                        \"default\": 10\n                    },\n                    \"region\": {\n                        \"type\": \"string\",\n                        \"description\": \"Region code for localized results (e.g., 'wt-wt' for worldwide, 'us-en' for US English)\",\n                        \"default\": \"wt-wt\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }\n]\n</code></pre> Non-StreamingStreaming CleanFull <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with updated messages\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload\n            )\n\n            followup_response.raise_for_status()\n            final_answer = followup_response.json()[\"choices\"][0][\"message\"][\"content\"]\n            print(final_answer)\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre> <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\n# Show full JSON response\nprint(response.json())\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with updated messages\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload\n            )\n\n            followup_response.raise_for_status()\n            # Show full JSON response\n            print(followup_response.json())\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre> CleanFull <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with streaming enabled\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages,\n                \"stream\": True\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload,\n                stream=True\n            )\n\n            # Extract only content from streamed JSON\n            for line in followup_response.iter_lines():\n                if line:\n                    decoded_line = line.decode('utf-8')\n                    if decoded_line.startswith('data: '):\n                        json_str = decoded_line[6:]\n                        if json_str.strip() == '[DONE]':\n                            break\n                        try:\n                            json_data = json.loads(json_str)\n                            if 'choices' in json_data:\n                                delta = json_data['choices'][0].get('delta', {})\n                                content = delta.get('content', '')\n                                if content:\n                                    print(content, end='', flush=True)\n                        except json.JSONDecodeError:\n                            pass\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre> <pre><code># Create messages list\nmessages = [\n    {\"role\": \"user\", \"content\": \"Find me some news about AI\"}\n]\n\n# Initial request with tools\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\npayload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"\n}\n\nresponse = requests.post(\n    f\"{REGOLO_BASE_URL}/chat/completions\",\n    headers=headers,\n    json=payload\n)\n\n# Show full JSON response\nprint(response.json())\n\nmessage = response.json()[\"choices\"][0][\"message\"]\n\n# Handle function call\nif message.get(\"tool_calls\"):\n    for tool_call in message[\"tool_calls\"]:\n        if tool_call[\"function\"][\"name\"] == \"duckduckgo_search\":\n            args = json.loads(tool_call[\"function\"][\"arguments\"])\n            search_results = duckduckgo_search(\n                query=args.get(\"query\"),\n                max_results=args.get(\"max_results\", 10),\n                region=args.get(\"region\", \"wt-wt\")\n            )\n\n            # Add assistant message with tool_calls and tool response\n            messages.append(message)\n            messages.append({\n                \"role\": \"tool\",\n                \"tool_call_id\": tool_call[\"id\"],\n                \"name\": tool_call[\"function\"][\"name\"],\n                \"content\": search_results\n            })\n\n            # Make follow-up call with streaming enabled\n            followup_payload = {\n                \"model\": \"gpt-oss-120b\",\n                \"messages\": messages,\n                \"stream\": True\n            }\n\n            followup_response = requests.post(\n                f\"{REGOLO_BASE_URL}/chat/completions\",\n                headers=headers,\n                json=followup_payload,\n                stream=True\n            )\n\n            # Show full JSON response\n            for line in followup_response.iter_lines():\n                if line:\n                    print(line.decode('utf-8'))\nelse:\n    print(message.get(\"content\", \"\"))\n</code></pre>"},{"location":"core-features/advanced/function-calling/#tool-choice","title":"Tool Choice","text":"<p>By default, the model will determine when and how many tools to use. You can force specific behavior with the <code>tool_choice</code> parameter.</p>"},{"location":"core-features/advanced/function-calling/#auto-default","title":"Auto (Default)","text":"<p>The model decides whether to call zero, one, or multiple functions:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"auto\"  # Default behavior\n}\n</code></pre>"},{"location":"core-features/advanced/function-calling/#required","title":"Required","text":"<p>Force the model to call one or more functions:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"required\"\n}\n</code></pre>"},{"location":"core-features/advanced/function-calling/#forced-function","title":"Forced Function","text":"<p>Force the model to call exactly one specific function:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": {\n        \"type\": \"function\",\n        \"function\": {\"name\": \"duckduckgo_search\"}\n    }\n}\n</code></pre>"},{"location":"core-features/advanced/function-calling/#allowed-tools","title":"Allowed Tools","text":"<p>Restrict the tool calls the model can make to a subset of the tools available. This is useful when you want to make only a subset of tools available across model requests without modifying the list of tools you pass in, maximizing savings from prompt caching.</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,  # All available tools\n    \"tool_choice\": {\n        \"type\": \"allowed_tools\",\n        \"mode\": \"auto\",\n        \"tools\": [\n            {\"type\": \"function\", \"name\": \"duckduckgo_search\"}\n        ]\n    }\n}\n</code></pre>"},{"location":"core-features/advanced/function-calling/#none","title":"None","text":"<p>Disable function calling entirely, imitating the behavior of passing no functions:</p> <pre><code>payload = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": messages,\n    \"tools\": tools,\n    \"tool_choice\": \"none\"\n}\n</code></pre>"},{"location":"core-features/advanced/function-calling/#further-reading","title":"Further Reading","text":"<ul> <li>OpenAI Function Calling docs</li> </ul>"},{"location":"core-features/advanced/overview/","title":"Advanced Features","text":"<p>Unlock powerful advanced capabilities for enterprise and production needs.</p> <p>Regolo provides a suite of enterprise-grade features designed to optimize performance, security, and cost.</p>"},{"location":"core-features/advanced/overview/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Private Endpoints: Dedicated, isolated infrastructure for maximum security</li> <li>Enterprise SLA: Guaranteed uptime and response times with dedicated support</li> <li>Webhook Integration: Real-time event notifications and integrations</li> <li>Caching &amp; Optimization: Reduce latency and costs with intelligent caching</li> </ul>"},{"location":"core-features/advanced/overview/#when-to-use-advanced-features","title":"When to Use Advanced Features","text":"<p>Advanced features are ideal for:</p> <ul> <li>Enterprise customers requiring guaranteed uptime and performance</li> <li>High-volume applications needing cost optimization</li> <li>Security-sensitive workloads requiring isolated infrastructure</li> <li>Real-time systems needing webhook-based event notifications</li> </ul>"},{"location":"core-features/advanced/private-endpoints/","title":"Private Endpoints","text":"<p>Deploy private endpoints for enhanced security and control.</p>"},{"location":"core-features/advanced/private-endpoints/#overview","title":"Overview","text":"<p>Private Endpoints provide dedicated, isolated API endpoints running on your own infrastructure or Regolo's isolated environment. This ensures your API traffic never traverses the public internet.</p>"},{"location":"core-features/advanced/private-endpoints/#benefits","title":"Benefits","text":"<ul> <li>Data Isolation: Dedicated infrastructure with no shared resources</li> <li>Network Security: VPC/VPN integration options available</li> <li>Compliance: Meet regulatory requirements for data residency</li> <li>Performance: Dedicated bandwidth and processing capacity</li> <li>Control: Full control over access, authentication, and rate limiting</li> </ul>"},{"location":"core-features/advanced/private-endpoints/#deployment-options","title":"Deployment Options","text":""},{"location":"core-features/advanced/private-endpoints/#option-1-regolo-hosted-private-endpoint","title":"Option 1: Regolo-Hosted Private Endpoint","text":"<p>Regolo manages the infrastructure on isolated, dedicated hardware:</p> <pre><code>Endpoint: https://private-&lt;org-id&gt;.regolo.ai/v1\nAccess: IP whitelisting, API key authentication\n</code></pre>"},{"location":"core-features/advanced/private-endpoints/#option-2-vpc-endpoint-aws","title":"Option 2: VPC Endpoint (AWS)","text":"<p>Integrate directly with your AWS VPC:</p> <pre><code># AWS PrivateLink configuration\nService Name: com.amazonaws.regolo.vpce.us-east-1\n</code></pre>"},{"location":"core-features/advanced/private-endpoints/#setup","title":"Setup","text":"<ol> <li>Contact enterprise support to enable Private Endpoints</li> <li>Configure network access policies</li> <li>Update your application endpoints</li> <li>Test connectivity in staging environment</li> <li>Deploy to production</li> </ol>"},{"location":"core-features/advanced/private-endpoints/#authentication","title":"Authentication","text":"<p>Private endpoints support the same authentication methods as public endpoints:</p> API KeyOAuth 2.0 <pre><code>import regolo\n\nregolo.default_key = \"YOUR_API_KEY\"\nregolo.base_url = \"https://private-&lt;org-id&gt;.regolo.ai\"\n</code></pre> <pre><code>from requests_oauthlib import OAuth2Session\n\nclient = OAuth2Session(\n    client_id,\n    token=token,\n    redirect_uri=\"https://private-&lt;org-id&gt;.regolo.ai/callback\"\n)\n</code></pre>"},{"location":"core-features/advanced/private-endpoints/#monitoring","title":"Monitoring","text":"<p>Monitor your private endpoint usage:</p> <ul> <li>Real-time request metrics</li> <li>Bandwidth monitoring</li> <li>Error rate tracking</li> <li>IP access logs</li> </ul>"},{"location":"core-features/advanced/response-parameters/","title":"Response Parameters","text":"<p>The Regolo API follows the OpenAI Chat Completions API specification and supports standard parameters that control the behavior and quality of model responses.</p>"},{"location":"core-features/advanced/response-parameters/#overview","title":"Overview","text":"<p>These parameters allow you to fine-tune how the model generates responses, controlling aspects like creativity, length, diversity, and repetition.</p>"},{"location":"core-features/advanced/response-parameters/#standard-parameters","title":"Standard Parameters","text":""},{"location":"core-features/advanced/response-parameters/#temperature","title":"<code>temperature</code>","text":"<p>Controls the randomness of the model's output. </p> <ul> <li>Type: <code>float</code></li> <li>Range: 0.0 to 2.0</li> <li>Default: 1.0</li> </ul> <p>Lower values (e.g., 0.2) make the output more deterministic and focused, while higher values (e.g., 0.8) increase creativity and variability.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me a story\"}\n    ],\n    \"temperature\": 0.7  # Balanced creativity\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#max_tokens","title":"<code>max_tokens</code>","text":"<p>Sets the maximum number of tokens that can be generated in the response.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: Varies by model</li> </ul> <p>Higher values allow for longer responses but may increase latency and costs. Lower values provide more concise outputs.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ],\n    \"max_tokens\": 1000  # Limit response length\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#top_p","title":"<code>top_p</code>","text":"<p>Nucleus sampling parameter that controls the diversity of tokens considered.</p> <ul> <li>Type: <code>float</code></li> <li>Range: 0.0 to 1.0</li> <li>Default: 1.0</li> </ul> <p>Lower values make the model more focused on likely tokens, while higher values allow more diverse choices.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a creative poem\"}\n    ],\n    \"top_p\": 0.9  # Allow more diverse word choices\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#frequency_penalty","title":"<code>frequency_penalty</code>","text":"<p>Reduces the likelihood of the model repeating frequent tokens.</p> <ul> <li>Type: <code>float</code></li> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> </ul> <p>Positive values reduce repetition, while negative values increase it. Useful for avoiding repetitive phrases.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"List 10 different ideas\"}\n    ],\n    \"frequency_penalty\": 0.5  # Reduce repetition\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#presence_penalty","title":"<code>presence_penalty</code>","text":"<p>Reduces the likelihood of the model reusing tokens that are already present in the text.</p> <ul> <li>Type: <code>float</code></li> <li>Range: -2.0 to 2.0</li> <li>Default: 0</li> </ul> <p>Positive values encourage the model to use new tokens and explore different approaches.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Generate diverse solutions\"}\n    ],\n    \"presence_penalty\": 0.6  # Encourage new approaches\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#stop","title":"<code>stop</code>","text":"<p>Array of strings that cause the model to stop generating when encountered.</p> <ul> <li>Type: <code>array[string]</code> or <code>string</code></li> <li>Default: <code>null</code></li> </ul> <p>Useful for controlling where the response ends or preventing certain phrases.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Count to 10\"}\n    ],\n    \"stop\": [\"11\", \"twelve\"]  # Stop at these sequences\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#n","title":"<code>n</code>","text":"<p>Number of completions to generate for each prompt.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: 1</li> </ul> <p>Generates multiple response variations.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Suggest a name\"}\n    ],\n    \"n\": 3  # Generate 3 different suggestions\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#seed","title":"<code>seed</code>","text":"<p>Seed for reproducible outputs.</p> <ul> <li>Type: <code>integer</code></li> <li>Default: <code>null</code></li> </ul> <p>When set, the model will produce more deterministic outputs, useful for testing and reproducibility.</p> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Generate a random number\"}\n    ],\n    \"seed\": 42  # Reproducible output\n}\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#complete-example","title":"Complete Example","text":"<pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Write a creative story\"}\n    ],\n    \"temperature\": 0.8,\n    \"max_tokens\": 500,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.3,\n    \"presence_penalty\": 0.4\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nresult = response.json()\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"core-features/advanced/response-parameters/#best-practices","title":"Best Practices","text":"<ul> <li>For factual tasks: Use lower <code>temperature</code> (0.2-0.4) for more deterministic outputs</li> <li>For creative tasks: Use higher <code>temperature</code> (0.7-0.9) for more varied responses</li> <li>For long responses: Set appropriate <code>max_tokens</code> to avoid truncation</li> <li>To reduce repetition: Use <code>frequency_penalty</code> (0.3-0.7) and <code>presence_penalty</code> (0.4-0.6)</li> <li>For consistency: Use <code>seed</code> when you need reproducible outputs</li> </ul>"},{"location":"core-features/advanced/sla-support/","title":"SLA &amp; Enterprise Support","text":"<p>Enterprise-grade SLA and dedicated support options.</p>"},{"location":"core-features/advanced/sla-support/#service-level-agreements-sla","title":"Service Level Agreements (SLA)","text":""},{"location":"core-features/advanced/sla-support/#standard-sla","title":"Standard SLA","text":"<ul> <li>Uptime: 99.5% monthly uptime guarantee</li> <li>Response Time: P50 latency &lt; 200ms</li> <li>Support: Business hours support (email)</li> </ul>"},{"location":"core-features/advanced/sla-support/#premium-sla","title":"Premium SLA","text":"<ul> <li>Uptime: 99.9% monthly uptime guarantee</li> <li>Response Time: P50 latency &lt; 150ms, P99 &lt; 500ms</li> <li>Support: 24/7 phone and email support</li> <li>SLA Credits: Automatic credits for downtime</li> </ul>"},{"location":"core-features/advanced/sla-support/#enterprise-sla","title":"Enterprise SLA","text":"<ul> <li>Uptime: 99.95% monthly uptime guarantee</li> <li>Response Time: P50 latency &lt; 100ms, P99 &lt; 300ms</li> <li>Support: 24/7 dedicated support team</li> <li>Features: Private endpoints, custom rate limits</li> <li>SLA Credits: Enhanced credits for any downtime</li> </ul>"},{"location":"core-features/advanced/sla-support/#support-tiers","title":"Support Tiers","text":""},{"location":"core-features/advanced/sla-support/#standard-support","title":"Standard Support","text":"<ul> <li>Email support during business hours</li> <li>Response time: 24 hours</li> <li>Access to knowledge base</li> <li>Community forum access</li> </ul>"},{"location":"core-features/advanced/sla-support/#premium-support","title":"Premium Support","text":"<ul> <li>24/7 phone and email support</li> <li>Response time: 4 hours (critical), 8 hours (non-critical)</li> <li>Dedicated support engineer</li> <li>Quarterly business reviews</li> </ul>"},{"location":"core-features/advanced/sla-support/#enterprise-support","title":"Enterprise Support","text":"<ul> <li>24/7 dedicated support team</li> <li>Response time: 1 hour (critical), 4 hours (non-critical)</li> <li>Designated account manager</li> <li>Monthly business reviews</li> <li>Custom integration assistance</li> <li>Performance optimization consulting</li> </ul>"},{"location":"core-features/advanced/sla-support/#incident-response","title":"Incident Response","text":""},{"location":"core-features/advanced/sla-support/#critical-issues","title":"Critical Issues","text":"<p>For production outages:</p> <ol> <li>Immediate notification via SMS/phone</li> <li>Incident commander assigned within 15 minutes</li> <li>Status updates every 30 minutes</li> <li>Root cause analysis within 24 hours</li> </ol>"},{"location":"core-features/advanced/sla-support/#escalation-process","title":"Escalation Process","text":"<pre><code>Support Team \u2192 Lead Engineer \u2192 VP Engineering \u2192 CTO\n</code></pre>"},{"location":"core-features/advanced/sla-support/#sla-credits","title":"SLA Credits","text":"<p>Automatic credits are issued for downtime:</p> Downtime Credit 99.5-99.0% 10% monthly bill 99.0-98.0% 25% monthly bill &lt;98.0% 50% monthly bill"},{"location":"core-features/advanced/sla-support/#service-coverage","title":"Service Coverage","text":"<p>SLA covers:</p> <p>\u2705 API availability and response times \u2705 Data processing and inference \u2705 Authentication and authorization \u2705 Real-time metrics and monitoring  </p> <p>SLA excludes:</p> <p>\u274c Third-party service dependencies \u274c Customer misconfiguration \u274c Scheduled maintenance \u274c Force majeure events</p>"},{"location":"core-features/advanced/thinking/","title":"Thinking","text":"<p>Thinking is a feature that allows models to reason through problems step by step, showing their internal thought process before providing a final answer.</p>"},{"location":"core-features/advanced/thinking/#overview","title":"Overview","text":"<p>The thinking feature enables models to break down complex problems into smaller steps, making their reasoning process transparent and allowing for better understanding of how they arrive at their conclusions.</p>"},{"location":"core-features/advanced/thinking/#usage","title":"Usage","text":"<p>To enable thinking, you can use the <code>thinking</code> parameter in your API requests.</p> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What color was Napoleon's white horse?\"}\n    ],\n    \"thinking\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nresult = response.json()\n\n# Extract main content and reasoning\nmessage = result.get(\"choices\", [{}])[0].get(\"message\", {})\ncontent = message.get(\"content\", \"\")\nreasoning = message.get(\"reasoning_content\", \"\")\n\nif reasoning:\n    print(\"=== Reasoning ===\")\n    print(reasoning)\n    print(\"\\n=== Final Answer ===\")\n\nprint(content)\n</code></pre>"},{"location":"core-features/advanced/thinking/#parameters","title":"Parameters","text":""},{"location":"core-features/advanced/thinking/#reasoning_effort","title":"<code>reasoning_effort</code>","text":"<p>Controls the depth and detail of the reasoning process. Available values:</p> <ul> <li><code>low</code>: Minimal reasoning effort, faster responses with brief reasoning</li> <li><code>medium</code>: Balanced reasoning effort (default)</li> <li><code>high</code>: Maximum reasoning effort, more detailed and thorough reasoning</li> </ul> <pre><code>data = {\n    \"model\": \"gpt-oss-120b\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Solve this step by step: What is 15% of 240?\"}\n    ],\n    \"thinking\": True,\n    \"reasoning_effort\": \"high\"  # low, medium, or high\n}\n</code></pre> <p>Standard Parameters</p> <p>Standard API parameters like <code>temperature</code>, <code>max_tokens</code>, <code>top_p</code>, <code>frequency_penalty</code>, and <code>presence_penalty</code> can also influence the thinking process. See Response Parameters for detailed documentation.</p>"},{"location":"core-features/advanced/thinking/#benefits","title":"Benefits","text":"<ul> <li>Transparency: See how the model reasons through problems</li> <li>Debugging: Understand where the model might make mistakes</li> <li>Education: Learn problem-solving strategies from the model's reasoning</li> <li>Quality: Better results for complex, multi-step problems</li> </ul>"},{"location":"core-features/advanced/webhooks-events/","title":"Webhooks &amp; Events","text":"<p>Integrate with webhooks and real-time event streams.</p>"},{"location":"core-features/advanced/webhooks-events/#event-types","title":"Event Types","text":"<p>Regolo sends events for key system activities:</p>"},{"location":"core-features/advanced/webhooks-events/#inference-events","title":"Inference Events","text":"<ul> <li><code>inference.started</code> - Inference request initiated</li> <li><code>inference.completed</code> - Inference request completed</li> <li><code>inference.failed</code> - Inference request failed</li> <li><code>inference.timeout</code> - Request timeout</li> </ul>"},{"location":"core-features/advanced/webhooks-events/#batch-events","title":"Batch Events","text":"<ul> <li><code>batch.submitted</code> - Batch job submitted</li> <li><code>batch.processing</code> - Batch processing started</li> <li><code>batch.completed</code> - Batch processing completed</li> <li><code>batch.failed</code> - Batch processing failed</li> </ul>"},{"location":"core-features/advanced/webhooks-events/#account-events","title":"Account Events","text":"<ul> <li><code>account.usage_warning</code> - Usage quota approaching</li> <li><code>account.quota_exceeded</code> - Usage quota exceeded</li> <li><code>billing.invoice_created</code> - Invoice generated</li> </ul>"},{"location":"core-features/advanced/webhooks-events/#setting-up-webhooks","title":"Setting Up Webhooks","text":""},{"location":"core-features/advanced/webhooks-events/#1-create-webhook-endpoint","title":"1. Create Webhook Endpoint","text":"<p>Your endpoint must: - Accept POST requests - Return 200-299 status code within 30 seconds - Process events idempotently</p> <pre><code>from flask import Flask, request\n\napp = Flask(__name__)\n\n@app.route('/webhooks/regolo', methods=['POST'])\ndef handle_webhook():\n    event = request.json\n\n    # Verify signature\n    verify_webhook_signature(request)\n\n    # Process event\n    if event['type'] == 'inference.completed':\n        handle_inference_complete(event['data'])\n\n    return {'success': True}, 200\n</code></pre>"},{"location":"core-features/advanced/webhooks-events/#2-register-webhook","title":"2. Register Webhook","text":"APIPython <pre><code>curl -X POST https://api.regolo.ai/v1/webhooks \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"url\": \"https://your-app.com/webhooks/regolo\",\n        \"events\": [\"inference.completed\", \"batch.completed\"],\n        \"active\": true\n    }'\n</code></pre> <pre><code>import regolo\n\nwebhook = regolo.create_webhook(\n    url=\"https://your-app.com/webhooks/regolo\",\n    events=[\"inference.completed\", \"batch.completed\"],\n    active=True\n)\n</code></pre>"},{"location":"core-features/advanced/webhooks-events/#event-structure","title":"Event Structure","text":"<pre><code>{\n  \"id\": \"evt_abc123\",\n  \"type\": \"inference.completed\",\n  \"timestamp\": \"2024-12-18T10:30:00Z\",\n  \"data\": {\n    \"request_id\": \"req_xyz789\",\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"status\": \"success\",\n    \"tokens_used\": 150,\n    \"latency_ms\": 1250\n  },\n  \"request\": {\n    \"id\": \"req_xyz789\",\n    \"webhook_id\": \"wh_12345\"\n  }\n}\n</code></pre>"},{"location":"core-features/advanced/webhooks-events/#webhook-security","title":"Webhook Security","text":""},{"location":"core-features/advanced/webhooks-events/#signature-verification","title":"Signature Verification","text":"<p>All webhooks are signed with HMAC-SHA256:</p> <pre><code>import hmac\nimport hashlib\n\ndef verify_webhook_signature(request, webhook_secret):\n    signature = request.headers.get('X-Regolo-Signature')\n    timestamp = request.headers.get('X-Regolo-Timestamp')\n    body = request.get_data()\n\n    # Check timestamp (prevent replay attacks)\n    if abs(time.time() - int(timestamp)) &gt; 300:\n        return False\n\n    # Verify signature\n    signed_content = f\"{timestamp}.{body.decode()}\"\n    expected_signature = hmac.new(\n        webhook_secret.encode(),\n        signed_content.encode(),\n        hashlib.sha256\n    ).hexdigest()\n\n    return hmac.compare_digest(signature, expected_signature)\n</code></pre>"},{"location":"core-features/advanced/webhooks-events/#retry-policy","title":"Retry Policy","text":"<p>Failed webhook deliveries are retried:</p> Attempt Delay Status 1st Immediate Sent 2nd 5 seconds Retrying 3rd 30 seconds Retrying 4th 2 minutes Final attempt <p>After 4 failed attempts, the webhook is marked as failed.</p>"},{"location":"core-features/advanced/webhooks-events/#monitoring-webhooks","title":"Monitoring Webhooks","text":"<pre><code># Get webhook details\nwebhook = regolo.get_webhook(webhook_id)\nprint(f\"Status: {webhook.status}\")\nprint(f\"Last delivery: {webhook.last_delivery_at}\")\nprint(f\"Failed attempts: {webhook.failed_attempts}\")\n\n# List webhook events\nevents = regolo.list_webhook_events(\n    webhook_id=webhook_id,\n    limit=100\n)\n\nfor event in events:\n    print(f\"{event.type} - {event.status} - {event.delivered_at}\")\n</code></pre>"},{"location":"core-features/advanced/webhooks-events/#best-practices","title":"Best Practices","text":"<ol> <li>Verify signatures: Always verify webhook signatures</li> <li>Process asynchronously: Queue events for async processing</li> <li>Idempotent handling: Handle duplicate events safely</li> <li>Fast responses: Return 200 quickly, process in background</li> <li>Monitor delivery: Track failed webhook deliveries</li> <li>Secure endpoints: Use HTTPS and authentication</li> </ol>"},{"location":"core-features/inference-api/batch-processing/","title":"Batch Processing","text":"<p>Process large batches of requests efficiently with lower latency requirements.</p>"},{"location":"core-features/inference-api/batch-processing/#overview","title":"Overview","text":"<p>Batch processing is ideal for:</p> <ul> <li>Processing large datasets asynchronously</li> <li>Reducing per-request latency requirements</li> <li>Cost optimization through bulk processing</li> <li>Off-peak processing for non-time-critical workloads</li> </ul>"},{"location":"core-features/inference-api/batch-processing/#when-to-use-batch-processing","title":"When to Use Batch Processing","text":""},{"location":"core-features/inference-api/batch-processing/#ideal-use-cases","title":"Ideal Use Cases","text":"<p>\u2705 Processing logs for analysis \u2705 Bulk content classification \u2705 Dataset augmentation \u2705 Daily report generation \u2705 Offline analysis of large documents  </p>"},{"location":"core-features/inference-api/batch-processing/#not-recommended","title":"Not Recommended","text":"<p>\u274c Real-time customer-facing responses \u274c Interactive applications \u274c Time-sensitive operations  </p>"},{"location":"core-features/inference-api/batch-processing/#submitting-a-batch-job","title":"Submitting a Batch Job","text":""},{"location":"core-features/inference-api/batch-processing/#step-1-prepare-requests","title":"Step 1: Prepare Requests","text":"<p>Format your requests as JSONL (JSON Lines):</p> <pre><code>{\"messages\": [{\"role\": \"user\", \"content\": \"What is AI?\"}], \"model\": \"Llama-3.3-70B-Instruct\"}\n{\"messages\": [{\"role\": \"user\", \"content\": \"Define machine learning\"}], \"model\": \"Llama-3.3-70B-Instruct\"}\n{\"messages\": [{\"role\": \"user\", \"content\": \"Explain neural networks\"}], \"model\": \"Llama-3.3-70B-Instruct\"}\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#step-2-create-batch-job","title":"Step 2: Create Batch Job","text":"PythoncURL <pre><code>import regolo\n\n# Read your JSONL file\nwith open('requests.jsonl', 'r') as f:\n    batch_data = f.read()\n\n# Submit batch job\nbatch = regolo.submit_batch(\n    requests=batch_data,\n    model=\"Llama-3.3-70B-Instruct\"\n)\n\nprint(f\"Batch ID: {batch.id}\")\nprint(f\"Status: {batch.status}\")\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/batches \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"input_file_id\": \"file-abc123\",\n        \"endpoint\": \"/v1/chat/completions\",\n        \"timeout_minutes\": 1440\n    }'\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#step-3-monitor-progress","title":"Step 3: Monitor Progress","text":"<pre><code>import regolo\n\nbatch = regolo.get_batch(batch_id=\"batch_123\")\nprint(f\"Status: {batch.status}\")\nprint(f\"Processed: {batch.request_counts.completed} / {batch.request_counts.total}\")\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#step-4-retrieve-results","title":"Step 4: Retrieve Results","text":"<pre><code># Get results when complete\nresults = regolo.get_batch_results(batch_id=\"batch_123\")\n\nfor result in results:\n    print(result)\n    # {\n    #   \"id\": \"batch-req-123\",\n    #   \"custom_id\": \"request-1\",\n    #   \"response\": {\n    #     \"body\": {\n    #       \"choices\": [{\"message\": {\"content\": \"AI is...\"}}],\n    #       \"usage\": {\"total_tokens\": 150}\n    #     }\n    #   },\n    #   \"error\": null\n    # }\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#batch-job-lifecycle","title":"Batch Job Lifecycle","text":"<pre><code>Submitted\n    \u2193\nValidating\n    \u2193\nQueued\n    \u2193\nProcessing (in progress)\n    \u2193\nCompleted / Failed\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#pricing-benefits","title":"Pricing Benefits","text":"<p>Batch processing offers cost savings:</p>"},{"location":"core-features/inference-api/batch-processing/#regular-processing","title":"Regular Processing","text":"<pre><code>1000 requests \u00d7 $0.00002/token = Cost based on tokens\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#batch-processing_1","title":"Batch Processing","text":"<pre><code>1000 requests in batch = 10% discount\nCost: 90% of regular pricing\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#request-format","title":"Request Format","text":""},{"location":"core-features/inference-api/batch-processing/#basic-format","title":"Basic Format","text":"<pre><code>{\n  \"custom_id\": \"request-1\",\n  \"params\": {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n      {\"role\": \"user\", \"content\": \"Hello\"}\n    ]\n  }\n}\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#with-advanced-options","title":"With Advanced Options","text":"<pre><code>{\n  \"custom_id\": \"request-1\",\n  \"params\": {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Summarize this document\"}],\n    \"max_tokens\": 100,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9\n  }\n}\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#response-format","title":"Response Format","text":"<pre><code>{\n  \"id\": \"batch-req-123\",\n  \"custom_id\": \"request-1\",\n  \"response\": {\n    \"status_code\": 200,\n    \"request_id\": \"req-123\",\n    \"body\": {\n      \"id\": \"chatcmpl-abc\",\n      \"object\": \"chat.completion\",\n      \"created\": 1703001234,\n      \"model\": \"Llama-3.3-70B-Instruct\",\n      \"choices\": [\n        {\n          \"index\": 0,\n          \"message\": {\"role\": \"assistant\", \"content\": \"Response content\"},\n          \"finish_reason\": \"stop\"\n        }\n      ],\n      \"usage\": {\n        \"prompt_tokens\": 10,\n        \"completion_tokens\": 50,\n        \"total_tokens\": 60\n      }\n    }\n  },\n  \"error\": null\n}\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#complete-example","title":"Complete Example","text":""},{"location":"core-features/inference-api/batch-processing/#create-batch-file","title":"Create Batch File","text":"<pre><code>import json\n\nrequests = [\n    {\"custom_id\": f\"req-{i}\", \"params\": {\n        \"model\": \"Llama-3.3-70B-Instruct\",\n        \"messages\": [{\"role\": \"user\", \"content\": f\"Question {i}\"}]\n    }} for i in range(1000)\n]\n\n# Save as JSONL\nwith open('batch_requests.jsonl', 'w') as f:\n    for req in requests:\n        f.write(json.dumps(req) + '\\n')\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#submit-and-monitor","title":"Submit and Monitor","text":"<pre><code>import regolo\nimport time\n\n# Submit batch\nwith open('batch_requests.jsonl', 'r') as f:\n    batch = regolo.submit_batch(requests=f.read())\n\nprint(f\"Batch {batch.id} submitted\")\n\n# Monitor progress\nwhile True:\n    batch = regolo.get_batch(batch.id)\n    print(f\"Status: {batch.status}\")\n    print(f\"Progress: {batch.request_counts.completed}/{batch.request_counts.total}\")\n\n    if batch.status == \"completed\":\n        break\n\n    time.sleep(10)  # Check every 10 seconds\n\n# Get results\nresults = regolo.get_batch_results(batch.id)\nfor result in results:\n    if result['error'] is None:\n        content = result['response']['body']['choices'][0]['message']['content']\n        print(f\"{result['custom_id']}: {content}\")\n    else:\n        print(f\"{result['custom_id']}: Error - {result['error']}\")\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#best-practices","title":"Best Practices","text":"<ol> <li>Use custom IDs - Track results with meaningful identifiers</li> <li>Batch size - 1000-10000 requests per batch is optimal</li> <li>Error handling - Check each result for errors</li> <li>Cleanup - Process results and delete when done</li> <li>Timeout - Set appropriate timeout for your workload</li> </ol>"},{"location":"core-features/inference-api/batch-processing/#limits","title":"Limits","text":"Parameter Limit Batch size 10,000 requests File size 500 MB Timeout 24 hours Concurrent batches 10 per account"},{"location":"core-features/inference-api/batch-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"core-features/inference-api/batch-processing/#batch-stuck-in-processing","title":"Batch Stuck in Processing","text":"<pre><code># Cancel if needed\nregolo.cancel_batch(batch_id=\"batch_123\")\n</code></pre>"},{"location":"core-features/inference-api/batch-processing/#high-error-rate","title":"High Error Rate","text":"<ul> <li>Check request format</li> <li>Verify model availability</li> <li>Review error messages in results</li> </ul>"},{"location":"core-features/inference-api/batch-processing/#slow-processing","title":"Slow Processing","text":"<ul> <li>Submit during off-peak hours</li> <li>Reduce batch size</li> <li>Check system load</li> </ul>"},{"location":"core-features/inference-api/completions-and-chat/","title":"Completions and Chat","text":""},{"location":"core-features/inference-api/completions-and-chat/#static-chat-completions","title":"Static Chat Completions","text":"<p>Static chat completions enable a more interactive session by providing conversation-like exchanges, you can send a series of messages. Each message has a role, such as <code>user</code>, <code>assistant</code> or <code>system</code>. The model processes these to continue the conversation naturally. This is useful for applications requiring a back-and-forth dialogue.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_chat_completions(messages=[{\"role\": \"user\", \"content\": \"Tell me something about rome\"}]))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ]\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_REGOLO_KEY\" \\\n    -d '{\n     \"model\": \"Llama-3.3-70B-Instruct\",\n     \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about Rome.\"\n        }\n    ]\n}'\n</code></pre>"},{"location":"core-features/inference-api/completions-and-chat/#stream-chat-completions","title":"Stream Chat Completions","text":"<p>Stream chat completions provide real-time, incremental responses from the model, enabling dynamic interactions and reducing latency. This feature is beneficial for applications that require immediate feedback and continuous conversation flow.</p> <p>The streaming response is structured as JSON objects sent line by line. Each line typically contains metadata, including fields like <code>id</code>, <code>created</code>, <code>model</code>, and <code>object</code>, along with the <code>choices</code> array. Within <code>choices</code>, there is a <code>delta</code> object, which holds the <code>content</code> field representing the actual text response from the model. This structure allows applications to parse and process the conversational content as it arrives, ensuring efficient and timely updates to the user interface.</p> Regolo Client - CleanRegolo Client - FullPython - CleanPython - Full <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(\n    user_prompt=\"Tell me something about Rome.\",\n    full_output=True,\n    stream=True\n)\n\n# Process streamed responses - extract only content (words)\nwhile True:\n    try:\n        chunk = next(response)\n        # Extract content from delta if it's a dict\n        if isinstance(chunk, dict):\n            content = chunk.get('choices', [{}])[0].get('delta', {}).get('content', '')\n            if content:\n                print(content, end=\"\", flush=True)\n        else:\n            # If it's already a string, print it directly\n            print(chunk, end=\"\", flush=True)\n    except StopIteration:\n        break\n</code></pre> <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nclient = regolo.RegoloClient()\nresponse = client.run_chat(\n    user_prompt=\"Tell me something about Rome.\",\n    full_output=True,\n    stream=True\n)\n\n# Process streamed responses - show full JSON structure\nwhile True:\n    try:\n        chunk = next(response)\n        print(chunk)\n    except StopIteration:\n        break\n</code></pre> <pre><code>import requests\nimport json\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\n# Extract only content from streamed JSON\nfor line in response.iter_lines():\n    if line:\n        decoded_line = line.decode('utf-8')\n        if decoded_line.startswith('data: '):\n            json_str = decoded_line[6:]\n            if json_str.strip() == '[DONE]':\n                break\n            try:\n                json_data = json.loads(json_str)\n                if 'choices' in json_data:\n                    delta = json_data['choices'][0].get('delta', {})\n                    content = delta.get('content', '')\n                    if content:\n                        print(content, end='', flush=True)\n            except json.JSONDecodeError:\n                pass\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"Tell me something about Rome.\"}\n    ],\n    \"stream\": True\n}\n\nresponse = requests.post(api_url, headers=headers, json=data, stream=True)\n\n# Show full JSON response\nfor line in response.iter_lines():\n    if line:\n        print(line.decode('utf-8'))\n</code></pre>"},{"location":"core-features/inference-api/completions-and-chat/#text-static-completions-deprecated","title":"Text Static Completions (Deprecated)","text":"<p>Warning</p> <p>The static text completions are currently deprecated, and Regolo no longer provides any model that supports them natively. They are still listed among the available endpoints only for backward compatibility and internal use, but no public model currently supports them. Use the chat completions instead.</p> <p>Static completions allow you to generate text responses based on a given prompt using the Regolo API.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nprint(regolo.static_completions(prompt=\"Tell me something about Rome.\"))\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/completions \n-H \"Content-Type: application/json\" \n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \n-d '{\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"prompt\": \"Tell me something about Rome.\",\n    \"temperature\": 0.7\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"core-features/inference-api/embedding/","title":"Embedding","text":"<p>The embedding API allows you to get a vector representation of the input to be used from machine learning models or algorithms, leveraging models like <code>gte-Qwen2</code>.</p>"},{"location":"core-features/inference-api/embedding/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>input</code>: A string describing the sentence, such as \"A white cat resting in Rome.\"</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"gte-Qwen2.\"</li> </ul> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"&lt;YOUR_REGOLO_KEY&gt;\"\nregolo.default_embedder_model = \"gte-Qwen2\"\n\n\nembeddings = regolo.static_embeddings(input_text=[\"A white cat resting in Rome\", \"A white cat resting in Paris\"])\n\nprint(embeddings)\n</code></pre> <pre><code>import requests\nimport json\n\nurl = 'https://api.regolo.ai/v1/embeddings'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"input\": \"A white cat resting in Rome\",\n    \"model\": \"gte-Qwen2\",\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    with open(\"./embedding.json\", 'w') as _file:\n        json.dump(response.json(), _file)\nelse:\n    print(\"Failed embedding request:\", response.status_code, response.text)\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/embeddings\n-H \"Content-Type: application/json\"\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\"\n-d '{\n    \"model\": \"gte-Qwen2\",\n    \"input\": \"The quick brown fox jumps over the lazy dog\"\n}'\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"core-features/inference-api/image-generation/","title":"Image Generation","text":"<p>The image generation API allows you to create images based on textual descriptions, leveraging models like <code>Qwen-Image</code>.</p>"},{"location":"core-features/inference-api/image-generation/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>prompt</code>: A string describing the desired image, such as \"A white cat resting in Rome.\"</li> <li><code>n</code>: An integer specifying the number of images to generate. Generating more images increases response time, so it's best to keep this number small for faster performance.</li> <li><code>model</code>: The identifier for the model used in image generation, e.g., \"Qwen-Image.\"</li> <li><code>size</code>: A string defining the dimensions of the images. Supported sizes are \"256x256,\" \"512x512,\" and \"1024x1024.\"</li> </ul> <p>Larger images take longer to generate, so consider using smaller sizes for quicker results.</p> <p>Tip</p> <p>If you require larger images, consider using an image upscaler after generation. This can help achieve the desired resolution without increasing the generation time</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nfrom io import BytesIO\nfrom PIL import Image\n\n# pip install regolo Pillow\n\nregolo.default_image_generation_model = \"Qwen-Image\"\nregolo.default_key = \"YOUR_REGOLO_KEY\"\n\nimg_bytes = regolo.static_image_create(prompt=\"A Boat in the sea\")[0]\n\nimage = Image.open(BytesIO(img_bytes))\n\n# Save the Image\noutput_path = \"generated_image.png\"\nimage.save(output_path)\nprint(f\"Image saved to: {output_path}\")\n</code></pre> <pre><code>import requests\nimport json\nfrom PIL import Image\nimport io\nimport base64\n\nurl = 'https://api.regolo.ai/v1/images/generations'\nheaders = {\n    'Authorization': 'Bearer YOUR_REGOLO_KEY',\n    'Content-Type': 'application/json'\n}\n\ndata = {\n    \"prompt\": \"A white cat resting in Rome\",\n    \"n\": 2,\n    \"model\": \"Qwen-Image\",\n    \"size\": \"1024x1024\"\n}\n\nresponse = requests.post(url, headers=headers, data=json.dumps(data))\n\nif response.status_code == 200:\n    response_data = response.json()\n\n    for index, item in enumerate(response_data['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n\n        image_stream = io.BytesIO(image_data)\n        image = Image.open(image_stream)\n\n        # Save the Image\n        output_path = f\"generated_image_{index + 1}.png\"\n        image.save(output_path)\n        print(f\"Image saved to: {output_path}\")\nelse:\n    print(\"Failed to generate images:\", response.status_code, response.text)\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/images/generations' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"prompt\": \"A Boat in the sea\",\n    \"n\": 2,\n    \"model\": \"Qwen-Image\",\n    \"size\": \"1024x1024\"\n}' | python3 -c \"\nimport sys\nimport json\nimport base64\n\nresponse = json.load(sys.stdin)\nif 'data' in response:\n    for index, item in enumerate(response['data']):\n        b64_image = item['b64_json']\n        image_data = base64.b64decode(b64_image)\n        output_path = f'generated_image_{index + 1}.png'\n        with open(output_path, 'wb') as f:\n            f.write(image_data)\n        print(f'Image saved to: {output_path}')\nelse:\n    print('Failed to generate images:', response)\n\"\n</code></pre> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"core-features/inference-api/llm-inference/","title":"LLM Inference","text":"<p>Advanced LLM inference capabilities for production workloads.</p>"},{"location":"core-features/inference-api/llm-inference/#supported-models","title":"Supported Models","text":"<p>Regolo supports leading open-source and commercial LLMs:</p>"},{"location":"core-features/inference-api/llm-inference/#large-language-models","title":"Large Language Models","text":"Model Parameters Context Speed Best For Llama-3.3-70B-Instruct 70B 8K Medium General purpose, instruction-following Llama-2-70B-Chat 70B 4K Medium Chat, conversational Llama-2-7B 7B 4K Fast Edge, mobile Mistral-7B 7B 8K Fast Efficient, cost-effective"},{"location":"core-features/inference-api/llm-inference/#request-format","title":"Request Format","text":""},{"location":"core-features/inference-api/llm-inference/#basic-completion","title":"Basic Completion","text":"<pre><code>import regolo\n\nresponse = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#advanced-parameters","title":"Advanced Parameters","text":"<pre><code>response = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a poem about AI\"}\n    ],\n    max_tokens=100,           # Limit output length\n    temperature=0.7,          # Creativity (0=deterministic, 1=random)\n    top_p=0.9,               # Nucleus sampling\n    top_k=40,                # Top-k sampling\n    frequency_penalty=0,      # Reduce repetition\n    presence_penalty=0        # Encourage new topics\n)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#message-format","title":"Message Format","text":""},{"location":"core-features/inference-api/llm-inference/#message-roles","title":"Message Roles","text":"<pre><code>messages = [\n    # System message sets behavior\n    {\"role\": \"system\", \"content\": \"You are an expert Python developer.\"},\n\n    # Previous conversation for context\n    {\"role\": \"user\", \"content\": \"What's the capital of France?\"},\n    {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n\n    # Current user question\n    {\"role\": \"user\", \"content\": \"What's its population?\"}\n]\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#content-types","title":"Content Types","text":"<pre><code># Text only\n{\"role\": \"user\", \"content\": \"Hello\"}\n\n# Multiple content blocks\n{\"role\": \"user\", \"content\": [\n    {\"type\": \"text\", \"text\": \"What's in this image?\"},\n    {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://...\"}}\n]}\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#streaming-responses","title":"Streaming Responses","text":"<p>For real-time response streaming:</p> <pre><code>stream = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell a long story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content:\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#response-format","title":"Response Format","text":"<pre><code>{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\",\n  \"created\": 1703001234,\n  \"model\": \"Llama-3.3-70B-Instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Quantum computing uses quantum mechanics...\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 25,\n    \"completion_tokens\": 150,\n    \"total_tokens\": 175\n  }\n}\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#finish-reasons","title":"Finish Reasons","text":"Reason Meaning <code>stop</code> Model reached natural end <code>length</code> <code>max_tokens</code> limit reached <code>content_filter</code> Content policy violation <code>function_call</code> Function call triggered"},{"location":"core-features/inference-api/llm-inference/#error-handling","title":"Error Handling","text":""},{"location":"core-features/inference-api/llm-inference/#common-errors","title":"Common Errors","text":"<pre><code>import regolo\n\ntry:\n    response = regolo.static_chat_completions(\n        model=\"Llama-3.3-70B-Instruct\",\n        messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n    )\nexcept regolo.APIError as e:\n    print(f\"API Error: {e.status_code} - {e.message}\")\n    # Handle API errors\nexcept regolo.RateLimitError as e:\n    print(f\"Rate limited, retry after {e.retry_after} seconds\")\n    # Implement exponential backoff\nexcept regolo.TimeoutError as e:\n    print(\"Request timed out, try again\")\n    # Handle timeout\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#retry-logic","title":"Retry Logic","text":"<pre><code>import time\nimport random\n\ndef call_with_retry(messages, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            return regolo.static_chat_completions(\n                model=\"Llama-3.3-70B-Instruct\",\n                messages=messages\n            )\n        except regolo.RateLimitError:\n            wait_time = 2 ** attempt + random.random()\n            print(f\"Rate limited, retrying in {wait_time:.1f}s\")\n            time.sleep(wait_time)\n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise\n            time.sleep(2 ** attempt)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#performance-optimization","title":"Performance Optimization","text":""},{"location":"core-features/inference-api/llm-inference/#1-model-selection","title":"1. Model Selection","text":"<p>Choose the right model for your use case:</p> <pre><code># Fast but less capable\nresponse = regolo.static_chat_completions(\n    model=\"Llama-2-7B\",  # 80ms, cheaper\n    messages=messages\n)\n\n# Slower but more capable\nresponse = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",  # 150ms, more accurate\n    messages=messages\n)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#2-prompt-optimization","title":"2. Prompt Optimization","text":"<pre><code># Before: 500 token prompt\nprompt = \"\"\"\nYou are an expert AI assistant with years of experience.\nPlease follow these guidelines carefully:\n1. Be accurate\n2. Be concise\n...(many more guidelines)...\n\"\"\"\n\n# After: 50 token optimized prompt\nprompt = \"Expert AI assistant. Accurate, concise answers.\"\n\n# Result: 10x faster, 10x cheaper\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#3-token-estimation","title":"3. Token Estimation","text":"<pre><code># Estimate before request\nestimated = regolo.estimate_tokens(messages=messages)\nprint(f\"Estimated cost: ${estimated * 0.00001:.4f}\")\n\n# Adjust if necessary\nif estimated &gt; 1000:\n    # Simplify prompt\n    messages = simplify_messages(messages)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#4-caching","title":"4. Caching","text":"<pre><code># Cache common system prompts\nsystem_prompt = \"You are a helpful assistant.\"\n\nresponse = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": system_prompt, \"cache_ttl\": 3600},\n        {\"role\": \"user\", \"content\": \"Your question\"}\n    ]\n)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#production-best-practices","title":"Production Best Practices","text":""},{"location":"core-features/inference-api/llm-inference/#1-rate-limiting","title":"1. Rate Limiting","text":"<pre><code>from ratelimit import limits, sleep_and_retry\nimport regolo\n\n@sleep_and_retry\n@limits(calls=100, period=60)  # 100 requests per minute\ndef call_llm(messages):\n    return regolo.static_chat_completions(\n        model=\"Llama-3.3-70B-Instruct\",\n        messages=messages\n    )\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#2-monitoring","title":"2. Monitoring","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\nresponse = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=messages\n)\n\nlogger.info(\n    f\"LLM call complete\",\n    extra={\n        \"model\": \"Llama-3.3-70B-Instruct\",\n        \"tokens\": response.usage.total_tokens,\n        \"latency_ms\": response.usage.latency\n    }\n)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#3-fallback-strategy","title":"3. Fallback Strategy","text":"<pre><code>def get_response(messages):\n    try:\n        # Try primary model\n        return regolo.static_chat_completions(\n            model=\"Llama-3.3-70B-Instruct\",\n            messages=messages\n        )\n    except Exception as e:\n        logger.warning(f\"Primary model failed: {e}\")\n        try:\n            # Fallback to faster model\n            return regolo.static_chat_completions(\n                model=\"Llama-2-7B\",\n                messages=messages\n            )\n        except Exception as e:\n            logger.error(f\"Both models failed: {e}\")\n            return None\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"core-features/inference-api/llm-inference/#function-calling","title":"Function Calling","text":"<pre><code>tools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\"}\n                },\n                \"required\": [\"location\"]\n            }\n        }\n    }\n]\n\nresponse = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=messages,\n    tools=tools\n)\n</code></pre>"},{"location":"core-features/inference-api/llm-inference/#few-shot-prompting","title":"Few-Shot Prompting","text":"<pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"Classify sentiment as positive, negative, or neutral.\"},\n\n    # Examples\n    {\"role\": \"user\", \"content\": \"I love this!\"},\n    {\"role\": \"assistant\", \"content\": \"positive\"},\n\n    {\"role\": \"user\", \"content\": \"This is terrible.\"},\n    {\"role\": \"assistant\", \"content\": \"negative\"},\n\n    # Actual request\n    {\"role\": \"user\", \"content\": \"Pretty good product\"}\n]\n\nresponse = regolo.static_chat_completions(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=messages\n)\n</code></pre>"},{"location":"core-features/inference-api/ocr/","title":"OCR","text":"<p>DeepSeek OCR is a powerful optical character recognition model that enables accurate text extraction from images and documents. It supports various modes including native and dynamic resolutions, making it suitable for different use cases from simple OCR to complex document parsing.</p>"},{"location":"core-features/inference-api/ocr/#deepseek-ocr-usage-with-regolo-api","title":"Deepseek-OCR Usage with Regolo API","text":"<p>Use DeepSeek OCR on the Regolo platform with model name: <code>\"deepseek-ocr\"</code></p>"},{"location":"core-features/inference-api/ocr/#python-examples","title":"Python Examples","text":"Remote Image URLBase64 (Image from PATH)Base64 Encoding (Remote URL) <pre><code>import requests\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": \"deepseek-ocr\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Convert the document to markdown.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/86/22386-050-51E63D13/Silicon-silicon-symbol-square-Si-properties-some.jpg\",\n                        \"format\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"skip_special_tokens\": False\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nprint(response.json())\n</code></pre> <pre><code>import base64\nimport requests\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"deepseek-ocr\"\n\nIMAGE_PATH = Path(\"document.png\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Convert the document to markdown.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{image_b64}\",\n                        \"format\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096,\n    \"skip_special_tokens\": False\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\nresponse = requests.post(API_URL, headers=headers, json=payload)\nresult = response.json()\ncontent = result[\"choices\"][0][\"message\"][\"content\"]\nprint(content)\n</code></pre> <pre><code>import base64\nimport requests\n\napi_key = \"YOUR_API_KEY\"\nmodel = \"deepseek-ocr\"\n\nimage_url = \"https://example.com/document.png\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": model,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Convert the document to markdown.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{image_b64}\",\n                        \"format\": \"image/png\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096,\n    \"skip_special_tokens\": False\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nresult = response.json()\ncontent = result[\"choices\"][0][\"message\"][\"content\"]\nprint(content)\n</code></pre> <p>Note on skip_special_tokens</p> <p>The <code>skip_special_tokens</code> parameter controls whether special tokens (like <code>&lt;|grounding|&gt;</code>) are included in the response: - <code>skip_special_tokens=False</code>: Keeps special tokens (default). Use when you need document structure info. - <code>skip_special_tokens=True</code>: Removes special tokens. Use for clean text output only.</p>"},{"location":"core-features/inference-api/ocr/#prompt-examples","title":"Prompt Examples","text":"<pre><code>DeepSeek OCR supports various prompt formats for different use cases:\n\n# Convert the document contents to markdown format\n&lt;|grounding|&gt;Convert the document to markdown.\n\n# Perform text recognition on this image\n&lt;|grounding|&gt;OCR this image.\n\n# Extract all text without layout consideration\nFree OCR.\n\n# Parse any figures or tables in the document\nParse the figure.\n\n# Provide a detailed description of the image content\nDescribe this image in detail.\n\n# Locate the position of &lt;|ref|&gt;xxxx&lt;|/ref|&gt; in the image\nLocate &lt;|ref|&gt;xxxx&lt;|/ref|&gt; in the image.\n</code></pre>"},{"location":"core-features/inference-api/ocr/#pdf-ocr-reader","title":"PDF OCR Reader","text":"<p>This example demonstrates how to extract text from a PDF document by converting each page to an image and processing it through the OCR API. All extracted text is aggregated and saved to a markdown file.</p> <pre><code>import base64\nimport requests\nimport fitz\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"deepseek-ocr\"\n\nPDF_PATH = Path(\"document.pdf\")\nOUTPUT_PATH = PDF_PATH.with_suffix(\".md\")\n\ndoc = fitz.open(PDF_PATH)\nall_text = []\n\nfor page_num in range(len(doc)):\n    page = doc[page_num]\n    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n    img_bytes = pix.tobytes(\"png\")\n\n    image_b64 = base64.b64encode(img_bytes).decode(\"utf-8\")\n\n    payload = {\n        \"model\": MODEL,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": \"Convert the document to markdown.\"},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/png;base64,{image_b64}\",\n                            \"format\": \"image/png\"\n                        }\n                    }\n                ]\n            }\n        ],\n        \"max_tokens\": 4096,\n        \"skip_special_tokens\": False\n    }\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {API_KEY}\"\n    }\n\n    response = requests.post(API_URL, headers=headers, json=payload)\n    result = response.json()\n    content = result[\"choices\"][0][\"message\"][\"content\"]\n\n    all_text.append(f\"\\n\\n--- Page {page_num + 1} ---\\n\\n\")\n    all_text.append(content)\n\ndoc.close()\n\nwith open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n    f.write(\"\".join(all_text))\n\nprint(f\"Completed : file saved in \\\"{OUTPUT_PATH}\\\"\")\n</code></pre>"},{"location":"core-features/inference-api/ocr/#resources","title":"Resources","text":"<p>Deepseek-OCR Links: - GitHub Repository - Hugging Face - Arxiv Paper</p> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"core-features/inference-api/rerank/","title":"Rerank","text":"<p>The Rerank API lets you re\u2011order a list of documents (or passages) according to their relevance to a given query. It is powered by models such as <code>Qwen3\u2011Reranker\u20114B</code> and returns the top\u2011N most relevant documents together with a relevance score.</p> <p>When to use it \u2013 After you have retrieved a large set of candidate documents (e.g., with BM25, vector search, or another LLM), feed them to the Rerank endpoint to obtain a concise, high\u2011quality ranking that can be directly presented to users or passed to a downstream LLM for answer generation.</p>"},{"location":"core-features/inference-api/rerank/#api-call-parameters","title":"API Call Parameters","text":"<p>Tip</p> <p>Important note \u2013 The endpoint expects a JSON payload and the total request size (including all documents) must stay lighter as possible. If you have many candidates, consider chunking them and calling the API multiple times.</p> Parameter Type Description model <code>string</code> (required) Identifier of the reranker model, e.g., <code>Qwen3\u2011Reranker\u20114B</code>. query <code>string</code> (required) The user\u2019s question or search query. documents <code>array[string]</code> (required) List of candidate documents/passages to be reranked. top_n <code>integer</code> (optional) Number of highest\u2011scoring documents to return."},{"location":"core-features/inference-api/rerank/#technical-explanation-of-prompt-tags","title":"Technical Explanation of Prompt Tags","text":"<p>To achieve optimal performance with the Qwen3-Reranker model, the input strings must follow a specific prompt template. This structure allows the underlying Cross-Encoder to differentiate between instructions, user intent, and the content to be ranked.</p> Tag Purpose Description <code>&lt;Instruct&gt;</code> Task Specification Defines the context of the retrieval (e.g., \"retrieve relevant passages\"). This guides the model's focus based on the specific use case. <code>&lt;Query&gt;</code> User Intent Marks the beginning of the actual question or search term. This helps the model isolate the core information the user is looking for. <code>&lt;Document&gt;</code> Content Boundary Identifies each candidate passage. By explicitly tagging documents, the model better understands where one passage ends and another begins during the scoring process."},{"location":"core-features/inference-api/rerank/#why-these-tags-are-necessary","title":"Why these tags are necessary","text":"<p>Modern LLM-based rerankers are trained using these semantic markers to improve zero-shot accuracy. Omitting these tags or using inconsistent formatting between the query and the documents can lead to sub-optimal relevance scores, as the model might fail to distinguish between the instruction and the data.</p> CURLPython <pre><code>  curl --request POST \\\n    --url https://api.regolo.ai/rerank \\\n    --header 'Authorization: Bearer REGOLO_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n      \"model\": \"Qwen3-Reranker-4B\",\n      \"query\": \"&lt;Instruct&gt;: Given a web search query, retrieve relevant passages that answer the query\\n&lt;Query&gt;: What is the capital of China?\",\n      \"documents\": [\n        \"&lt;Document&gt;: The capital of China is Beijing.\",\n        \"&lt;Document&gt;: Gravity is a force that attracts two bodies towards each other...\"\n      ],\n      \"top_n\": 5\n    }'\n</code></pre> <pre><code>import requests\n\napi_key = \"REGOLO_API_KEY\"\nurl = \"https://api.regolo.ai/rerank\"\n\ntask = \"Given a web search query, retrieve relevant passages that answer the query\"\nquery_text = \"What is the capital of China?\"\n\ndocuments = [\n    \"The capital of China is Beijing.\",\n    \"Gravity is a force that attracts two bodies towards each other...\"\n]\n\npayload = {\n    \"model\": \"Qwen3-Reranker-4B\",\n    \"query\": f\"&lt;Instruct&gt;: {task}\\n&lt;Query&gt;: {query_text}\",\n    \"documents\": [f\"&lt;Document&gt;: {doc}\" for doc in documents],\n    \"top_n\": 5\n}\n\nresponse = requests.post(\n    url,\n    json=payload,\n    headers={\"Authorization\": f\"Bearer {api_key}\"}\n)\n\nresults = response.json().get('results', [])\nfor res in results:\n    score = res['relevance_score']\n    clean_text = res['document']['text'].replace(\"&lt;Document&gt;: \", \"\")\n    print(f\"Score: {score:.4f} | Text: {clean_text}\")\n</code></pre>"},{"location":"core-features/inference-api/rerank/#response","title":"Response","text":"<pre><code>{\n  \"id\": \"rerank-8b37844a3beeecb7\",\n  \"results\": [\n    {\n      \"index\": 0,\n      \"relevance_score\": 0.8835278153419495,\n      \"document\": {\n        \"text\": \"&lt;Document&gt;: The capital of China is Beijing.\"\n      }\n    },\n    {\n      \"index\": 1,\n      \"relevance_score\": 0.08649543672800064,\n      \"document\": {\n        \"text\": \"&lt;Document&gt;: Gravity is a force that attracts two bodies towards each other...\"\n      }\n    }\n  ],\n  \"meta\": null\n}\n</code></pre>"},{"location":"core-features/inference-api/speech-to-text/","title":"Speech To Text","text":"<p>The Speech to Text (STT) API enables you to extract and transcribe text from audio files using models such as <code>faster-whisper-large-v3</code>. We recommend using audio chunks of less than 2 minutes to prevent hallucinations and duplicate transcriptions.</p>"},{"location":"core-features/inference-api/speech-to-text/#api-call-parameters","title":"API Call Parameters","text":"<ul> <li><code>file</code>: A binary audio file in OGG format.</li> <li><code>model</code>: The identifier for the model used for transcription, e.g., <code>faster-whisper-large-v3</code>.</li> <li><code>language</code>: A two-letter ISO language code specifying the language of the audio, such as <code>en</code> (English), <code>it</code> (Italian), etc.</li> </ul>"},{"location":"core-features/inference-api/speech-to-text/#important-note","title":"Important Note","text":"<p>The models have a timeout limit. It is recommended to split audio files into smaller segments, such as five-minute clips, to ensure optimal performance.</p>"},{"location":"core-features/inference-api/speech-to-text/#example-requests","title":"Example Requests","text":"Using Regolo ClientOpenAI ClientPythonCURL <pre><code>import regolo\nfrom pathlib import Path\n\n# Regolo configuration\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_audio_transcription_model = \"faster-whisper-large-v3\"\n\n# Audio file to transcribe\nAUDIO_FILE = \"/path/to/your/audio\"\nOUTPUT_FILE = \"/path/to/output/transcription.txt\"\n\n# Transcribe the file\ntranscript = regolo.static_audio_transcription(file=AUDIO_FILE)\n\n# Save the transcription\noutput_path = Path(OUTPUT_FILE)\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(transcript)\n\nprint(f\"Transcription saved to: {OUTPUT_FILE}\")\n</code></pre> <pre><code>import openai\nfrom pathlib import Path\n\n# OpenAI client configuration\nopenai.api_key = \"YOUR_REGOLO_KEY\"\nopenai.base_url = \"https://api.regolo.ai/v1/\"\n\n# Audio file to transcribe\nAUDIO_FILE = \"/path/to/your/audio\"\nOUTPUT_FILE = \"/path/to/output/transcription.txt\"\n\n# Transcribe the file\nwith open(AUDIO_FILE, \"rb\") as audio_file:\n    transcript = openai.audio.transcriptions.create(\n        model=\"faster-whisper-large-v3\",\n        file=audio_file,\n        language=\"en\",\n        response_format=\"text\"\n    )\n\n# Save the transcription\noutput_path = Path(OUTPUT_FILE)\noutput_path.parent.mkdir(parents=True, exist_ok=True)\n\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(transcript)\n\nprint(f\"Transcription saved to: {OUTPUT_FILE}\")\n</code></pre> <pre><code>import requests\nfrom pathlib import Path\n\ndef main():\n    api_url = \"https://api.regolo.ai/v1/audio/transcriptions\"\n    api_key = \"YOUR_REGOLO_KEY\"\n\n    AUDIO_FILE = \"/path/to/your/audio.ogg\"\n\n    audio_path = Path(AUDIO_FILE)\n    if not audio_path.is_file():\n        print(f\"Audio file does not exist: {audio_path}\")\n        return\n\n    headers = {\n        # Don't set Content-Type here; requests will set correct multipart boundary\n        \"Authorization\": f\"Bearer {api_key}\",\n    }\n\n    with audio_path.open(\"rb\") as audio_file:\n        files = {\n            \"file\": (audio_path.name, audio_file, \"application/octet-stream\")\n        }\n        data = {\n            \"model\": \"faster-whisper-large-v3\",\n            \"language\": \"en\",\n            \"response_format\": \"text\",\n        }\n\n        response = requests.post(api_url, headers=headers, data=data, files=files)\n\n    if response.status_code == 200:\n        transcript_text = response.text\n        print(\"=== Transcription ===\")\n        print(transcript_text)\n        print(\"=====================\")\n    else:\n        print(\"Failed transcription request:\")\n        print(\"Status code:\", response.status_code)\n        print(\"Response body:\", response.text)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>curl --request POST \\\n  --url 'https://api.regolo.ai/v1/audio/transcriptions' \\\n  --header 'Authorization: Bearer YOUR_REGOLO_KEY' \\\n  -F \"file=@/path/to/your/audio\" \\\n  -F \"model=faster-whisper-large-v3\"\n</code></pre>"},{"location":"core-features/inference-api/speech-to-text/#example-implementation","title":"Example Implementation","text":"<p>For a practical example of how to use this API, you can refer to the Telegram Transcriber GitHub Repository. This repository provides a complete implementation for transcribing audio messages from Telegram using the Speech to Text API.</p> <p>For the exhaustive API's endpoints documentation visit docs.api.regolo.ai.</p>"},{"location":"core-features/inference-api/vision-analysis/","title":"Vision Analysis","text":"<p>Vision completions enable the processing of images alongside text, allowing for a wide range of applications such as image description, object recognition, and data extraction from visual content. By sending a combination of text prompts and image URLs, the model can provide insightful responses based on the visual input.</p>"},{"location":"core-features/inference-api/vision-analysis/#note","title":"Note","text":"<p>This API supports only images, PDF files are not supported.</p>"},{"location":"core-features/inference-api/vision-analysis/#vision-completions","title":"Vision Completions","text":"<p>You can provide images in three ways:</p> <ul> <li>Remote URL: Supply a publicly accessible URL pointing to the image.</li> <li>Base64 (Image from PATH): Read a local image file, encode it as Base64, and send it to the model.</li> <li>Base64 Encoding (Remote URL): Download an image from a URL, encode it as Base64, and send it to the model.</li> </ul>"},{"location":"core-features/inference-api/vision-analysis/#remote-image-url","title":"Remote Image URL","text":"<p>This section demonstrates how to use a remote image URL directly with the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\nprint(regolo.static_chat_completions(messages=[{\n    \"role\": \"user\",\n    \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Describe this image in detail.\"\n        },\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                \"format\": \"image/jpeg\"\n            }\n        }\n    ]\n}]))\n</code></pre> <pre><code>import requests\n\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": \"qwen3-vl-32b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ]\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_REGOLO_KEY\"\n}\n\nresponse = requests.post(url, json=payload, headers=headers)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_REGOLO_KEY\" \\\n-d '{\n    \"model\": \"qwen3-vl-32b\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ]\n}'\n</code></pre>"},{"location":"core-features/inference-api/vision-analysis/#base64-image-from-path","title":"Base64 (Image from PATH)","text":"<p>This section demonstrates how to read a local image file from your filesystem, encode it as Base64, and send it to the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nimport base64\nfrom pathlib import Path\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\nIMAGE_PATH = Path(\"beagle-hound-dog.jpg\")\n\nif not IMAGE_PATH.exists():\n    raise FileNotFoundError(f\"Image not found: {IMAGE_PATH.resolve()}\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\nresult = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in detail.\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                    \"format\": \"image/jpeg\"\n                }\n            }\n        ]\n    }],\n    max_tokens=4096,  # Important: maintain a wide output token window to avoid issues\n    full_output=True\n)\n\nprint(\"Model response:\")\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <pre><code>import base64\nimport json\nimport requests\nfrom pathlib import Path\n\nAPI_URL = \"https://api.regolo.ai/v1/chat/completions\"\nAPI_KEY = \"YOUR-API-KEY\"\nMODEL = \"qwen3-vl-32b\"\n\nIMAGE_PATH = Path(\"beagle-hound-dog.jpg\")\n\nif not IMAGE_PATH.exists():\n    raise FileNotFoundError(f\"Image not found: {IMAGE_PATH.resolve()}\")\n\nwith open(IMAGE_PATH, \"rb\") as f:\n    image_bytes = f.read()\n\nimage_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n\npayload = {\n    \"model\": MODEL,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"max_tokens\": 4096  # Important: maintain a wide output token window to avoid issues\n}\n\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {API_KEY}\"\n}\n\nprint(\"Sending request to Regolo AI API...\")\nresponse = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n\nif response.status_code != 200:\n    print(f\"Error {response.status_code}:\")\n    print(response.text)\nelse:\n    result = response.json()\n    try:\n        content = result[\"choices\"][0][\"message\"][\"content\"]\n        print(\"Model response:\")\n        print(content)\n    except Exception:\n        print(\"Unexpected response format:\")\n        print(response.text)\n</code></pre> <pre><code># Encode local image to base64\nIMAGE_B64=$(base64 -w 0 beagle-hound-dog.jpg)\n\ncurl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR-API-KEY\" \\\n-d \"{\n    \\\"model\\\": \\\"qwen3-vl-32b\\\",\n    \\\"messages\\\": [\n        {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": [\n                {\n                    \\\"type\\\": \\\"text\\\",\n                    \\\"text\\\": \\\"Describe this image in detail.\\\"\n                },\n                {\n                    \\\"type\\\": \\\"image_url\\\",\n                    \\\"image_url\\\": {\n                        \\\"url\\\": \\\"data:image/jpeg;base64,$IMAGE_B64\\\",\n                        \\\"format\\\": \\\"image/jpeg\\\"\n                    }\n                }\n            ]\n        }\n    ],\n    \\\"max_tokens\\\": 4096  # Important: maintain a wide output token window to avoid issues\n}\"\n</code></pre>"},{"location":"core-features/inference-api/vision-analysis/#base64-encoding-remote-url","title":"Base64 Encoding (Remote URL)","text":"<p>This section demonstrates how to download an image from a remote URL, encode it as Base64, and send it to the vision model.</p> Using Regolo ClientPythonCURL <pre><code>import regolo\nimport base64\nimport requests\n\nregolo.default_key = \"YOUR_REGOLO_KEY\"\nregolo.default_chat_model = \"qwen3-vl-32b\"\n\n# Download image and convert to base64\nimage_url = \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\nresult = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Describe this image in detail.\"\n            },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                    \"format\": \"image/jpeg\"\n                }\n            }\n        ]\n    }],\n    temperature=0.2,\n    max_tokens=4096,  # Important: maintain a wide output token window to avoid issues\n    full_output=True\n)\n\nprint(\"Model response:\")\nprint(result[\"choices\"][0][\"message\"][\"content\"])\n</code></pre> <pre><code>import base64\nimport requests\nimport json\n\n# Regolo API credentials configuration\napi_key = \"YOUR_API_KEY\"\nmodel = \"qwen3-vl-32b\"  # Vision-compatible model\n\n# Download image and convert to base64\nimage_url = \"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nresponse = requests.get(image_url)\nimage_b64 = base64.b64encode(response.content).decode('utf-8')\n\n# Direct API call for multimodal messages\nurl = \"https://api.regolo.ai/v1/chat/completions\"\npayload = {\n    \"model\": model,\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Describe this image in detail.\"\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image_b64}\",\n                        \"format\": \"image/jpeg\"\n                    }\n                }\n            ]\n        }\n    ],\n    \"temperature\": 0.2,\n    \"max_tokens\": 4096  # Important: maintain a wide output token window to avoid issues\n}\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": f\"Bearer {api_key}\"\n}\n\nprint(\"Sending request to Regolo AI API...\")\nresponse = requests.post(url, json=payload, headers=headers)\n\nif response.status_code == 200:\n    result = response.json()\n    try:\n        content = result[\"choices\"][0][\"message\"][\"content\"]\n        print(\"Model response:\")\n        print(content)\n    except KeyError as e:\n        print(f\"KeyError: {e}\")\n        print(\"Response structure:\")\n        print(json.dumps(result, indent=2))\nelse:\n    print(f\"Error {response.status_code}:\")\n    print(response.text)\n</code></pre> <pre><code># First, download the image and encode it to base64\n# This example assumes you have the image URL\nIMAGE_URL=\"https://cdn.britannica.com/16/234216-050-C66F8665/beagle-hound-dog.jpg\"\nIMAGE_B64=$(curl -s \"$IMAGE_URL\" | base64 -w 0)\n\ncurl -X POST https://api.regolo.ai/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-H \"Authorization: Bearer YOUR_API_KEY\" \\\n-d \"{\n    \\\"model\\\": \\\"qwen3-vl-32b\\\",\n    \\\"messages\\\": [\n        {\n            \\\"role\\\": \\\"user\\\",\n            \\\"content\\\": [\n                {\n                    \\\"type\\\": \\\"text\\\",\n                    \\\"text\\\": \\\"Describe this image in detail.\\\"\n                },\n                {\n                    \\\"type\\\": \\\"image_url\\\",\n                    \\\"image_url\\\": {\n                        \\\"url\\\": \\\"data:image/jpeg;base64,$IMAGE_B64\\\",\n                        \\\"format\\\": \\\"image/jpeg\\\"\n                    }\n                }\n            ]\n        }\n    ],\n    \\\"temperature\\\": 0.2,\n    \\\"max_tokens\\\": 4096  # Important: maintain a wide output token window to avoid issues\n}\"\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/","title":"Custom Endpoints","text":"<p>Deploy custom models and create endpoints with full control over configuration and resources.</p>"},{"location":"core-features/model-management/custom-endpoints/#overview","title":"Overview","text":"<p>Custom endpoints allow you to:</p> <ul> <li>Deploy any Hugging Face model from the library</li> <li>Configure resource allocation and GPU selection</li> <li>Create private endpoints for your models</li> <li>Control request/response handling</li> <li>Set custom SLAs</li> <li>Isolate traffic for different use cases</li> </ul>"},{"location":"core-features/model-management/custom-endpoints/#adding-hugging-face-models","title":"Adding Hugging Face Models","text":""},{"location":"core-features/model-management/custom-endpoints/#upload-model-from-hugging-face","title":"Upload Model from Hugging Face","text":"<p>Follow these 5 steps to add any Hugging Face model to your custom library:</p>"},{"location":"core-features/model-management/custom-endpoints/#step-1-start-upload","title":"Step 1: Start Upload","text":"<p>Click \"Add model\" button in the top-right corner of your Custom Model Library.</p>"},{"location":"core-features/model-management/custom-endpoints/#step-2-enter-model-url","title":"Step 2: Enter Model URL","text":"<p>Enter the Hugging Face model URL (e.g., <code>https://huggingface.co/meta-llama/Llama-2-7b-hf</code>).</p>"},{"location":"core-features/model-management/custom-endpoints/#step-3-deploy-model","title":"Step 3: Deploy Model","text":"<p>Click \"Deploy model\" in your library to initialize the deployment.</p>"},{"location":"core-features/model-management/custom-endpoints/#step-4-choose-instance","title":"Step 4: Choose Instance","text":"<p>Select the instance that fits your needs based on: - Model size (7B, 13B, 70B, etc.) - Required GPU resources (NVIDIA A100, H100, etc.) - Expected throughput - Budget constraints</p>"},{"location":"core-features/model-management/custom-endpoints/#step-5-get-api-access","title":"Step 5: Get API Access","text":"<p>Use your regolo 'All models' active API key to access the deployed model.</p>"},{"location":"core-features/model-management/custom-endpoints/#using-your-custom-models","title":"Using Your Custom Models","text":"<pre><code>import regolo\n\n# Initialize client with your API key\nclient = regolo.RegoloClient(api_key=\"YOUR_API_KEY\")\n\n# Make requests to your custom model\nresponse = client.static_chat_completions(\n    model=\"your-custom-model-name\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#custom-model-endpoint-url","title":"Custom Model Endpoint URL","text":"<p>Your custom models are accessible at:</p> <pre><code>https://api.regolo.ai/custom-model/v1/chat/completions/\n</code></pre> <p>Example with cURL:</p> <pre><code>curl https://api.regolo.ai/custom-model/v1/chat/completions/ \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"your-custom-model-name\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n  }'\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#pricing-for-custom-models","title":"Pricing for Custom Models","text":""},{"location":"core-features/model-management/custom-endpoints/#hourly-usage-model","title":"Hourly Usage Model","text":"<p>Custom models use hourly prepaid billing:</p> <ul> <li>Charged from first hour - You pay hourly once your model is deployed</li> <li>Monthly invoice - Receive dedicated invoice at month-end showing total hours used and charges</li> <li>No per-request charges - All requests included in hourly rate</li> </ul>"},{"location":"core-features/model-management/custom-endpoints/#cost-estimation","title":"Cost Estimation","text":"<pre><code>Hourly rate depends on:\n- Model size (larger models cost more)\n- GPU instance type selected (A100, H100, etc.)\n- Replicas/concurrent instances\n\nExample:\n- Llama-2-7B on single A100: ~$0.50/hour = $360/month\n- Llama-2-70B on multiple A100s: ~$2.00/hour = $1,440/month\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#creating-endpoints","title":"Creating Endpoints","text":""},{"location":"core-features/model-management/custom-endpoints/#basic-endpoint","title":"Basic Endpoint","text":"<pre><code>import regolo\n\nendpoint = regolo.create_endpoint(\n    name=\"sentiment-api\",\n    model=\"sentiment-classifier\",\n    version=\"1.0\",\n    replicas=2  # Number of instances\n)\n\nprint(f\"Endpoint ID: {endpoint.id}\")\nprint(f\"URL: {endpoint.url}\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>import regolo\n\nendpoint = regolo.create_endpoint(\n    name=\"sentiment-api-prod\",\n    model=\"sentiment-classifier\",\n    version=\"1.0\",\n\n    # Scaling\n    replicas=5,\n    min_replicas=2,\n    max_replicas=20,\n    target_utilization=0.7,\n\n    # Resources\n    cpu_request=\"2\",\n    memory_request=\"4Gi\",\n    cpu_limit=\"4\",\n    memory_limit=\"8Gi\",\n\n    # Timeout\n    request_timeout_seconds=30,\n\n    # Network\n    enable_cache=True,\n    enable_compression=True\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#deploying-models","title":"Deploying Models","text":""},{"location":"core-features/model-management/custom-endpoints/#deploy-model-version","title":"Deploy Model Version","text":"<pre><code>import regolo\n\nendpoint = regolo.create_endpoint(\n    name=\"classifier-v1-1\",\n    model=\"sentiment-classifier\",\n    version=\"1.1\",\n    replicas=3\n)\n\nprint(f\"Endpoint created: {endpoint.status}\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#canary-deployment","title":"Canary Deployment","text":"<p>Gradually roll out new versions:</p> <pre><code>import regolo\n\n# Deploy new version to canary\ncanary = regolo.create_canary(\n    endpoint_id=\"sentiment-api\",\n    new_version=\"1.1\",\n    traffic_percentage=5  # Start with 5% traffic\n)\n\nprint(f\"Canary deployment started\")\nprint(f\"New version: {canary.new_version}\")\nprint(f\"Traffic: {canary.traffic_percentage}%\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#managing-endpoints","title":"Managing Endpoints","text":""},{"location":"core-features/model-management/custom-endpoints/#list-endpoints","title":"List Endpoints","text":"<pre><code>import regolo\n\nendpoints = regolo.list_endpoints()\n\nfor endpoint in endpoints:\n    print(f\"{endpoint.name} ({endpoint.id})\")\n    print(f\"  Model: {endpoint.model}:{endpoint.version}\")\n    print(f\"  Status: {endpoint.status}\")\n    print(f\"  Replicas: {endpoint.replicas}\")\n    print(f\"  URL: {endpoint.url}\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#update-endpoint","title":"Update Endpoint","text":"<pre><code>import regolo\n\nregolo.update_endpoint(\n    endpoint_id=\"sentiment-api\",\n    replicas=10,  # Scale up\n    request_timeout_seconds=60\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#delete-endpoint","title":"Delete Endpoint","text":"<pre><code>import regolo\n\nregolo.delete_endpoint(endpoint_id=\"sentiment-api\")\nprint(\"Endpoint deleted\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#traffic-management","title":"Traffic Management","text":""},{"location":"core-features/model-management/custom-endpoints/#split-traffic","title":"Split Traffic","text":"<pre><code>import regolo\n\n# A/B testing: 50% traffic to each version\nregolo.update_endpoint_traffic(\n    endpoint_id=\"sentiment-api\",\n    version_traffic={\n        \"1.0\": 0.5,\n        \"1.1\": 0.5\n    }\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#blue-green-deployment","title":"Blue-Green Deployment","text":"<pre><code>import regolo\n\n# Route all traffic to green (new version)\nregolo.update_endpoint_traffic(\n    endpoint_id=\"sentiment-api\",\n    version_traffic={\n        \"1.0\": 0,     # Blue (old) - no traffic\n        \"1.1\": 1.0    # Green (new) - all traffic\n    }\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#using-endpoints","title":"Using Endpoints","text":""},{"location":"core-features/model-management/custom-endpoints/#making-requests","title":"Making Requests","text":"<pre><code>import requests\n\nendpoint_url = \"https://sentiment-api.regolo.ai/v1\"\n\nresponse = requests.post(\n    f\"{endpoint_url}/chat/completions\",\n    headers={\n        \"Authorization\": \"Bearer YOUR_API_KEY\",\n        \"Content-Type\": \"application/json\"\n    },\n    json={\n        \"model\": \"sentiment-classifier\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"This is great!\"}]\n    }\n)\n\nresult = response.json()\nprint(result['choices'][0]['message']['content'])\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#using-python-client","title":"Using Python Client","text":"<pre><code>import regolo\n\nclient = regolo.RegoloClient(\n    api_key=\"YOUR_API_KEY\",\n    endpoint_url=\"https://sentiment-api.regolo.ai\"\n)\n\nresponse = client.static_chat_completions(\n    messages=[{\"role\": \"user\", \"content\": \"This is great!\"}]\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#monitoring-endpoints","title":"Monitoring Endpoints","text":""},{"location":"core-features/model-management/custom-endpoints/#endpoint-metrics","title":"Endpoint Metrics","text":"<pre><code>import regolo\n\nmetrics = regolo.get_endpoint_metrics(\n    endpoint_id=\"sentiment-api\",\n    time_range=\"24h\"\n)\n\nprint(f\"Requests: {metrics.request_count}\")\nprint(f\"Average latency: {metrics.avg_latency_ms}ms\")\nprint(f\"Error rate: {metrics.error_rate:.2%}\")\nprint(f\"Availability: {metrics.availability:.2%}\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#health-checks","title":"Health Checks","text":"<pre><code>import regolo\n\nhealth = regolo.check_endpoint_health(endpoint_id=\"sentiment-api\")\n\nprint(f\"Status: {health.status}\")\nprint(f\"Replicas ready: {health.ready_replicas} / {health.total_replicas}\")\nprint(f\"Last check: {health.last_check}\")\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#auto-scaling","title":"Auto-Scaling","text":""},{"location":"core-features/model-management/custom-endpoints/#configure-autoscaling","title":"Configure Autoscaling","text":"<pre><code>import regolo\n\nregolo.configure_autoscaling(\n    endpoint_id=\"sentiment-api\",\n    metrics=[\n        {\n            \"metric\": \"cpu_usage\",\n            \"target\": 0.7,\n            \"scale_up_threshold\": 0.8,\n            \"scale_down_threshold\": 0.3\n        },\n        {\n            \"metric\": \"request_rate\",\n            \"target\": 100,  # requests/min\n            \"scale_up_threshold\": 150,\n            \"scale_down_threshold\": 50\n        }\n    ],\n    min_replicas=2,\n    max_replicas=50\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#cost-optimization","title":"Cost Optimization","text":""},{"location":"core-features/model-management/custom-endpoints/#right-size-resources","title":"Right-Size Resources","text":"<pre><code>import regolo\n\n# Monitor and optimize\nmetrics = regolo.get_endpoint_metrics(endpoint_id=\"sentiment-api\")\n\nif metrics.cpu_usage &lt; 0.3:\n    print(\"CPU is underutilized, consider reducing\")\n    regolo.update_endpoint(\n        endpoint_id=\"sentiment-api\",\n        cpu_request=\"1\"\n    )\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#batch-requests","title":"Batch Requests","text":"<pre><code>import regolo\n\n# Use batch API for non-time-critical work\nregolo.submit_batch(\n    requests=\"batch.jsonl\",\n    endpoint_id=\"sentiment-api\"\n)\n</code></pre>"},{"location":"core-features/model-management/custom-endpoints/#best-practices","title":"Best Practices","text":"<ol> <li>Resource Allocation - Monitor and adjust resources based on usage</li> <li>Autoscaling - Enable for variable workloads</li> <li>Health Checks - Monitor endpoint health regularly</li> <li>Error Handling - Implement proper error handling in clients</li> <li>Load Testing - Test endpoints under expected load</li> <li>Cost Monitoring - Track and optimize endpoint costs</li> <li>Versioning - Use canary deployments for new versions</li> </ol>"},{"location":"core-features/model-management/fine-tuning/","title":"Fine-Tuning Models","text":"<p>Customize models for your specific use case with fine-tuning.</p>"},{"location":"core-features/model-management/fine-tuning/#overview","title":"Overview","text":"<p>Fine-tuning allows you to:</p> <ul> <li>Adapt models to your domain</li> <li>Improve performance on specific tasks</li> <li>Reduce hallucinations and errors</li> <li>Lower latency and costs</li> <li>Maintain privacy with your data</li> </ul>"},{"location":"core-features/model-management/fine-tuning/#when-to-fine-tune","title":"When to Fine-Tune","text":""},{"location":"core-features/model-management/fine-tuning/#ideal-use-cases","title":"Ideal Use Cases","text":"<p>\u2705 Domain-specific language (medical, legal, technical) \u2705 Specific writing style or tone \u2705 Instruction following improvements \u2705 Custom classification tasks \u2705 Reducing model bias  </p>"},{"location":"core-features/model-management/fine-tuning/#not-recommended","title":"Not Recommended","text":"<p>\u274c General knowledge tasks (use base model) \u274c Small datasets (&lt; 100 examples) \u274c One-off requests  </p>"},{"location":"core-features/model-management/fine-tuning/#preparing-training-data","title":"Preparing Training Data","text":""},{"location":"core-features/model-management/fine-tuning/#data-format","title":"Data Format","text":"<p>Prepare JSONL format with examples:</p> <pre><code>{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is Python?\"}, {\"role\": \"assistant\", \"content\": \"Python is a programming language...\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"What is JavaScript?\"}, {\"role\": \"assistant\", \"content\": \"JavaScript is a scripting language...\"}]}\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#data-quality","title":"Data Quality","text":"<pre><code>import regolo\n\n# Validate training data\nvalidation = regolo.validate_training_data(\n    file_path=\"training_data.jsonl\"\n)\n\nprint(f\"Total examples: {validation.total_examples}\")\nprint(f\"Valid examples: {validation.valid_examples}\")\nprint(f\"Errors: {validation.errors}\")\n\nif validation.valid_examples &gt;= 100:\n    print(\"Data is ready for fine-tuning\")\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#recommended-guidelines","title":"Recommended Guidelines","text":"<ul> <li>Minimum examples: 100 (1000+ recommended)</li> <li>Quality over quantity: High-quality examples outweigh quantity</li> <li>Diversity: Include various input types</li> <li>Validation: Reserve 10-20% for validation</li> </ul>"},{"location":"core-features/model-management/fine-tuning/#starting-fine-tuning","title":"Starting Fine-Tuning","text":""},{"location":"core-features/model-management/fine-tuning/#basic-fine-tuning","title":"Basic Fine-Tuning","text":"<pre><code>import regolo\n\n# Start fine-tuning job\nfinetune_job = regolo.create_finetuning_job(\n    model=\"Llama-3.3-70B-Instruct\",\n    training_file=\"training_data.jsonl\",\n    output_model_name=\"my-finetuned-model\",\n    epochs=3\n)\n\nprint(f\"Job ID: {finetune_job.id}\")\nprint(f\"Status: {finetune_job.status}\")\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>import regolo\n\nfinetune_job = regolo.create_finetuning_job(\n    model=\"Llama-3.3-70B-Instruct\",\n    training_file=\"training_data.jsonl\",\n    validation_file=\"validation_data.jsonl\",\n    output_model_name=\"my-finetuned-model\",\n\n    # Hyperparameters\n    epochs=3,\n    batch_size=32,\n    learning_rate=2e-5,\n    warmup_steps=100,\n\n    # Optimization\n    lora_rank=16,\n    lora_alpha=32\n)\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#monitoring-fine-tuning","title":"Monitoring Fine-Tuning","text":""},{"location":"core-features/model-management/fine-tuning/#check-status","title":"Check Status","text":"<pre><code>import regolo\n\njob = regolo.get_finetuning_job(job_id=\"ft_abc123\")\n\nprint(f\"Status: {job.status}\")\nprint(f\"Progress: {job.progress_percent}%\")\nprint(f\"Epoch: {job.current_epoch} / {job.epochs}\")\nprint(f\"Estimated time remaining: {job.eta_hours} hours\")\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#monitor-training-metrics","title":"Monitor Training Metrics","text":"<pre><code>import regolo\n\nmetrics = regolo.get_finetuning_metrics(job_id=\"ft_abc123\")\n\nfor epoch, epoch_metrics in metrics.items():\n    print(f\"Epoch {epoch}:\")\n    print(f\"  Loss: {epoch_metrics['loss']:.4f}\")\n    print(f\"  Validation loss: {epoch_metrics['val_loss']:.4f}\")\n    print(f\"  Learning rate: {epoch_metrics['learning_rate']:.2e}\")\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#using-fine-tuned-models","title":"Using Fine-Tuned Models","text":""},{"location":"core-features/model-management/fine-tuning/#deploy-fine-tuned-model","title":"Deploy Fine-Tuned Model","text":"<pre><code>import regolo\n\n# After job completes, use the fine-tuned model\nresponse = regolo.static_chat_completions(\n    model=\"my-finetuned-model\",  # Your fine-tuned model\n    messages=[{\"role\": \"user\", \"content\": \"Your prompt\"}]\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#ab-testing","title":"A/B Testing","text":"<pre><code>import regolo\n\n# Compare base model vs fine-tuned\ntest_prompts = [\n    \"What is machine learning?\",\n    \"Explain neural networks\",\n    \"Define overfitting\"\n]\n\nfor prompt in test_prompts:\n    # Base model\n    response_base = regolo.static_chat_completions(\n        model=\"Llama-3.3-70B-Instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n\n    # Fine-tuned model\n    response_ft = regolo.static_chat_completions(\n        model=\"my-finetuned-model\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n\n    # Compare results\n    print(f\"Prompt: {prompt}\")\n    print(f\"Base: {response_base.choices[0].message.content}\")\n    print(f\"Fine-tuned: {response_ft.choices[0].message.content}\")\n    print()\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#cost-analysis","title":"Cost Analysis","text":""},{"location":"core-features/model-management/fine-tuning/#fine-tuning-costs","title":"Fine-Tuning Costs","text":"<pre><code>Training cost: Tokens in training data \u00d7 $0.00001\nStorage cost: Model size \u00d7 $0.10/GB/month\nInference cost: Same as base model\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#example","title":"Example","text":"<pre><code>Training data: 100K tokens\nTraining cost: 100K \u00d7 $0.00001 = $1.00\n\nModel size: 70GB\nMonthly storage: 70 \u00d7 $0.10 = $7.00\n\nTotal monthly: ~$8.00\n</code></pre>"},{"location":"core-features/model-management/fine-tuning/#best-practices","title":"Best Practices","text":"<ol> <li>Start Simple - Begin with default hyperparameters</li> <li>Monitor Training - Watch for overfitting</li> <li>Validate Thoroughly - Test on validation set</li> <li>Version Control - Track which data was used</li> <li>Document Changes - Record improvements</li> <li>Cost Tracking - Monitor training and storage costs</li> </ol>"},{"location":"core-features/model-management/fine-tuning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"core-features/model-management/fine-tuning/#loss-not-decreasing","title":"Loss Not Decreasing","text":"<ul> <li>Check data quality</li> <li>Verify learning rate</li> <li>Ensure examples are diverse</li> </ul>"},{"location":"core-features/model-management/fine-tuning/#overfitting","title":"Overfitting","text":"<ul> <li>Increase dropout</li> <li>Add more training data</li> <li>Reduce epochs</li> </ul>"},{"location":"core-features/model-management/fine-tuning/#memory-issues","title":"Memory Issues","text":"<ul> <li>Reduce batch size</li> <li>Use LoRA for efficient fine-tuning</li> <li>Use smaller base model</li> </ul>"},{"location":"core-features/model-management/model-registry/","title":"Model Registry","text":"<p>Centralized hub for discovering, managing, and organizing your models.</p>"},{"location":"core-features/model-management/model-registry/#overview","title":"Overview","text":"<p>The Model Registry is your central repository for:</p> <ul> <li>All your trained and deployed models</li> <li>Model metadata and documentation</li> <li>Performance metrics and benchmarks</li> <li>Access control and permissions</li> <li>Model lineage and dependencies</li> </ul>"},{"location":"core-features/model-management/model-registry/#registering-models","title":"Registering Models","text":""},{"location":"core-features/model-management/model-registry/#register-from-local-files","title":"Register from Local Files","text":"<pre><code>import regolo\n\nmodel = regolo.register_model(\n    name=\"sentiment-classifier\",\n    version=\"1.0\",\n    model_path=\"./models/sentiment.pkl\",\n    description=\"Sentiment analysis model trained on 50K tweets\",\n    tags=[\"nlp\", \"sentiment\", \"production\"],\n    metadata={\n        \"framework\": \"scikit-learn\",\n        \"accuracy\": 0.92,\n        \"f1_score\": 0.89,\n        \"training_data\": \"tweets-50k\"\n    }\n)\n\nprint(f\"Model registered: {model.id}\")\n</code></pre>"},{"location":"core-features/model-management/model-registry/#register-from-different-frameworks","title":"Register from Different Frameworks","text":"<pre><code>import regolo\n\n# PyTorch model\nregolo.register_model(\n    name=\"pytorch-classifier\",\n    framework=\"pytorch\",\n    model_path=\"./models/model.pt\"\n)\n\n# TensorFlow model\nregolo.register_model(\n    name=\"tf-classifier\",\n    framework=\"tensorflow\",\n    model_path=\"./models/model\"\n)\n\n# ONNX model\nregolo.register_model(\n    name=\"onnx-classifier\",\n    framework=\"onnx\",\n    model_path=\"./models/model.onnx\"\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#searching-models","title":"Searching Models","text":""},{"location":"core-features/model-management/model-registry/#search-by-name","title":"Search by Name","text":"<pre><code>import regolo\n\nmodels = regolo.search_models(\n    query=\"sentiment\",\n    limit=10\n)\n\nfor model in models:\n    print(f\"{model.name} ({model.id}) - v{model.latest_version}\")\n</code></pre>"},{"location":"core-features/model-management/model-registry/#filter-by-tags","title":"Filter by Tags","text":"<pre><code># Find all NLP models\nmodels = regolo.search_models(\n    tags=[\"nlp\"]\n)\n\n# Find production-ready models\nmodels = regolo.search_models(\n    tags=[\"production\"],\n    status=\"active\"\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#advanced-search","title":"Advanced Search","text":"<pre><code># Search with multiple criteria\nmodels = regolo.search_models(\n    query=\"classifier\",\n    tags=[\"nlp\", \"production\"],\n    framework=\"pytorch\",\n    min_accuracy=0.85,\n    sort_by=\"created_at\",\n    sort_order=\"desc\"\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#model-metadata","title":"Model Metadata","text":""},{"location":"core-features/model-management/model-registry/#view-model-details","title":"View Model Details","text":"<pre><code>import regolo\n\nmodel = regolo.get_model(model_id=\"sentiment-classifier\")\n\nprint(f\"Name: {model.name}\")\nprint(f\"Description: {model.description}\")\nprint(f\"Framework: {model.metadata['framework']}\")\nprint(f\"Accuracy: {model.metadata['accuracy']:.2%}\")\nprint(f\"Tags: {', '.join(model.tags)}\")\nprint(f\"Created by: {model.created_by}\")\nprint(f\"Created at: {model.created_at}\")\n</code></pre>"},{"location":"core-features/model-management/model-registry/#update-metadata","title":"Update Metadata","text":"<pre><code>import regolo\n\nregolo.update_model(\n    model_id=\"sentiment-classifier\",\n    description=\"Updated: now supports 10 languages\",\n    tags=[\"nlp\", \"sentiment\", \"multilingual\"],\n    metadata={\n        \"languages\": 10,\n        \"accuracy\": 0.94  # Improved\n    }\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#organizing-models","title":"Organizing Models","text":""},{"location":"core-features/model-management/model-registry/#collections","title":"Collections","text":"<p>Group related models:</p> <pre><code>import regolo\n\n# Create a collection\ncollection = regolo.create_collection(\n    name=\"sentiment-analysis\",\n    description=\"All sentiment analysis models\",\n    tags=[\"nlp\", \"sentiment\"]\n)\n\n# Add models to collection\nregolo.add_to_collection(\n    collection_id=collection.id,\n    model_ids=[\"sentiment-classifier\", \"aspect-sentiment-model\"]\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#teams","title":"Teams","text":"<p>Manage access by team:</p> <pre><code>import regolo\n\n# Create a team\nteam = regolo.create_team(\n    name=\"NLP Team\",\n    description=\"Natural language processing models\"\n)\n\n# Add team members\nregolo.add_team_member(\n    team_id=team.id,\n    user_email=\"engineer@company.com\",\n    role=\"editor\"\n)\n\n# Grant team access to model\nregolo.grant_model_access(\n    model_id=\"sentiment-classifier\",\n    team_id=team.id,\n    permission=\"edit\"\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#model-benchmarks","title":"Model Benchmarks","text":""},{"location":"core-features/model-management/model-registry/#view-benchmarks","title":"View Benchmarks","text":"<pre><code>import regolo\n\nbenchmarks = regolo.get_model_benchmarks(\n    model_id=\"sentiment-classifier\"\n)\n\nfor benchmark in benchmarks:\n    print(f\"Benchmark: {benchmark.name}\")\n    print(f\"  Score: {benchmark.score:.2%}\")\n    print(f\"  Date: {benchmark.date}\")\n</code></pre>"},{"location":"core-features/model-management/model-registry/#compare-with-baselines","title":"Compare with Baselines","text":"<pre><code>import regolo\n\n# Get baseline models\nbaselines = regolo.search_models(\n    tags=[\"baseline\"],\n    category=\"sentiment\"\n)\n\nfor baseline in baselines:\n    comparison = regolo.compare_models(\n        model_a=\"sentiment-classifier\",\n        model_b=baseline.id,\n        metric=\"accuracy\"\n    )\n    print(f\"vs {baseline.name}: {comparison.improvement:.2%} improvement\")\n</code></pre>"},{"location":"core-features/model-management/model-registry/#access-control","title":"Access Control","text":""},{"location":"core-features/model-management/model-registry/#public-models","title":"Public Models","text":"<p>Share models publicly:</p> <pre><code>import regolo\n\nregolo.set_model_visibility(\n    model_id=\"sentiment-classifier\",\n    visibility=\"public\",\n    require_approval=True  # Require approval for use\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#private-models","title":"Private Models","text":"<p>Restrict access to team:</p> <pre><code>regolo.set_model_visibility(\n    model_id=\"internal-model\",\n    visibility=\"private\",\n    allowed_users=[\"team-member@company.com\"]\n)\n</code></pre>"},{"location":"core-features/model-management/model-registry/#best-practices","title":"Best Practices","text":"<ol> <li>Clear Naming - Use descriptive, consistent names</li> <li>Document Purpose - Include description and use cases</li> <li>Tag Models - Use tags for easy discovery</li> <li>Version Consistently - Follow semantic versioning</li> <li>Track Metrics - Record accuracy, latency, etc.</li> <li>Set Permissions - Control who can access models</li> <li>Keep Updated - Remove deprecated models</li> </ol>"},{"location":"core-features/model-management/overview/","title":"Model Management","text":"<p>Manage, organize, and deploy your models with Regolo's comprehensive model management tools.</p>"},{"location":"core-features/model-management/overview/#core-capabilities","title":"Core Capabilities","text":""},{"location":"core-features/model-management/overview/#model-registry","title":"Model Registry","text":"<p>Centralized hub for all your models:</p> <ul> <li>Browse and search available models</li> <li>View model metadata and metrics</li> <li>Track model lineage and dependencies</li> <li>Access model documentation</li> </ul>"},{"location":"core-features/model-management/overview/#version-control","title":"Version Control","text":"<p>Track and manage model versions:</p> <ul> <li>Automatic version tracking</li> <li>Rollback to previous versions</li> <li>Compare versions side-by-side</li> <li>Tag versions with metadata</li> </ul>"},{"location":"core-features/model-management/overview/#fine-tuning","title":"Fine-Tuning","text":"<p>Customize models for your use case:</p> <ul> <li>Fine-tune on your data</li> <li>Monitor training progress</li> <li>Evaluate performance</li> <li>Deploy fine-tuned models</li> </ul>"},{"location":"core-features/model-management/overview/#custom-endpoints","title":"Custom Endpoints","text":"<p>Deploy models with custom configurations:</p> <ul> <li>Create private endpoints</li> <li>Configure resource allocation</li> <li>Set up autoscaling</li> <li>Implement custom request/response handling</li> </ul>"},{"location":"core-features/model-management/overview/#common-workflows","title":"Common Workflows","text":""},{"location":"core-features/model-management/overview/#development-to-production","title":"Development to Production","text":"<pre><code>Local Development\n    \u2193\nUpload to Registry\n    \u2193\nVersion Control\n    \u2193\nFine-tuning (optional)\n    \u2193\nTest in Staging\n    \u2193\nDeploy to Production\n    \u2193\nMonitor &amp; Optimize\n</code></pre>"},{"location":"core-features/model-management/overview/#model-iteration","title":"Model Iteration","text":"<pre><code>Base Model\n    \u2193\nExperiment 1: Adjust parameters\n    \u2193\nExperiment 2: Different dataset\n    \u2193\nExperiment 3: New architecture\n    \u2193\nCompare Results\n    \u2193\nSelect Best Model\n    \u2193\nPromote to Production\n</code></pre>"},{"location":"core-features/model-management/overview/#key-concepts","title":"Key Concepts","text":""},{"location":"core-features/model-management/overview/#model","title":"Model","text":"<p>A complete ML model with weights, configuration, and metadata.</p>"},{"location":"core-features/model-management/overview/#version","title":"Version","text":"<p>A specific snapshot of a model at a point in time.</p>"},{"location":"core-features/model-management/overview/#endpoint","title":"Endpoint","text":"<p>A deployed instance of a model serving predictions.</p>"},{"location":"core-features/model-management/overview/#getting-started","title":"Getting Started","text":"<ol> <li>Upload a Model - Add your model to the registry</li> <li>Create Versions - Track different iterations</li> <li>Fine-tune - Customize for your use case</li> <li>Deploy - Create an endpoint</li> <li>Monitor - Track performance and costs</li> </ol>"},{"location":"core-features/model-management/version-control/","title":"Version Control","text":"<p>Manage model versions and track changes throughout your model lifecycle.</p>"},{"location":"core-features/model-management/version-control/#overview","title":"Overview","text":"<p>Version control allows you to:</p> <ul> <li>Track model iterations over time</li> <li>Compare different versions</li> <li>Rollback to previous versions</li> <li>Document changes and experiments</li> <li>Collaborate with team members</li> </ul>"},{"location":"core-features/model-management/version-control/#creating-versions","title":"Creating Versions","text":""},{"location":"core-features/model-management/version-control/#automatic-versioning","title":"Automatic Versioning","text":"<p>Each time you update a model, a new version is created:</p> <pre><code>import regolo\n\n# Version 1.0 is created automatically\nmodel = regolo.register_model(\n    name=\"sentiment-classifier\",\n    model_data=model_artifact,\n    version=\"1.0\"\n)\n</code></pre>"},{"location":"core-features/model-management/version-control/#manual-version-creation","title":"Manual Version Creation","text":"<p>Create versions for experiments:</p> <pre><code>import regolo\n\n# Version 1.1 - Experiment with new data\nmodel_v1_1 = regolo.create_model_version(\n    model_id=\"sentiment-classifier\",\n    version=\"1.1\",\n    model_data=updated_artifact,\n    metadata={\n        \"experiment\": \"expanded-training-data\",\n        \"accuracy\": 0.92,\n        \"f1_score\": 0.89\n    }\n)\n</code></pre>"},{"location":"core-features/model-management/version-control/#version-metadata","title":"Version Metadata","text":"<p>Each version stores important information:</p> <pre><code>version = regolo.get_model_version(\n    model_id=\"sentiment-classifier\",\n    version=\"1.0\"\n)\n\nprint(f\"Created: {version.created_at}\")\nprint(f\"Author: {version.created_by}\")\nprint(f\"Size: {version.size_mb} MB\")\nprint(f\"Status: {version.status}\")\nprint(f\"Metrics: {version.metadata['accuracy']}\")\n</code></pre>"},{"location":"core-features/model-management/version-control/#comparing-versions","title":"Comparing Versions","text":""},{"location":"core-features/model-management/version-control/#side-by-side-comparison","title":"Side-by-Side Comparison","text":"<pre><code>import regolo\n\ncomparison = regolo.compare_model_versions(\n    model_id=\"sentiment-classifier\",\n    version_a=\"1.0\",\n    version_b=\"1.1\"\n)\n\nprint(f\"Size difference: {comparison.size_diff_mb} MB\")\nprint(f\"Accuracy improvement: {comparison.metrics['accuracy']['diff']:.2%}\")\nprint(f\"F1 score improvement: {comparison.metrics['f1_score']['diff']:.2%}\")\n</code></pre>"},{"location":"core-features/model-management/version-control/#performance-comparison","title":"Performance Comparison","text":"<pre><code>metrics_comparison = {\n    \"1.0\": {\"accuracy\": 0.89, \"latency_ms\": 150},\n    \"1.1\": {\"accuracy\": 0.92, \"latency_ms\": 165},\n}\n\nfor version, metrics in metrics_comparison.items():\n    print(f\"Version {version}:\")\n    print(f\"  Accuracy: {metrics['accuracy']:.2%}\")\n    print(f\"  Latency: {metrics['latency_ms']}ms\")\n</code></pre>"},{"location":"core-features/model-management/version-control/#versioning-strategy","title":"Versioning Strategy","text":""},{"location":"core-features/model-management/version-control/#semantic-versioning","title":"Semantic Versioning","text":"<p>Use MAJOR.MINOR.PATCH format:</p> <pre><code>1.0.0  - Initial release\n1.1.0  - New features\n1.1.1  - Bug fix\n2.0.0  - Breaking changes\n</code></pre>"},{"location":"core-features/model-management/version-control/#by-environment","title":"By Environment","text":"<pre><code>dev-1.0   - Development\nstaging-1.0 - Staging\nprod-1.0  - Production\n</code></pre>"},{"location":"core-features/model-management/version-control/#rollback","title":"Rollback","text":""},{"location":"core-features/model-management/version-control/#rollback-to-previous-version","title":"Rollback to Previous Version","text":"<pre><code>import regolo\n\n# Issue found with v1.1, rollback to v1.0\nregolo.rollback_endpoint(\n    endpoint_id=\"sentiment-endpoint\",\n    to_version=\"1.0\"\n)\n\nprint(\"Rolled back to version 1.0\")\n</code></pre>"},{"location":"core-features/model-management/version-control/#gradual-rollback","title":"Gradual Rollback","text":"<pre><code># Canary deployment: 10% traffic to new version\nregolo.update_endpoint_traffic(\n    endpoint_id=\"sentiment-endpoint\",\n    version_traffic={\n        \"1.0\": 0.9,   # 90% traffic\n        \"1.1\": 0.1    # 10% traffic\n    }\n)\n\n# Monitor metrics\nmetrics = regolo.get_endpoint_metrics(endpoint_id=\"sentiment-endpoint\")\nprint(f\"Error rate (v1.1): {metrics.version['1.1']['error_rate']:.2%}\")\n\n# If errors low, increase traffic to v1.1\nif metrics.version['1.1']['error_rate'] &lt; 0.01:\n    regolo.update_endpoint_traffic(\n        endpoint_id=\"sentiment-endpoint\",\n        version_traffic={\"1.0\": 0.5, \"1.1\": 0.5}  # 50/50 split\n    )\n</code></pre>"},{"location":"core-features/model-management/version-control/#best-practices","title":"Best Practices","text":"<ol> <li>Version Naming - Use consistent naming scheme</li> <li>Document Changes - Record what changed in each version</li> <li>Test Thoroughly - Validate versions before production</li> <li>Monitor Metrics - Track performance of each version</li> <li>Clean Up - Remove old versions you don't need</li> </ol>"},{"location":"core-features/model-management/version-control/#listing-versions","title":"Listing Versions","text":"<pre><code>import regolo\n\n# Get all versions\nversions = regolo.list_model_versions(\n    model_id=\"sentiment-classifier\"\n)\n\nfor version in versions:\n    print(f\"{version.version} - {version.created_at} - {version.status}\")\n</code></pre>"},{"location":"core-features/model-management/version-control/#version-promotion","title":"Version Promotion","text":""},{"location":"core-features/model-management/version-control/#promote-to-production","title":"Promote to Production","text":"<pre><code>import regolo\n\n# After testing in staging, promote to production\nregolo.promote_model_version(\n    model_id=\"sentiment-classifier\",\n    version=\"1.1\",\n    environment=\"production\",\n    replicas=10  # Number of instances\n)\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/","title":"Alerts &amp; Notifications","text":"<p>Set up alerts and notifications for important events and thresholds.</p>"},{"location":"core-features/observability/alerts-notifications/#alert-types","title":"Alert Types","text":""},{"location":"core-features/observability/alerts-notifications/#quota-alerts","title":"Quota Alerts","text":"<p>Notify when approaching usage limits:</p> <pre><code>Daily token quota: 1M tokens\nAlert at 80%: 800K tokens\nAlert at 90%: 900K tokens\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#cost-alerts","title":"Cost Alerts","text":"<p>Notify when spending exceeds budget:</p> <pre><code>Daily budget: $100\nAlert at $80 (80%)\nAlert at $100 (100%)\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#performance-alerts","title":"Performance Alerts","text":"<p>Notify on performance degradation:</p> <pre><code>P95 latency threshold: 500ms\nAlert when exceeded\n\nError rate threshold: 1%\nAlert when exceeded\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#availability-alerts","title":"Availability Alerts","text":"<p>Notify on service issues:</p> <pre><code>Consecutive failed requests: 5\nAlert triggered\n\nSustained error rate &gt; 5%\nAlert triggered\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#creating-alerts","title":"Creating Alerts","text":""},{"location":"core-features/observability/alerts-notifications/#via-dashboard","title":"Via Dashboard","text":"<ol> <li>Go to dashboard.regolo.ai/alerts</li> <li>Click Create Alert</li> <li>Select alert type</li> <li>Set threshold</li> <li>Choose notification method</li> <li>Save</li> </ol>"},{"location":"core-features/observability/alerts-notifications/#via-api","title":"Via API","text":"<pre><code>import regolo\n\nalert = regolo.create_alert(\n    name=\"Daily quota warning\",\n    type=\"quota\",\n    threshold=800000,  # 80% of 1M\n    comparison=\"greater_than\",\n    notification_method=\"email\",\n    cooldown_minutes=30\n)\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#notification-channels","title":"Notification Channels","text":""},{"location":"core-features/observability/alerts-notifications/#email","title":"Email","text":"<p>Receive alerts via email:</p> <pre><code>Alert triggered at 10:30 AM\nEmail sent to your registered address\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#sms","title":"SMS","text":"<p>Critical alerts via SMS:</p> <pre><code>[Regolo] Error rate exceeded 5% - Check dashboard\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#slack","title":"Slack","text":"<p>Integrate with Slack:</p> <pre><code>[Regolo Alert] Cost limit reached\nDaily spend: $100 / $100\nDetails: &lt;link to dashboard&gt;\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#webhook","title":"Webhook","text":"<p>Custom webhooks:</p> <pre><code>alert = regolo.create_alert(\n    type=\"cost\",\n    threshold=100,\n    notification_method=\"webhook\",\n    webhook_url=\"https://your-app.com/alerts\"\n)\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#alert-configuration","title":"Alert Configuration","text":""},{"location":"core-features/observability/alerts-notifications/#severity-levels","title":"Severity Levels","text":"Level Description Notification Info FYI events Dashboard only Warning Approaching limits Email Critical Urgent action needed Email + SMS"},{"location":"core-features/observability/alerts-notifications/#cooldown-periods","title":"Cooldown Periods","text":"<p>Prevent alert spam:</p> <pre><code>Alert fires at 10:30 AM\nCooldown: 30 minutes\nNext alert: Can fire at 11:00 AM earliest\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#alert-grouping","title":"Alert Grouping","text":"<p>Group related alerts:</p> <pre><code>[Regolo] 5 alerts triggered\n\u251c\u2500 Error rate: 5.2%\n\u251c\u2500 P95 latency: 600ms\n\u251c\u2500 Daily quota: 95%\n\u251c\u2500 Cost limit: 80%\n\u2514\u2500 Failed requests: 10\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#example-alert-configurations","title":"Example Alert Configurations","text":""},{"location":"core-features/observability/alerts-notifications/#development-monitoring","title":"Development Monitoring","text":"<pre><code>Alert 1: Error rate &gt; 1% \u2192 Email\nAlert 2: P99 latency &gt; 5000ms \u2192 Email\nAlert 3: Failed requests &gt; 5 \u2192 Dashboard\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#production-monitoring","title":"Production Monitoring","text":"<pre><code>Alert 1: Error rate &gt; 0.5% \u2192 Email + SMS\nAlert 2: P95 latency &gt; 500ms \u2192 Email\nAlert 3: Daily cost &gt; $500 \u2192 Email\nAlert 4: Failed requests &gt; 3 \u2192 SMS\nAlert 5: Quota &gt; 90% \u2192 Email\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#cost-control","title":"Cost Control","text":"<pre><code>Alert 1: Daily cost &gt; 70% budget \u2192 Email\nAlert 2: Daily cost &gt; 90% budget \u2192 Email + SMS\nAlert 3: Monthly cost &gt; 50% budget \u2192 Email\nAlert 4: Monthly cost &gt; 90% budget \u2192 Email + SMS\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#slack-integration","title":"Slack Integration","text":""},{"location":"core-features/observability/alerts-notifications/#setup","title":"Setup","text":"<ol> <li>Go to Integrations in dashboard</li> <li>Click Connect Slack</li> <li>Authorize Regolo app</li> <li>Select channel for alerts</li> <li>Configure which alerts to send</li> </ol>"},{"location":"core-features/observability/alerts-notifications/#custom-messages","title":"Custom Messages","text":"<pre><code>alert = regolo.create_alert(\n    type=\"cost\",\n    threshold=100,\n    notification_method=\"slack\",\n    slack_config={\n        \"channel\": \"#alerts\",\n        \"mention\": \"@devops\",\n        \"message_template\": \"Cost threshold exceeded: ${{ actual }} / ${{ threshold }}\"\n    }\n)\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#alert-status","title":"Alert Status","text":""},{"location":"core-features/observability/alerts-notifications/#dismissing-alerts","title":"Dismissing Alerts","text":"<p>Temporarily dismiss repeated alerts:</p> <pre><code>Alert is dismissed\nWill reappear in 24 hours or if condition changes\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#muting-alerts","title":"Muting Alerts","text":"<p>Disable alerts during maintenance:</p> <pre><code>regolo.mute_alerts(\n    duration_minutes=60,\n    alert_type=\"cost\"  # Specific type\n)\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#alert-analytics","title":"Alert Analytics","text":""},{"location":"core-features/observability/alerts-notifications/#alert-history","title":"Alert History","text":"<p>View past alerts:</p> <pre><code>December 18:\n  10:30 - Cost limit reached\n  11:00 - Error rate spike\n  15:30 - Quota alert\n\nDecember 17:\n  09:30 - Daily quota reset\n</code></pre>"},{"location":"core-features/observability/alerts-notifications/#alert-trends","title":"Alert Trends","text":"<p>Analyze alert frequency:</p> <pre><code>Last 7 days: 5 alerts\nLast 30 days: 12 alerts\nMost common: Cost limit alerts\n</code></pre>"},{"location":"core-features/observability/cost-analytics/","title":"Cost Analytics","text":"<p>Analyze and optimize your API costs.</p>"},{"location":"core-features/observability/cost-analytics/#cost-breakdown","title":"Cost Breakdown","text":"<p>Regolo pricing is based on tokens consumed:</p> <pre><code>Cost = (Input Tokens \u00d7 Input Rate) + (Output Tokens \u00d7 Output Rate)\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#example","title":"Example","text":"<pre><code>Input: 100 tokens \u00d7 $0.00001 = $0.001\nOutput: 50 tokens \u00d7 $0.00002 = $0.001\nTotal: $0.002 per request\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#cost-analytics-dashboard","title":"Cost Analytics Dashboard","text":"<p>Access cost analytics at dashboard.regolo.ai/costs</p>"},{"location":"core-features/observability/cost-analytics/#cost-by-model","title":"Cost by Model","text":"<p>See which models drive your costs:</p> <pre><code>Llama-3.3-70B-Instruct: $1,250 (65%)\nGPT-4 Vision: $450 (23%)\nEmbedding Model: $300 (15%)\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#cost-over-time","title":"Cost Over Time","text":"<p>Track spending trends:</p> <pre><code>Week 1: $100\nWeek 2: $115\nWeek 3: $142\nWeek 4: $198\nMonth Total: $555\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#cost-by-project","title":"Cost by Project","text":"<p>Allocate costs to projects:</p> <pre><code>Production API: $400 (72%)\nDevelopment: $120 (22%)\nTesting: $35 (6%)\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#cost-by-api-key","title":"Cost by API Key","text":"<p>Track usage by API key:</p> <pre><code>api_key_prod: $350\napi_key_staging: $150\napi_key_dev: $55\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#optimization-recommendations","title":"Optimization Recommendations","text":""},{"location":"core-features/observability/cost-analytics/#1-model-selection","title":"1. Model Selection","text":"<p>Switching to cost-efficient models:</p> <pre><code>Current: Llama-3.3-70B @ $0.00002/token\nOptimal: Llama-2-7B @ $0.00001/token\nSavings: 50%\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#2-batch-processing","title":"2. Batch Processing","text":"<p>Group requests for better efficiency:</p> <pre><code>Before: 1000 individual requests\nAfter: 10 batch requests\nLatency: Acceptable (non-critical)\nSavings: 20%\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#3-caching","title":"3. Caching","text":"<p>Cache frequent requests:</p> <pre><code>Common query: \"Summarize [document]\"\nHit rate: 60%\nCost reduction: 40% on this query\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#4-prompt-optimization","title":"4. Prompt Optimization","text":"<p>Reduce unnecessary tokens:</p> <pre><code>Before: 500 token prompt with examples\nAfter: 150 token prompt, tuned\nTokens saved: 70%\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#cost-alerts","title":"Cost Alerts","text":"<p>Set up alerts for unexpected spending:</p> Daily LimitWeekly SpikeMonthly Budget <p>Alert when daily cost exceeds $100</p> <p>Alert if weekly cost increases by 50%</p> <p>Alert when monthly spend reaches 80% of $5,000 budget</p>"},{"location":"core-features/observability/cost-analytics/#cost-export","title":"Cost Export","text":""},{"location":"core-features/observability/cost-analytics/#csv-export","title":"CSV Export","text":"<p>Export cost data for external analysis:</p> <pre><code>curl -X GET https://api.regolo.ai/v1/analytics/costs \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -H \"Accept: text/csv\" \\\n    -G \\\n    -d \"start_date=2024-12-01\" \\\n    -d \"end_date=2024-12-18\" \\\n    &gt; costs.csv\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#json-export","title":"JSON Export","text":"<pre><code>import regolo\n\ncost_data = regolo.export_costs(\n    start_date=\"2024-12-01\",\n    end_date=\"2024-12-18\",\n    format=\"json\"\n)\n\nimport json\nwith open('costs.json', 'w') as f:\n    json.dump(cost_data, f)\n</code></pre>"},{"location":"core-features/observability/cost-analytics/#billing","title":"Billing","text":""},{"location":"core-features/observability/cost-analytics/#invoice-frequency","title":"Invoice Frequency","text":"<ul> <li>Monthly invoices on the 1st</li> <li>Usage calculated from previous month</li> <li>Billed to associated payment method</li> </ul>"},{"location":"core-features/observability/cost-analytics/#payment-methods","title":"Payment Methods","text":"<p>Accepted: - Credit card (Visa, Mastercard, Amex) - Bank transfer (ACH) - Purchase order (enterprise)</p>"},{"location":"core-features/observability/cost-analytics/#volume-discounts","title":"Volume Discounts","text":"Monthly Tokens Discount 1M - 10M 0% 10M - 100M 5% 100M - 500M 10% 500M+ 15% <p>Contact sales for custom volume discounts.</p>"},{"location":"core-features/observability/dashboards/","title":"Dashboards","text":"<p>View real-time metrics and insights with interactive dashboards.</p>"},{"location":"core-features/observability/dashboards/#default-dashboards","title":"Default Dashboards","text":""},{"location":"core-features/observability/dashboards/#overview-dashboard","title":"Overview Dashboard","text":"<p>High-level summary of your API activity:</p> <ul> <li>Requests: Total requests and request rate (req/min)</li> <li>Tokens: Input and output tokens consumed</li> <li>Cost: Daily and monthly spend</li> <li>Latency: Average, P95, and P99 response times</li> <li>Errors: Error rate and recent errors</li> </ul>"},{"location":"core-features/observability/dashboards/#models-dashboard","title":"Models Dashboard","text":"<p>Breakdown by model:</p> <ul> <li>Requests per model</li> <li>Token consumption per model</li> <li>Cost per model</li> <li>Error rate per model</li> <li>Latency distribution by model</li> </ul>"},{"location":"core-features/observability/dashboards/#projects-dashboard","title":"Projects Dashboard","text":"<p>If using multiple projects:</p> <ul> <li>Requests per project</li> <li>Costs per project</li> <li>Team member activity</li> <li>API key usage</li> </ul>"},{"location":"core-features/observability/dashboards/#creating-custom-dashboards","title":"Creating Custom Dashboards","text":""},{"location":"core-features/observability/dashboards/#1-add-dashboard","title":"1. Add Dashboard","text":"<p>From the dashboard homepage:</p> <ol> <li>Click Create Dashboard</li> <li>Name your dashboard</li> <li>Select widgets to add</li> </ol>"},{"location":"core-features/observability/dashboards/#2-add-widgets","title":"2. Add Widgets","text":"<p>Available widgets:</p> <ul> <li>Metric cards - Single metrics (requests, tokens, cost)</li> <li>Line charts - Trends over time</li> <li>Bar charts - Comparisons across dimensions</li> <li>Pie charts - Distribution breakdown</li> <li>Heatmaps - Usage patterns by hour/day</li> <li>Tables - Raw data with filtering</li> </ul>"},{"location":"core-features/observability/dashboards/#3-configure-filters","title":"3. Configure Filters","text":"<p>Limit data by:</p> <ul> <li>Time range</li> <li>Model</li> <li>Project</li> <li>API key</li> <li>User</li> <li>Status code</li> </ul>"},{"location":"core-features/observability/dashboards/#example-dashboards","title":"Example Dashboards","text":""},{"location":"core-features/observability/dashboards/#development-monitoring","title":"Development Monitoring","text":"<pre><code>Layout:\n- Requests (line chart, 7 days)\n- Errors (metric card)\n- P99 Latency (metric card)\n- Tokens by Model (bar chart)\n</code></pre>"},{"location":"core-features/observability/dashboards/#production-monitoring","title":"Production Monitoring","text":"<pre><code>Layout:\n- Request Rate (metric card)\n- Error Rate (metric card)\n- P95/P99 Latency (metric cards)\n- Cost Trend (line chart, 30 days)\n- Tokens by Model (pie chart)\n- Recent Errors (table)\n</code></pre>"},{"location":"core-features/observability/dashboards/#cost-optimization","title":"Cost Optimization","text":"<pre><code>Layout:\n- Daily Cost (line chart)\n- Cost by Model (pie chart)\n- Tokens per Dollar (metric card)\n- Cost by User (bar chart)\n- Top Models (table)\n</code></pre>"},{"location":"core-features/observability/dashboards/#metric-definitions","title":"Metric Definitions","text":"Metric Description Requests Total API requests (includes retries) Tokens Input + output tokens processed Cost Estimated cost based on token usage Latency (P50) Median response time Latency (P95) 95th percentile response time Latency (P99) 99th percentile response time Error Rate Percentage of requests that failed Throughput Requests per minute"},{"location":"core-features/observability/dashboards/#dashboard-sharing","title":"Dashboard Sharing","text":""},{"location":"core-features/observability/dashboards/#share-with-team","title":"Share with Team","text":"<ol> <li>Click Share on dashboard</li> <li>Select team members</li> <li>Set permissions (view-only or edit)</li> <li>Copy shareable link</li> </ol>"},{"location":"core-features/observability/dashboards/#public-dashboard","title":"Public Dashboard","text":"<p>Create public dashboards for status pages:</p> <pre><code>https://dashboard.regolo.ai/public/dashboards/abc123\n</code></pre>"},{"location":"core-features/observability/dashboards/#api-access","title":"API Access","text":"<p>Access dashboard data programmatically:</p> <pre><code>import regolo\n\n# Get dashboard metrics\nmetrics = regolo.get_dashboard_metrics(\n    dashboard_id=\"dashboard_123\",\n    time_range=\"7d\"\n)\n\nprint(f\"Requests: {metrics['requests']}\")\nprint(f\"Cost: ${metrics['cost']:.2f}\")\nprint(f\"Error rate: {metrics['error_rate']:.2%}\")\n</code></pre>"},{"location":"core-features/observability/latency-performance/","title":"Latency &amp; Performance","text":"<p>Monitor latency and performance metrics across your API calls.</p>"},{"location":"core-features/observability/latency-performance/#latency-metrics","title":"Latency Metrics","text":""},{"location":"core-features/observability/latency-performance/#request-latency","title":"Request Latency","text":"<p>Time from request submission to response:</p> <pre><code>Total Latency = Network Delay + Queue Time + Processing Time + Network Return\n</code></pre>"},{"location":"core-features/observability/latency-performance/#processing-time","title":"Processing Time","text":"<p>Time spent in the model:</p> <pre><code>Processing Time = Token Generation Time + Model Overhead\n</code></pre>"},{"location":"core-features/observability/latency-performance/#performance-tiers","title":"Performance Tiers","text":""},{"location":"core-features/observability/latency-performance/#p50-median","title":"P50 (Median)","text":"<p>50% of requests complete faster:</p> <ul> <li>Good: &lt; 200ms for completions</li> <li>Excellent: &lt; 100ms for embeddings</li> </ul>"},{"location":"core-features/observability/latency-performance/#p95","title":"P95","text":"<p>95% of requests complete faster:</p> <ul> <li>Good: &lt; 500ms for completions</li> <li>Excellent: &lt; 300ms for embeddings</li> </ul>"},{"location":"core-features/observability/latency-performance/#p99","title":"P99","text":"<p>99% of requests complete faster:</p> <ul> <li>Good: &lt; 1000ms for completions</li> <li>Excellent: &lt; 1000ms for embeddings</li> </ul>"},{"location":"core-features/observability/latency-performance/#performance-factors","title":"Performance Factors","text":""},{"location":"core-features/observability/latency-performance/#model-size","title":"Model Size","text":"<p>Larger models = slower responses:</p> <pre><code>Llama-2-7B:       P50=80ms\nLlama-3.3-70B:    P50=150ms\nGPT-4:            P50=300ms\n</code></pre>"},{"location":"core-features/observability/latency-performance/#request-complexity","title":"Request Complexity","text":"<p>More tokens = longer processing:</p> <pre><code>100 input tokens:  P50=80ms\n1000 input tokens: P50=150ms\n</code></pre>"},{"location":"core-features/observability/latency-performance/#output-length","title":"Output Length","text":"<p>More output tokens = longer generation:</p> <pre><code>10 output tokens:   P50=50ms\n100 output tokens:  P50=150ms\n500 output tokens:  P50=500ms\n</code></pre>"},{"location":"core-features/observability/latency-performance/#performance-monitoring","title":"Performance Monitoring","text":""},{"location":"core-features/observability/latency-performance/#dashboard-widgets","title":"Dashboard Widgets","text":"<pre><code>Latency Trend (7 days)\n\u251c\u2500 Line chart showing P50, P95, P99\n\u251c\u2500 Color coding: Green (good), Yellow (acceptable), Red (poor)\n\u2514\u2500 Hover for exact values\n\nLatency by Model\n\u251c\u2500 Bar chart\n\u251c\u2500 Sortable by metric\n\u2514\u2500 Drill-down to see detailed stats\n\nLatency Distribution\n\u251c\u2500 Histogram\n\u251c\u2500 Shows bucket distribution\n\u2514\u2500 Identify outliers\n</code></pre>"},{"location":"core-features/observability/latency-performance/#api-access","title":"API Access","text":"<pre><code>import regolo\n\n# Get performance metrics\nmetrics = regolo.get_performance_metrics(\n    model=\"Llama-3.3-70B-Instruct\",\n    time_range=\"24h\"\n)\n\nprint(f\"P50: {metrics.p50}ms\")\nprint(f\"P95: {metrics.p95}ms\")\nprint(f\"P99: {metrics.p99}ms\")\nprint(f\"Error rate: {metrics.error_rate:.2%}\")\n</code></pre>"},{"location":"core-features/observability/latency-performance/#optimization-tips","title":"Optimization Tips","text":""},{"location":"core-features/observability/latency-performance/#1-reduce-input-size","title":"1. Reduce Input Size","text":"<pre><code># Before: 2000 token prompt\nresponse = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": very_long_prompt\n    }]\n)\n# P50 latency: 300ms\n\n# After: 500 token optimized prompt\nresponse = regolo.static_chat_completions(\n    messages=[{\n        \"role\": \"user\",\n        \"content\": optimized_prompt\n    }]\n)\n# P50 latency: 80ms\n</code></pre>"},{"location":"core-features/observability/latency-performance/#2-use-smaller-models","title":"2. Use Smaller Models","text":"<p>Trade quality for speed:</p> <pre><code># Accurate but slow\nmodel = \"Llama-3.3-70B-Instruct\"  # P50=150ms\n\n# Faster alternative\nmodel = \"Llama-2-7B\"  # P50=80ms\n</code></pre>"},{"location":"core-features/observability/latency-performance/#3-batch-processing","title":"3. Batch Processing","text":"<p>For non-latency-critical workloads:</p> <pre><code># Before: Sequential requests\nfor prompt in prompts:\n    response = regolo.static_chat_completions(messages=prompt)\n\n# After: Batch processing\nresponses = regolo.batch_completions(requests=prompts)\n</code></pre>"},{"location":"core-features/observability/latency-performance/#4-enable-caching","title":"4. Enable Caching","text":"<p>Cache frequent requests:</p> <pre><code>response = regolo.static_chat_completions(\n    messages=messages,\n    cache_ttl=3600\n)\n# Cache hit: &lt;10ms latency\n</code></pre>"},{"location":"core-features/observability/latency-performance/#performance-sla","title":"Performance SLA","text":"<p>Our performance guarantees:</p> Tier P50 P95 P99 Standard &lt;300ms &lt;800ms &lt;2000ms Premium &lt;200ms &lt;500ms &lt;1000ms Enterprise &lt;100ms &lt;300ms &lt;500ms"},{"location":"core-features/observability/latency-performance/#troubleshooting-slow-responses","title":"Troubleshooting Slow Responses","text":""},{"location":"core-features/observability/latency-performance/#check-model-performance","title":"Check Model Performance","text":"<pre><code>Is the model slower than baseline?\n\u2192 Check model-specific metrics\n\u2192 Compare with historical performance\n</code></pre>"},{"location":"core-features/observability/latency-performance/#check-network","title":"Check Network","text":"<pre><code>Is network delay significant?\n\u2192 Check from different regions\n\u2192 Try private endpoint (if available)\n</code></pre>"},{"location":"core-features/observability/latency-performance/#check-queue","title":"Check Queue","text":"<pre><code>Is request queuing?\n\u2192 Check system load\n\u2192 Consider upgrade or reserved capacity\n</code></pre>"},{"location":"core-features/observability/latency-performance/#check-request","title":"Check Request","text":"<pre><code>Is the prompt unusually large?\n\u2192 Optimize prompt size\n\u2192 Reduce input tokens\n</code></pre>"},{"location":"core-features/observability/overview/","title":"Observability &amp; Monitoring","text":"<p>Monitor your API usage, performance, and costs with comprehensive observability tools.</p>"},{"location":"core-features/observability/overview/#key-metrics","title":"Key Metrics","text":"<p>Regolo provides real-time visibility into:</p> <ul> <li>Request Metrics: Volume, latency, error rates</li> <li>Token Usage: Prompt and completion tokens by model</li> <li>Cost Analytics: Per-request, per-model, per-user costs</li> <li>Performance: P50, P95, P99 latencies</li> <li>Errors: Error types, failure rates, stack traces</li> </ul>"},{"location":"core-features/observability/overview/#access-observability","title":"Access Observability","text":""},{"location":"core-features/observability/overview/#dashboard","title":"Dashboard","text":"<p>Visit dashboard.regolo.ai to access:</p> <ul> <li>Real-time metrics dashboards</li> <li>Cost analytics and breakdown</li> <li>Request and token tracking</li> <li>Alert configuration</li> </ul>"},{"location":"core-features/observability/overview/#api","title":"API","text":"<p>Access metrics programmatically:</p> <pre><code>import regolo\n\n# Get usage metrics\nmetrics = regolo.get_metrics(\n    start_time=\"2024-12-01\",\n    end_time=\"2024-12-18\",\n    group_by=\"model\"\n)\n\nfor metric in metrics:\n    print(f\"{metric.model}: {metric.tokens} tokens, ${metric.cost}\")\n</code></pre>"},{"location":"core-features/observability/overview/#key-features","title":"Key Features","text":"<ul> <li>Real-time dashboards - Live metrics and performance data</li> <li>Cost analytics - Break down costs by model, user, project</li> <li>Token tracking - Monitor token consumption patterns</li> <li>Latency analysis - Understand performance bottlenecks</li> <li>Alerts - Get notified of issues and quota limits</li> <li>Export data - Download reports and raw metrics</li> </ul>"},{"location":"core-features/observability/token-tracking/","title":"Token Usage Tracking","text":"<p>Track and monitor token consumption across your API calls.</p>"},{"location":"core-features/observability/token-tracking/#token-types","title":"Token Types","text":""},{"location":"core-features/observability/token-tracking/#input-tokens","title":"Input Tokens","text":"<p>Tokens in your prompt:</p> <pre><code>messages = [{\"role\": \"user\", \"content\": \"Summarize this document...\"}]\n# Input tokens: ~10 tokens\n\nresponse = regolo.static_chat_completions(messages=messages)\nprint(f\"Input tokens: {response.usage.prompt_tokens}\")\n</code></pre>"},{"location":"core-features/observability/token-tracking/#output-tokens","title":"Output Tokens","text":"<p>Tokens generated by the model:</p> <pre><code>response = regolo.static_chat_completions(\n    messages=messages,\n    max_tokens=100\n)\nprint(f\"Output tokens: {response.usage.completion_tokens}\")\n</code></pre>"},{"location":"core-features/observability/token-tracking/#total-tokens","title":"Total Tokens","text":"<p>Sum of input and output:</p> <pre><code>response = regolo.static_chat_completions(messages=messages)\nprint(f\"Total tokens: {response.usage.total_tokens}\")\n# = input_tokens + output_tokens\n</code></pre>"},{"location":"core-features/observability/token-tracking/#token-tracking-dashboard","title":"Token Tracking Dashboard","text":"<p>Access token metrics at dashboard.regolo.ai/tokens</p>"},{"location":"core-features/observability/token-tracking/#daily-token-usage","title":"Daily Token Usage","text":"<pre><code>Day 1: 500K tokens\nDay 2: 650K tokens\nDay 3: 800K tokens\n...\nMonth Total: 15M tokens\n</code></pre>"},{"location":"core-features/observability/token-tracking/#tokens-by-model","title":"Tokens by Model","text":"<pre><code>Llama-3.3-70B: 8M tokens (53%)\nLlama-2-7B:    5M tokens (33%)\nEmbedding:     2M tokens (14%)\n</code></pre>"},{"location":"core-features/observability/token-tracking/#tokens-by-user","title":"Tokens by User","text":"<p>Track per-user consumption:</p> <pre><code>user_123: 5M tokens\nuser_456: 3M tokens\nuser_789: 2M tokens\n</code></pre>"},{"location":"core-features/observability/token-tracking/#token-estimation","title":"Token Estimation","text":""},{"location":"core-features/observability/token-tracking/#estimation-guide","title":"Estimation Guide","text":"<p>General rules of thumb:</p> <pre><code>1 English word    \u2248 1.3 tokens\n1 token          \u2248 4 characters\n1 sentence       \u2248 8 tokens\n1 paragraph      \u2248 40 tokens\n1 page of text   \u2248 500 tokens\n</code></pre>"},{"location":"core-features/observability/token-tracking/#estimation-in-code","title":"Estimation in Code","text":"<pre><code>import regolo\n\n# Estimate before making request\ntokens_estimate = regolo.estimate_tokens(\n    messages=[{\"role\": \"user\", \"content\": \"Your prompt here\"}]\n)\n\nprint(f\"Estimated tokens: {tokens_estimate}\")\nprint(f\"Estimated cost: ${tokens_estimate * 0.00001:.4f}\")\n</code></pre>"},{"location":"core-features/observability/token-tracking/#token-limits","title":"Token Limits","text":""},{"location":"core-features/observability/token-tracking/#model-specific-limits","title":"Model-Specific Limits","text":"Model Context Max Output Llama-3.3-70B 8K 4K Llama-2-7B 4K 2K Embedding 8K N/A"},{"location":"core-features/observability/token-tracking/#account-limits","title":"Account Limits","text":"<p>Daily token quotas:</p> <pre><code>Free: 100K tokens/day\nPro: 1M tokens/day\nEnterprise: Custom\n</code></pre>"},{"location":"core-features/observability/token-tracking/#token-optimization","title":"Token Optimization","text":""},{"location":"core-features/observability/token-tracking/#1-reduce-input-tokens","title":"1. Reduce Input Tokens","text":"<pre><code># Before: 500 token prompt\nprompt = \"\"\"\nYou are a helpful assistant. Please follow these guidelines:\n1. Be concise\n2. Be accurate\n...\n(many more guidelines)\n\"\"\"\n\n# After: 50 token optimized prompt\nprompt = \"Helpful AI assistant. Concise &amp; accurate answers.\"\n\n# Savings: 90%\n</code></pre>"},{"location":"core-features/observability/token-tracking/#2-limit-output-length","title":"2. Limit Output Length","text":"<pre><code># Before: No limit (may generate 500+ tokens)\nresponse = regolo.static_chat_completions(\n    messages=messages\n)\n\n# After: Limit to necessary tokens\nresponse = regolo.static_chat_completions(\n    messages=messages,\n    max_tokens=100  # Limit output\n)\n</code></pre>"},{"location":"core-features/observability/token-tracking/#3-use-efficient-models","title":"3. Use Efficient Models","text":"<pre><code># Expensive: 70B parameter model\nmodel = \"Llama-3.3-70B\"  # 150ms, more tokens\n\n# Efficient: 7B parameter model\nmodel = \"Llama-2-7B\"  # 80ms, fewer tokens\n</code></pre>"},{"location":"core-features/observability/token-tracking/#4-cache-common-requests","title":"4. Cache Common Requests","text":"<pre><code># Repeated question (no caching)\nfor _ in range(100):\n    response = regolo.static_chat_completions(\n        messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}]\n    )\n# Cost: 100 \u00d7 50 tokens = 5,000 tokens\n\n# With caching\nresponse = regolo.static_chat_completions(\n    messages=[{\"role\": \"user\", \"content\": \"What is AI?\"}],\n    cache_ttl=3600\n)\n# Cost: 1 \u00d7 50 tokens = 50 tokens (99% savings)\n</code></pre>"},{"location":"core-features/observability/token-tracking/#token-analytics-api","title":"Token Analytics API","text":""},{"location":"core-features/observability/token-tracking/#get-token-usage","title":"Get Token Usage","text":"<pre><code>import regolo\n\n# Get daily token usage\nusage = regolo.get_token_usage(\n    start_date=\"2024-12-01\",\n    end_date=\"2024-12-18\",\n    group_by=\"day\"\n)\n\nfor day_usage in usage:\n    print(f\"{day_usage.date}: {day_usage.tokens} tokens\")\n</code></pre>"},{"location":"core-features/observability/token-tracking/#get-model-specific-usage","title":"Get Model-Specific Usage","text":"<pre><code># Token usage by model\nusage_by_model = regolo.get_token_usage(\n    start_date=\"2024-12-01\",\n    end_date=\"2024-12-18\",\n    group_by=\"model\"\n)\n\nfor model_usage in usage_by_model:\n    print(f\"{model_usage.model}: {model_usage.tokens} tokens\")\n</code></pre>"},{"location":"core-features/observability/token-tracking/#export-token-data","title":"Export Token Data","text":"<pre><code># Export to CSV\ncsv_data = regolo.export_token_usage(\n    start_date=\"2024-12-01\",\n    end_date=\"2024-12-18\",\n    format=\"csv\"\n)\n</code></pre>"},{"location":"getting-started/choose-language/","title":"Choose Your Language","text":"<p>Regolo AI is fully compatible with the OpenAI API, so you can use any OpenAI-compatible client library. Below are setup instructions for the most popular options.</p>"},{"location":"getting-started/choose-language/#python","title":"Python","text":""},{"location":"getting-started/choose-language/#using-regolo-client-recommended","title":"Using Regolo Client (Recommended)","text":"<p>The official Regolo Python client provides a simple, Pythonic interface.</p> <pre><code>pip install regolo\n</code></pre> <pre><code>import regolo\n\nregolo.default_key = \"YOUR_API_KEY\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\n# Simple completion\nresponse = regolo.static_chat_completions(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response)\n\n# Using RegoloClient for chat sessions\nclient = regolo.RegoloClient()\nclient.add_prompt_to_chat(role=\"user\", prompt=\"Tell me a joke\")\nrole, content = client.run_chat()\nprint(content)\n</code></pre>"},{"location":"getting-started/choose-language/#using-openai-client","title":"Using OpenAI Client","text":"<p>You can also use the official OpenAI Python library:</p> <pre><code>pip install openai\n</code></pre> <pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    api_key=\"YOUR_API_KEY\",\n    base_url=\"https://api.regolo.ai/v1\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"Llama-3.3-70B-Instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"getting-started/choose-language/#nodejs","title":"Node.js","text":"<p>Use the official OpenAI Node.js library:</p> <pre><code>npm install openai\n</code></pre> <pre><code>import OpenAI from 'openai';\n\nconst client = new OpenAI({\n    apiKey: 'YOUR_API_KEY',\n    baseURL: 'https://api.regolo.ai/v1'\n});\n\nasync function main() {\n    const response = await client.chat.completions.create({\n        model: 'Llama-3.3-70B-Instruct',\n        messages: [{ role: 'user', content: 'Hello!' }]\n    });\n    console.log(response.choices[0].message.content);\n}\n\nmain();\n</code></pre>"},{"location":"getting-started/choose-language/#with-streaming","title":"With Streaming","text":"<pre><code>import OpenAI from 'openai';\n\nconst client = new OpenAI({\n    apiKey: 'YOUR_API_KEY',\n    baseURL: 'https://api.regolo.ai/v1'\n});\n\nasync function streamResponse() {\n    const stream = await client.chat.completions.create({\n        model: 'Llama-3.3-70B-Instruct',\n        messages: [{ role: 'user', content: 'Tell me a story' }],\n        stream: true\n    });\n\n    for await (const chunk of stream) {\n        process.stdout.write(chunk.choices[0]?.delta?.content || '');\n    }\n}\n\nstreamResponse();\n</code></pre>"},{"location":"getting-started/choose-language/#curl","title":"cURL","text":"<p>For quick testing or shell scripts, use cURL directly:</p> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -d '{\n        \"model\": \"Llama-3.3-70B-Instruct\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }'\n</code></pre>"},{"location":"getting-started/choose-language/#with-jq-for-pretty-output","title":"With jq for Pretty Output","text":"<pre><code>curl -s -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -d '{\n        \"model\": \"Llama-3.3-70B-Instruct\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n    }' | jq '.choices[0].message.content'\n</code></pre>"},{"location":"getting-started/choose-language/#other-languages","title":"Other Languages","text":"<p>Since Regolo AI is OpenAI-compatible, you can use any OpenAI client library by changing the base URL to <code>https://api.regolo.ai/v1</code>:</p> <ul> <li>Go: sashabaranov/go-openai</li> <li>Ruby: alexrudall/ruby-openai</li> <li>Java: TheoKanning/openai-java</li> <li>C#: betalgo/openai</li> <li>PHP: openai-php/client</li> </ul>"},{"location":"getting-started/choose-language/#environment-variables","title":"Environment Variables","text":"<p>For security, store your API key in environment variables:</p> Linux/macOSWindows (PowerShell) <pre><code>export REGOLO_API_KEY=\"your-api-key\"\n</code></pre> <pre><code>$env:REGOLO_API_KEY = \"your-api-key\"\n</code></pre> <p>Then access it in your code:</p> PythonNode.js <pre><code>import os\nimport regolo\n\nregolo.default_key = os.environ.get(\"REGOLO_API_KEY\")\n</code></pre> <pre><code>const client = new OpenAI({\n    apiKey: process.env.REGOLO_API_KEY,\n    baseURL: 'https://api.regolo.ai/v1'\n});\n</code></pre>"},{"location":"getting-started/first-api-call/","title":"Your First API Call","text":"<p>Make your first API call to Regolo AI.</p>"},{"location":"getting-started/first-api-call/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Regolo AI account (sign up here)</li> <li>An API key (learn how to create one)</li> </ul>"},{"location":"getting-started/first-api-call/#chat-completions-api","title":"Chat Completions API","text":"<p>The Chat Completions API is the primary way to interact with Regolo AI models. It accepts a list of messages and returns a model-generated response.</p>"},{"location":"getting-started/first-api-call/#request-format","title":"Request Format","text":"PythonPython (requests)cURL <pre><code>import regolo\n\nregolo.default_key = \"YOUR_API_KEY\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nresponse = regolo.static_chat_completions(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of Italy?\"}\n    ]\n)\nprint(response)\n</code></pre> <pre><code>import requests\n\napi_url = \"https://api.regolo.ai/v1/chat/completions\"\nheaders = {\n    \"Content-Type\": \"application/json\",\n    \"Authorization\": \"Bearer YOUR_API_KEY\"\n}\ndata = {\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the capital of Italy?\"}\n    ]\n}\n\nresponse = requests.post(api_url, headers=headers, json=data)\nprint(response.json())\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -d '{\n        \"model\": \"Llama-3.3-70B-Instruct\",\n        \"messages\": [\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": \"What is the capital of Italy?\"}\n        ]\n    }'\n</code></pre>"},{"location":"getting-started/first-api-call/#response-format","title":"Response Format","text":"<p>The API returns a JSON response with the following structure:</p> <pre><code>{\n    \"id\": \"chatcmpl-abc123\",\n    \"object\": \"chat.completion\",\n    \"created\": 1234567890,\n    \"model\": \"Llama-3.3-70B-Instruct\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"The capital of Italy is Rome.\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 25,\n        \"completion_tokens\": 8,\n        \"total_tokens\": 33\n    }\n}\n</code></pre>"},{"location":"getting-started/first-api-call/#message-roles","title":"Message Roles","text":"Role Description <code>system</code> Sets the behavior and context for the assistant <code>user</code> The human user's input <code>assistant</code> Previous responses from the model (for context)"},{"location":"getting-started/first-api-call/#error-handling","title":"Error Handling","text":"<p>Always handle potential errors in your API calls:</p> <pre><code>import requests\n\ntry:\n    response = requests.post(api_url, headers=headers, json=data)\n    response.raise_for_status()\n    result = response.json()\nexcept requests.exceptions.HTTPError as e:\n    print(f\"HTTP error: {e}\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"Request error: {e}\")\n</code></pre>"},{"location":"getting-started/first-api-call/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about streaming responses</li> <li>Explore response parameters</li> <li>Try vision models for image understanding</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start (&lt; 5 min)","text":"<p>Get up and running with Regolo AI in under 5 minutes.</p>"},{"location":"getting-started/quick-start/#1-get-your-api-key","title":"1. Get Your API Key","text":"<p>Sign up at dashboard.regolo.ai and create a new API key in the Virtual Keys section.</p>"},{"location":"getting-started/quick-start/#2-install-the-client","title":"2. Install the Client","text":"PythonNode.js <pre><code>pip install regolo\n</code></pre> <pre><code>npm install openai\n</code></pre>"},{"location":"getting-started/quick-start/#3-make-your-first-call","title":"3. Make Your First Call","text":"PythonNode.jscURL <pre><code>import regolo\n\nregolo.default_key = \"YOUR_API_KEY\"\nregolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\nresponse = regolo.static_chat_completions(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, Regolo!\"}]\n)\nprint(response)\n</code></pre> <pre><code>import OpenAI from 'openai';\n\nconst client = new OpenAI({\n    apiKey: 'YOUR_API_KEY',\n    baseURL: 'https://api.regolo.ai/v1'\n});\n\nconst response = await client.chat.completions.create({\n    model: 'Llama-3.3-70B-Instruct',\n    messages: [{ role: 'user', content: 'Hello, Regolo!' }]\n});\nconsole.log(response.choices[0].message.content);\n</code></pre> <pre><code>curl -X POST https://api.regolo.ai/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer YOUR_API_KEY\" \\\n    -d '{\n        \"model\": \"Llama-3.3-70B-Instruct\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Hello, Regolo!\"}]\n    }'\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Explore available models</li> <li>Learn about response parameters</li> <li>Check the API documentation</li> </ul>"},{"location":"getting-started/sandbox/","title":"Sandbox &amp; Playground","text":"<p>Explore Regolo AI models interactively without writing code.</p>"},{"location":"getting-started/sandbox/#regolo-playground","title":"Regolo Playground","text":"<p>The Regolo Playground is an interactive web interface where you can:</p> <ul> <li>Test different models - Compare responses across model families</li> <li>Experiment with parameters - Adjust temperature, max tokens, and more</li> <li>Save conversations - Keep track of your experiments</li> <li>Export code - Generate code snippets in your preferred language</li> </ul> <p>Open Playground \u2192</p>"},{"location":"getting-started/sandbox/#available-models","title":"Available Models","text":"<p>In the playground, you can test all available models:</p> Model Family Example Models Best For Llama Llama-3.3-70B-Instruct General chat, reasoning Vision Llama-3.2-11B-Vision-Instruct Image understanding Embedding multilingual-e5-large-instruct Text embeddings Rerank bge-reranker-v2-m3 Search result reranking <p>See the full model catalog for all available models.</p>"},{"location":"getting-started/sandbox/#playground-features","title":"Playground Features","text":""},{"location":"getting-started/sandbox/#system-prompts","title":"System Prompts","text":"<p>Set custom system prompts to control the assistant's behavior:</p> <pre><code>You are a helpful coding assistant specialized in Python.\nProvide concise answers with code examples when relevant.\n</code></pre>"},{"location":"getting-started/sandbox/#parameter-tuning","title":"Parameter Tuning","text":"Parameter Range Description <code>temperature</code> 0.0 - 2.0 Controls randomness (lower = more focused) <code>max_tokens</code> 1 - 4096+ Maximum response length <code>top_p</code> 0.0 - 1.0 Nucleus sampling threshold <code>frequency_penalty</code> -2.0 - 2.0 Reduces repetition"},{"location":"getting-started/sandbox/#conversation-history","title":"Conversation History","text":"<p>The playground maintains conversation context, allowing you to:</p> <ul> <li>Build multi-turn conversations</li> <li>Test follow-up questions</li> <li>Simulate real-world chat scenarios</li> </ul>"},{"location":"getting-started/sandbox/#api-testing","title":"API Testing","text":"<p>For direct API testing, check out the interactive API documentation:</p> <p>API Reference \u2192</p> <p>The Swagger UI allows you to:</p> <ul> <li>Browse all available endpoints</li> <li>Make authenticated requests</li> <li>View request/response schemas</li> <li>Download OpenAPI specification</li> </ul>"},{"location":"getting-started/sandbox/#tips-for-experimentation","title":"Tips for Experimentation","text":"<ol> <li>Start simple - Begin with basic prompts before adding complexity</li> <li>Compare models - Same prompt, different models, different results</li> <li>Iterate on prompts - Small changes can significantly improve outputs</li> <li>Save successful prompts - Export configurations for production use</li> <li>Monitor tokens - Watch token usage to optimize costs</li> </ol>"},{"location":"getting-started/sign-up/","title":"Sign Up &amp; API Keys","text":"<p>Learn how to sign up for Regolo AI and generate your API keys.</p>"},{"location":"getting-started/sign-up/#create-your-account","title":"Create Your Account","text":"<ol> <li>Visit dashboard.regolo.ai</li> <li>Click Sign Up and complete the registration form</li> <li>Verify your email address</li> <li>Log in to your new account</li> </ol>"},{"location":"getting-started/sign-up/#generate-an-api-key","title":"Generate an API Key","text":"<p>Once logged in, follow these steps to create your API key:</p> <ol> <li>Navigate to the Virtual Keys section in the sidebar</li> <li>Click Create New Key</li> <li>Choose a name for your key (e.g., \"Development\", \"Production\")</li> <li>Select the scope:<ul> <li>All models: Access to all available models</li> <li>Specific model: Restrict access to a single model</li> </ul> </li> <li>Click Generate</li> <li>Copy your key immediately - it won't be shown again!</li> </ol> <p>Keep Your Key Secure</p> <p>Never share your API key or commit it to version control. Use environment variables to store your keys securely.</p>"},{"location":"getting-started/sign-up/#managing-your-keys","title":"Managing Your Keys","text":"<p>From the Virtual Keys dashboard, you can:</p> <ul> <li>View usage: Monitor API calls and token consumption</li> <li>Rotate keys: Generate new keys and revoke old ones</li> <li>Set limits: Configure spending limits per key</li> <li>Delete keys: Remove keys that are no longer needed</li> </ul>"},{"location":"getting-started/sign-up/#best-practices","title":"Best Practices","text":"<ul> <li>Use separate keys for development and production</li> <li>Rotate keys periodically for security</li> <li>Set appropriate spending limits</li> <li>Monitor usage regularly to detect anomalies</li> </ul>"},{"location":"getting-started/welcome/","title":"Welcome to Regolo","text":"<p>To get started with Regolo.ai, sign up for an account at dashboard.regolo.ai.</p>"},{"location":"getting-started/welcome/#generate-an-api-key","title":"Generate an API Key","text":"<p>Once logged in, navigate to the Virtual Keys section and create a new key. You can choose a specific model or select \"All models\" to use the key across all available models.</p>"},{"location":"getting-started/welcome/#choose-your-client","title":"Choose your client","text":"<p>Regolo.ai is fully compatible with the OpenAI API, so you can use either:</p> <p>Regolo Python Library or OpenAI Python Library</p>"},{"location":"getting-started/welcome/#chat-example","title":"Chat Example","text":"<p>Below is an example of how to create a simple chat application in python using regolo client.</p> Using Regolo Client <pre><code>    import streamlit as st\n    import regolo\n\n    regolo.default_key = \"YOUR-API-KEY-HERE\"\n    regolo.default_chat_model = \"Llama-3.3-70B-Instruct\"\n\n    client = regolo.RegoloClient()\n\n    st.title(\"Regolo.ai Chat\")\n\n    if \"messages\" not in st.session_state:\n        st.session_state.messages = []\n\n    for msg in st.session_state.messages:\n        with st.chat_message(msg[\"role\"]):\n            st.markdown(msg[\"content\"])\n\n    user_input = st.chat_input(\"Write a message...\")\n    if user_input:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n\n        with st.chat_message(\"user\"):\n            st.markdown(user_input)\n\n        client.add_prompt_to_chat(role=\"user\", prompt=user_input)\n        for msg in st.session_state.messages:\n            client.add_prompt_to_chat(role=msg[\"role\"], prompt=msg[\"content\"])\n\n        response = client.run_chat()\n\n        role, content = response\n\n        st.session_state.messages.append({\"role\": role, \"content\": content})\n\n        with st.chat_message(role):\n            st.markdown(content)\n</code></pre>"},{"location":"models/custom-models/","title":"Custom Models","text":"<p>Regolo\u2019s Custom Models functionality allows you to upload and deploy your own AI models from Hugging Face onto dedicated infrastructure. You maintain full control over the GPU resources, ensuring consistent performance for your specific use cases.</p>"},{"location":"models/custom-models/#quick-start-guide","title":"Quick Start Guide","text":"<p>Follow these steps to bring your own model to Regolo:</p> <ol> <li>Add your model: Click the \"Add model\" button in the top-right corner of your dashboard.</li> <li>Import from Hugging Face: Enter the Hugging Face model URL.</li> <li>Save to Library: Rename the model if desired and save it to your personal library.</li> <li>Deploy: Click \"Deploy model\" from your library list.</li> <li>Select Instance: Choose a hardware instance with the appropriate GPU size and count for your model.</li> <li>Authenticate: Use an active Regolo API key with \"All models\" permissions to begin making calls.</li> </ol> <p>Warning</p> <p>A Regolo key with proper authorization for all models is required to ensure successful API calls.</p>"},{"location":"models/custom-models/#model-support-specific-endpoints","title":"Model Support &amp; Specific Endpoints","text":"<p>While you are free to upload any supported model, the API endpoint you use depends on the Model Family. You must follow the same URI and body structure used for our regular inference services.</p> Model Type Endpoint Path Text Generation (LLMs) <code>/custom-model/v1/chat/completions</code> Image Generation <code>/custom-model/v1/images/generations</code> Speech-To-Text <code>/custom-model/v1/audio/transcriptions</code> Embeddings <code>/custom-model/v1/embeddings</code> Rerankers <code>/custom-model/v1/rerank</code> <p>Info</p> <p>Ensure you use the specific endpoint corresponding to the model's functionality (e.g., Image, Audio, or Reranker) as detailed in our Swagger or on Model Families documentation.</p>"},{"location":"models/custom-models/#deployment-requirements-stability","title":"Deployment Requirements &amp; Stability","text":"<p>To ensure a successful deployment and avoid service interruptions, please verify the following before launching:</p> <ul> <li>vLLM Compatibility: For LLMs, ensure your model architecture is supported by vLLM. Check the vLLM supported models list.</li> <li>Hardware Matching: Verify the model's size and minimum VRAM requirements. The instance size must match or exceed these demands to prevent deployment failures.</li> <li>Startup Latency: Startup times typically range from a few minutes up to 15 minutes. During this window, the endpoint will remain unavailable while the weights are loaded.</li> </ul>"},{"location":"models/custom-models/#custom-inference-example","title":"Custom Inference Example","text":"<p>Base URL: <code>https://api.regolo.ai/custom-model/v1/</code></p> <pre><code>curl -X POST \\\nhttps://api.regolo.ai/custom-model/v1/chat/completions/ \\\n-H \"Authorization: Bearer YOUR-REGOLO-API-KEY\" \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"model\": \"YOUR_CUSTOM_MODEL_NAME\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n}'\n</code></pre>"},{"location":"models/custom-models/#pricing-billing","title":"Pricing &amp; Billing","text":"<ul> <li>Hourly Usage: Charges are based on the duration the instance is active.</li> <li>Initial Charge: You will be billed for the first hour immediately upon deployment.</li> <li>Invoicing: Monthly invoices detailing total hours and charges are available in your dashboard.</li> </ul>"}]}